[{"content":"","date":"2022-10-08","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"2022-10-08","permalink":"/","section":"多颗糖(duoketang)","summary":"","title":"多颗糖(duoketang)"},{"content":"建设中……\n","date":"2022-10-08","permalink":"/about/","section":"多颗糖(duoketang)","summary":"建设中……","title":"关于"},{"content":"","date":"2022-08-21","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"2022-08-21","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"2022-08-21","permalink":"/tags/tla/","section":"Tags","summary":"","title":"TLA"},{"content":" 与上一节讨论 Paxos 时一样，本文不介绍 Raft 协议基本知识，读者需已经了解 Raft 协议，如无这部分背景知识，请阅读 Raft 论文或参见《条分缕析Raft算法》。\n1、Raft 协议的 TLA+ 规约 # 本文参考 Diego Ongaro 提供的 TLA+ 规约：https://github.com/ongardie/raft.tla\n1.1、常量 # Raft 的 TLA+ 规约将系统抽象为以下几个常量：\nServer，节点 ID 集合 Value，客户端的一系列请求，发送到 Raft 状态机的值 Follower, Candidate, Leader，Raft 节点的三个状态 Nil，空的消息 四种消息类型 RequestVoteRequest, RequestVoteResponse, AppendEntriesRequest, AppendEntriesResponse 1.2、变量 # Raft 的 TLA+ 规约包括的变量主要还是和论文 Figure 2 相对应。\n所有节点都有：\ncurrentTerm，当前任期，需要持久化； votedFor，投票信息，需要持久化； log，日志，需要持久化； state，节点状态，三种 Follower, Candidate, Leader； Candidate 变量：\nvotesResponded，候选者在当前任期收到的 RequestVote 响应； votesGranted，候选者收到来自 RequestVote 的投票； Leader 变量：\nnextIndex，发送给 Follower 的下一个日志； matchIndex，领导者用来统计计算 commitIndex。如果超过半数节点的 matchIndex \u0026gt;= N 且任期一致，那么可以更新 commitIndex = N。 1.3、动作 # Raft 的动作较多，我们重点看节点是如何处理 RequestVote 和 AppendEntries 的。\n领导者选举的关键就是判断 logOk 和 grant。logOk 需要找出日志最新并且最完整的节点，最新就是任期最大，最完整就是日志最长。grant 就是在 logOk 正确的情况下，需要任期相同，并且没投过票或投票给请求节点。\n之后，候选者收集请求响应，判断是否收到超过半数的选票，来决定是否成为领导者。\nHandleAppendEntriesRequest 比较长，我们一步步来看。\n首先，根据 Receive(m)，如果消息体中的任期大于当前节点的任期，会先调用 UpdateTerm(i, j, m) 更新一些变量。之后，消息体中的任期小于等于当前节点的任期的情况。\n这里的 logOk 与 RequestVote 的检查不同，主要进行日志的一致性检查，AppendEntries 请求会包含新日志之前一条日志的索引(记为 prevLogIndex)和任期(记为 prevLogTerm)；跟随者收到请求后，会检查自己最后一条日志的索引和任期号是否与 prevLogIndex 和 prevLogTerm 相匹配，匹配则接收该记录；否则拒绝。\n该流程被称为一致性检查，如下图所示。\n一致性检查的原理可以用数学归纳法来证明，就是：首先，初始状态日志都是空的。其次，每追加一条日志都要通过一次性检查保证前一条日志是相同的。最后可得，这条日志之前的所有日志都是相同的，能够满足上述的日志安全性。\n接下来的代码比较长，主要分为：\n情况 1，领导者任期更小，或者任期相同但日志一致性检查不通过，拒绝这次请求。 情况 2，任期相同，但当前节点是 Candidate 状态，转为 Follower。 情况 3，前面的条件都满足，接受请求。 但是，情况三根据节点日志不同，又分为多种情况。\n情况 3.1，如果消息体中日志信息为空（即为心跳信息），或者该日志已经存在节点日志中，可能会导致 commitIndex 发生变化。 情况 3.2，如果日志冲突，将会以领导者的日志为准，删除节点的日志。 情况 3.3，如果没有冲突刚刚好，直接插入日志即可。 分析到这里，笔者觉得按照 Raft 算法的 TLA+ 去完成 6.824 的实验应该更清晰更简单。\n其他动作还有很多，但并没有这两处重要，究其根本这些动作主要还是围绕着服务器的三种状态来运行，我们可以通过 Follower、Leader、Candidate 之间的转换关系来梳理这些动作，篇幅所限，这里就不一一详述这些步骤的作用了，参见下图：\n2、运行 TLA+ model checker # 需要指出的是，Diego ongaro 的 raft.tla 仓库代码无法使用 TLA+ model checker 运行，而且作者本人也不打算继续维护。\n不过，Jin Lin fork 了代码，并且进行了补充，给出了一个能够通过 TLA+ model checker 运行的 tla+ 代码。地址是：https://github.com/jinlmsft/raft.tla\n如果不知道变量怎么设置，建议直接 clone 仓库，然后通过 TLA+ model checker 直接打开然后运行。我亲测是可以运行的。\nJin Lin 针对 Raft 的 TLA+ model checker 也有一个很好的 Talk。\n完结撒花 # Raft 的 TLA+ 跟伪代码十分接近，是学习该算法非常好的资料，感兴趣的同学可以用模型验证器跑一跑 Jin Lin 的代码。\n到这里我们的 TLA+ 教程就完结了，虽然这个系列阅读数据一般，但我觉得学习分布式系统还是需要掌握一些 TLA+ 知识，并不是说一定要会写，至少看得懂，知道怎么回事，能看懂一些经典算法的 TLA+ 程序，辅助自己实际编码。\n以后可能还会分享一些 TLA+ 相关的知识，这个系列教程先告一段落了。\n","date":"2022-08-21","permalink":"/posts/202208-tla-6/","section":"Posts","summary":"与上一节讨论 Paxos 时一样，本文不介绍 Raft 协议基本知识，读者需已经了","title":"TLA+ 入门教程（6）：Raft"},{"content":"","date":"2022-08-21","permalink":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/","section":"Categories","summary":"","title":"分布式系统"},{"content":" 1、Paxos 简介 # 上一节我们讨论了两阶段提交的 TLA+ 规约。众所周知，两阶段提交有一个明显的缺点，如果 TM 宕机，整个流程将停滞不前。\n有一个简单的工程解决办法：准备一个备 TM 节点，如果主 TM 故障，备 TM 节点将接替主 TM 的工作。这是在很多教材上都有写的解决方案，这个方案大多数时候都运行得很好，直到有一天：\n主 TM 决定 commit，但是紧接着就卡住或宕机了； 备 TM 认为主已经宕机，决定接管任务； 备 TM 广播 Abort 消息； 但此时，主 TM 恢复并广播 Commit 消息； 这会出现部分 RM 提交事务而部分 RM 中止事务，导致系统问题； 可见引入备 TM 仍然有瑕疵。寻找容错的分布式算法十分困难，很容易出现一些在测试阶段无法发现的问题。因此，在编码之前，我们必须先确定算法正确！\nLamport 说过：”编写和检查 TLA+ 规约是我目前知道如何做到这一点的最佳方式(We should get the algorithm right before we code.Writing and checking a TLA+ spec is the best way I know to do that.)\n可见，一个分布式算法的正确性需要经过必要的数学论证和形式化工具的检验。\n那么，我们如何才能保证多个 TM 节点就某个节点是主 TM 达成共识呢？更进一步，我们如何让分布式系统中的多个节点就某个值达成共识？经常阅读我的博客的读者应该知道，Paxos 就是解决这个问题的经典协议。\n本节需要读者了解 Paxos 协议的基本流程，如果你还不具备这部分预备知识，建议阅读我之前写的：《理解 Paxos（含伪代码）》，或购买我的书籍《深入理解分布式系统》。\n2、分布式共识 # 分布式共识问题亦可以用 TLA+ 来描述，源代码参见：https://github.com/tlaplus/Examples/blob/master/specifications/Paxos/Consensus.tla\n其中，常量 Value 表示所有可能的提案值的集合，提议的值由 Proposer 提出；变量 chosen 表示已被选定的值的集合。\n初始状态下 chosen 为空。下一个状态为：如果 chosen 为空，则从 Value 中任选一个值 v 加入 chosen 中(即选定 v)。\n不变式主要分为安全性和活性，安全性的不变式 Inv，要求 chosen 中的元素个数不超过 1，即安全性要求最多只有一个值被选定。\n活性的不变式分为 Success 和 LiveSpec，要求最终会有某个值被选定。\n注意：LiveSpec == Spec /\\ WF_chosen(Next) ，这里有个特别的语法，变量 chosen 前面有个 WF_，WF指弱公平性(weak fairness) 。WF_vars(A) 刻画了动作 A 的公平性，即要求: 如果动作 A 从某个时刻开始是持续可执行的,则 A 最终将被执行。\n简单来说，共识的核心问题就是分布式系统选定一个值，并且满足安全性和活性。\n3、Paxos 协议的 TLA+ 规约 # Paxos 协议的 TLA+ 规约来自：https://github.com/tlaplus/Examples/blob/master/specifications/Paxos/Paxos.tla\n篇幅原因，这里不再介绍 Paxos 协议基本流程等预备知识，请参见：《理解 Paxos（含伪代码）》。\n实际上，如果你不知道 Paxos 协议的流程，通过学习本文 TLA+ 规约，应该会使你更深入理解该协议细节。\n3.1、常量 # Paxos 协议的 TLA+ 规约有 3 个常量：\nValue：所有可能的提案值的集合，例如 {v1, v2, v3}，且有 None == CHOOSE v : v \\notin Value 表示不属于 Value 的某个特殊值； Acceptor：所有接受者的集合； Quorum：由接受者组成的多数派集合，QuorumAssumption 表示多数需要满足的条件。 提案编号用 Ballot 表示，且 Ballot == Nat，即 Ballot 是一个自然数。\n消息共有四种，对应 Paxos 算法的四个阶段： Phase1a、Phase1b、Phase2a 和 Phase2b。通过 Phase1b 的消息可以看到，用 -1 表示 Ballot 初始化时的最小提议编号。\n3.2、变量 # 变量有 4 个：\nmaxBal：maxBal[a] 表示接受者 a 见到过的最大提议编号，是 a 承诺可以接受的提议的最小编号； maxVBal：maxVBal[a] 表示接受者 a 接受过提议的最大编号； maxVal：对应 maxVBal[a] 的提案值； msgs：所有已发送消息的集合。参与者通过向该集合添加消息，或者从该集合读取消息模拟消息的发送(广播)与接收动作。Send(m) 将消息 m 加入到 msgs 中。如前所述，共有四种消息。 TypeOK 定义了各个变量的类型，Init 给出了各个变量的初始状态。\n3.3、动作 # Paxos 的 4 个动作对应协议的四个阶段。\nPhase1a(b)：Proposer 选取提议编号 b 并发送 Phase1a 消息，消息类型为 “1a”； Phase1b(a)：一旦 Acceptor 收到类型为 “1a” 的消息 m，如果 m 中的提案编号 m.bal 如果大于 maxBal[a]，则将 Acceptor 的 maxBal[a] 更新为 m.bal，并将接受过的最大的提案编号 maxVBal[a] 和提案的值 maxVal[a] 回复给 Proposer，消息类型是 “1b”； Phase2a(b, v)：Proposer 对提案 \u0026lt;b, v\u0026gt; 发起 Phase2b 消息。但是有两个前提，（1）第一句式子，要求 msgs 中没有提案编号为 b 的 “2a” 类型消息，即 Proposer 没有执行过 Phase2a(b, v)；（2）第二个较长的式子要求，存在多数派 Q，Q 中的每个 Acceptor 都回复过提案编号为 b 的 Phase1b 消息，即我们常说的，获得多数派 Acceptor 的回复。这两个条件都成立时，Proposer 发送 “2a” 消息。 Phase2b(a)：Acceptor 收到 “2a” 消息 m 时进行判断，当且仅当 m.bal \u0026gt;= maxBal[a]，Acceptor 才会接受提案 \u0026lt;b, v\u0026gt;，同时更新自己的变量 maxBal、maxVBal[a] 和 maxVal[a] 的状态，然后回复 “2b” 消息。 可见，Paxos 的 TLA+ 规约已经把整个协议的流程说得很明白了！同时修复了论文中 Phase2b 中并没有提到的，需要 Acceptor 更新 maxBal[a]。\n有读者曾在交流群里提出疑惑，Phase1b 阶段的 Acceptor 判断 Ballot 是大于还是大于等于，通过 TLA+ 消息，你能想明白吗？\n3.4、行为 # Next 定义了下一个状态，Spec 定义了完整的行为规约。\n4、深入 Paxos 协议 # 为了更好理解 Paxos 协议，Lamport 给出了另一个更抽象的规约，参见：https://github.com/tlaplus/Examples/blob/master/specifications/PaxosHowToWinATuringAward/Voting.tla\n根据 Voting 规约的信息，我们知道：Paxos 协议的核心在于 Acceptor 接受什么样的提案，第一阶段，Acceptor 只接受提案编号比它回复过的最大提案编号 maxBal[a] 还要大的提案；第二阶段，可被接受的提案 \u0026lt;b, v\u0026gt; 需要满足两个关键的条件：\nOneValuePerBallot，即，每个提案编号最多对应一个提案值，也隐含了一个 Acceptor 只能选定一个提案； SafeAt(b,v)，提案 \u0026lt;b, v\u0026gt; 是安全的，指对于任何提案编号小于 b 的提案，除了 v 以外没有其他的值被选定过，将来也不会有其他值被选定。 Lamport 使用 TLAPS(TLA+ proof system) 证明了 Paxos 协议正确性的关键在于上述两个条件，即只要满足这两个条件，就能保证 Paxos 的正确性。\n","date":"2022-08-01","permalink":"/posts/202208-tla-5/","section":"Posts","summary":"1、Paxos 简介 # 上一节我们讨论了两阶段提交的 TLA+ 规约。众所","title":"TLA+ 入门教程（5）：Paxos"},{"content":" 1、两阶段提交 # 本章要学习的是用在婚姻和数据库领域的分布式算法——两阶段提交。\n据 Lamport 回忆，数据库领域图灵奖得主 Jim Gray 喜欢用婚礼来描述事务，婚礼上除了新娘和新郎，还会有牧师或者司仪，\n我们假设新娘 Anne，新郎 Henry 以及牧师 Thomas。婚礼上，牧师先问新郎：“你愿意娶 Anne 为妻吗？”\n新郎 Henry：“我愿意。”\n牧师再问新娘：“你愿意嫁给 Henry 吗？”\n新娘 Anne：“我愿意。”\n然后，牧师就会在亲朋好友面前宣布这段关系。\n假如新郎或新娘其中一人说不愿意，牧师就会取消婚礼（虽然这很少见，逃。。。）\n这个对比很形象地展示了两阶段提交的核心，需要有一个协调者，向多个参与者询问是否可以提交本次事务；只有当多个参与者都确认提交，本次事务才会真正提交；只要有一个参与者表示无法提交，那么协调者就要中止本次事务。\n如今，大量分布式系统使用着两阶段提交，该算法简单清晰，容易建模。本节我们将学习两阶段提交的 TLA+ 规约，并认识到分布式系统一些重要且基本的特性。\n首先，我们将协调者称作事务管理者（Transaction Manager，TM），参与者称为资源管理者（Resource Managers，RMs），所有的参与者必须就事务提交还是中止达成共识。\n通常来说 RM 可以查询 TM 的状态以更新自己的状态，但是 RM 无法查询其他 RM 的状态。TM 可以读取所有 RM 的状态以更新自己的状态。\n2、用到的 TLA+ 操作符 # 本次会用到一个新的 TLA+ 类型：记录（Records），类似于大多数编程语言中的结构体，例如：\n有两个域（成员）prof 和 num，其中 r.prof = \u0026quot;Fred\u0026quot; 和 r.num = 42。\nASCII 的语法这样写：\nr = [prof |-\u0026gt; \u0026#34;Fred\u0026#34;, num |-\u0026gt; 42] 在 TLA+ 中，f.prof 等同于 f[\u0026quot;prof\u0026quot;]。并且，我们还可以将“f 中除了 prof 以外的都等于 Red”语句写为： [f EXCEPT ![\u0026quot;prof\u0026quot;] = \u0026quot;Red\u0026quot;] 或者 [f EXCEPT !.\u0026quot;prof\u0026quot; = \u0026quot;Red\u0026quot;]。\n记录的语法主要分为以下四类：\n3、两阶段提交的 TLA+ 规约 # 完整代码见：https://github.com/tlaplus/Examples/blob/master/specifications/transaction_commit/TwoPhase.tla\n3.1、常量 # 两阶段提交的 TLA+ 规约只有 1 个常量 RM，表示所有资源管理器的集合，如，{r1，r2，r3}。\nCONSTANT RM \\* The set of resource managers 注意：TLA+ 的每个变量或常量都是一个集合，而不只是一个值。\n3.2、变量 # VARIABLES rmState, \\* $rmState[rm]$ is the state of resource manager RM. tmState, \\* The state of the transaction manager. tmPrepared, \\* The set of RMs from which the TM has received $\u0026#34;Prepared\u0026#34;$ msgs \\* messages. 两阶段提交的 TLA+ 规约包括 4 个变量\nrmState：用 rmState[rm] 表示 rm 的状态，有 4 种可能的状态：\u0026ldquo;working\u0026rdquo;, \u0026ldquo;prepared\u0026rdquo;, \u0026ldquo;committed\u0026rdquo;, \u0026ldquo;aborted\u0026rdquo;； tmState：TM 的状态，有 3 种可能的状态：\u0026ldquo;init\u0026rdquo;, \u0026ldquo;committed\u0026rdquo;, \u0026ldquo;aborted\u0026rdquo;； tmPrepared：TM 已收到 Prepared 消息的 RM 的集合； msgs：描述正在传输的消息。 Message == [type : {\u0026#34;Prepared\u0026#34;}, rm : RM] \\cup [type : {\u0026#34;Commit\u0026#34;, \u0026#34;Abort\u0026#34;}] TPTypeOK == /\\ rmState \\in [RM -\u0026gt; {\u0026#34;working\u0026#34;, \u0026#34;prepared\u0026#34;, \u0026#34;committed\u0026#34;, \u0026#34;aborted\u0026#34;}] /\\ tmState \\in {\u0026#34;init\u0026#34;, \u0026#34;committed\u0026#34;, \u0026#34;aborted\u0026#34;} /\\ tmPrepared \\subseteq RM /\\ msgs \\subseteq Message TPTypeOK 是规约的不变式，用来检查变量必须满足的条件。其中 \\subseteq 代表子集，例如 msgs 一定是 Message 的子集。\n此外，Message 有一个 \\cup 操作符，即 Message 属于两个集合的并集（写作 \\cup 或 \\union 都可以）。例如 Message = A \\cup B，那么 Message 是 A 中的元素或 B 中的元素，或者两者都有的元素。如下图所示。\n进一步来看，[type : {\u0026quot;Prepared\u0026quot;}, rm : RM] 这一部分代表，RM 发送到 TM 的 Prepared 消息。[type : {\u0026quot;Commit\u0026quot;, \u0026quot;Abort\u0026quot;}] 代表 TM 向所有 RM 发送的 Commit 或 Abort 消息。\n系统的初始状态如下：\nTPInit == /\\ rmState = [rm \\in RM |-\u0026gt; \u0026#34;working\u0026#34;] /\\ tmState = \u0026#34;init\u0026#34; /\\ tmPrepared = {} /\\ msgs = {} rmState 中的每个 RM 的状态都为 working，tmState = \u0026quot;init\u0026quot; 和 tmPrepared = {} 代表 TM 刚初始化的状态，并没有收到任何消息，即 msg 为空 msg = {}。\n3.3、动作 # 两阶段提交的规约定义了 7 种动作，分别对应 TM 的 3 个动作和 RM 的 4 个动作。\nTMRcvPrepared(rm) == /\\ tmState = \u0026#34;init\u0026#34; /\\ [type |-\u0026gt; \u0026#34;Prepared\u0026#34;, rm |-\u0026gt; rm] \\in msgs /\\ tmPrepared\u0026#39; = tmPrepared \\cup {rm} /\\ UNCHANGED \u0026lt;\u0026lt;rmState, tmState, msgs\u0026gt;\u0026gt; 只有当 TM 为 init 状态，并且收到来自 RM 的 Prepared 消息之后，tmPrepared 会被更新为 tmPrepared 和发送消息的 RM 的并集，换句话说，发送消息的 RM 会被添加到集合 tmPrepared 中。这里如果 RM 已经在 tmPrepared 中，那么 tmPrepared' 将会不变。\n最后一行 UNCHANGED 表示其他状态都不变。\nTMCommit == /\\ tmState = \u0026#34;init\u0026#34; /\\ tmPrepared = RM /\\ tmState\u0026#39; = \u0026#34;committed\u0026#34; /\\ msgs\u0026#39; = msgs \\cup {[type |-\u0026gt; \u0026#34;Commit\u0026#34;]} /\\ UNCHANGED \u0026lt;\u0026lt;rmState, tmPrepared\u0026gt;\u0026gt; 只有当 TM 为 init 状态并且 tmPrepared = RM 时，即初始状态的 TM 收到来自所有 RM 的 Prepared 消息，TM 会进入提交阶段。\nTMAbort == /\\ tmState = \u0026#34;init\u0026#34; /\\ tmState\u0026#39; = \u0026#34;aborted\u0026#34; /\\ msgs\u0026#39; = msgs \\cup {[type |-\u0026gt; \u0026#34;Abort\u0026#34;]} /\\ UNCHANGED \u0026lt;\u0026lt;rmState, tmPrepared\u0026gt;\u0026gt; 只有当 TM 为 init 状态并且收到来自一个 RM 的 Abort 消息时，该动作才会启用。\nRMPrepare(rm) == /\\ rmState[rm] = \u0026#34;working\u0026#34; /\\ rmState\u0026#39; = [rmState EXCEPT ![rm] = \u0026#34;prepared\u0026#34;] /\\ msgs\u0026#39; = msgs \\cup {[type |-\u0026gt; \u0026#34;Prepared\u0026#34;, rm |-\u0026gt; rm]} /\\ UNCHANGED \u0026lt;\u0026lt;tmState, tmPrepared\u0026gt;\u0026gt; 只有当 rmState[rm] = \u0026quot;working\u0026quot; 时，RM 表示自己准备好了，RM 更新自己状态为 prepared，并且向 TM 发送 Prepared 消息。\nRMChooseToAbort(rm) == /\\ rmState[rm] = \u0026#34;working\u0026#34; /\\ rmState\u0026#39; = [rmState EXCEPT ![rm] = \u0026#34;aborted\u0026#34;] /\\ UNCHANGED \u0026lt;\u0026lt;tmState, tmPrepared, msgs\u0026gt;\u0026gt; 当 rmState[rm] = \u0026quot;working\u0026quot; 时，也可能会中止事务。只要有一个 RM 中止事务，整个事务都必须中止，即 TM 最终会调用 TMAbort。在实际系统中，RM 会告诉 TM 它已经中止事务，因此 RM 会知道它应该中止事务，但这种优化可以省略。\nRMRcvCommitMsg(rm) == /\\ [type |-\u0026gt; \u0026#34;Commit\u0026#34;] \\in msgs /\\ rmState\u0026#39; = [rmState EXCEPT ![rm] = \u0026#34;committed\u0026#34;] /\\ UNCHANGED \u0026lt;\u0026lt;tmState, tmPrepared, msgs\u0026gt;\u0026gt; RMRcvAbortMsg(rm) == /\\ [type |-\u0026gt; \u0026#34;Abort\u0026#34;] \\in msgs /\\ rmState\u0026#39; = [rmState EXCEPT ![rm] = \u0026#34;aborted\u0026#34;] /\\ UNCHANGED \u0026lt;\u0026lt;tmState, tmPrepared, msgs\u0026gt;\u0026gt; 当 RM 收到 commit 或 abort 消息，他会更新自己到对应的状态。\n这里有一个细节读者可以思考下，根据 TLA+，RM 只有在状态为 working 的时候才能中止，为什么 RM 处于 prepared 状态的时候不能中止呢？\nRM 发送 prepared 消息后才进入 prepared 状态，换句话说，prepared 状态意味着准备好提交了，如果此时还允许 RM 进入 abort 状态，TM 仍可能会发送 Commit 事务消息——尽管一些 RM 并不同意提交，但这与规约相矛盾。\n3.4、行为 # TPNext 定义了次态关系，Spec 定义了完整的行为规约。\nTPNext == \\/ TMCommit \\/ TMAbort \\/ \\E rm \\in RM : TMRcvPrepared(rm) \\/ RMPrepare(rm) \\/ RMChooseToAbort(rm) \\/ RMRcvCommitMsg(rm) \\/ RMRcvAbortMsg(rm) ----------------------------------------------------------------------------- TPSpec == TPInit /\\ [][TPNext]_\u0026lt;\u0026lt;rmState, tmState, tmPrepared, msgs\u0026gt;\u0026gt; TPNext 是所有七个子动作的析取，其中第 3 行以下的语法等同于：\n\\/ \\E rm \\in RM: TMRcvPrepared(rm) \\/ \\E rm \\in RM: RMPrepare(rm) . . . \\/ \\E rm \\in RM: RMRcvAbortMsg(rm) TPSpec 这是按照之前说过的固定的格式写的。\n4、运行模型检验工具箱 # 按照下图设置运行模型必要的变量和不变式，然后运行模型，应该会得到正确的结果；否则，你应该仔细检查你的 TLA+ 规约。\n另外，除了 TPTypeOK 不变式，我们还可以检查模型的一致性，即两个不同 RM 不会一个选择提交一个选择中止，TPConsistent 不变式如下：\nTPConsistent == \\A rm1, rm2 \\in RM : ~ /\\ rmState[rm1] = \u0026#34;aborted\u0026#34; /\\ rmState[rm2] = \u0026#34;committed\u0026#34; 将 TPConsistent 加入你的规约，并且添加到模型检验工具，运行试试吧！\n5、深入两阶段提交 # 到此为止，我们没有去讨论两阶段提交发生故障时的行为，这方面内容网上也有很多博客进行更进一步地讨论，例如：TM 挂了会怎样，RM 挂了一个会怎样？其实这些内容都可以通过 TLA+ 进行模拟\n亚马逊的高级研究员 MURAT 就写了一篇使用 TLA+ 验证和探讨两阶段提交各种问题的博客：http://muratbuffalo.blogspot.com/2018/12/2-phase-commit-and-beyond.html\nMURAT 在文中修改了 Lamport 的两阶段提交代码，以支持模拟 TM 故障和 RM 故障时的算法情况，并且一步步引出 FLP 不可能和 Paxos 协议。感兴趣的读者可以仔细阅读博客，并且复制他的 TLA+ 代码来自己验证一下，这样你能更深入的理解两阶段提交的局限。\nPaxos, making the world a better place\n下一节，我们就来学习 Paxos 协议的 TLA+ 规约。\n","date":"2022-07-18","permalink":"/posts/202207-tla-4/","section":"Posts","summary":"1、两阶段提交 # 本章要学习的是用在婚姻和数据库领域的分布式算","title":"TLA+ 入门教程（4）：两阶段提交"},{"content":" 1、倒水问题的 TLA+ 描述 # 本节我们来看一个更有趣、更具体的例子，布鲁斯·威利斯主要的电影《虎胆龙威3》中有一个经典的倒水问题，电影中主角需要用一个 3 加仑的桶和一个 5 加仑的桶，准确装出 4 加仑的水（说实话第一次听到这个问题应该是脑筋急转弯的题目，还有某些公司的面试题……）。\n注意：我们必须精确得到 4 加仑的水才能拯救主角，不能用估算的方式，4.1 加仑或 3.9 加仑都会害死我们的主角。\n对于这个问题，我们可以通过 TLA+ 进行抽象，并通过 TLA+ 工具箱运行模型来得出问题的解。\n本节只有一个 TLA+ 语法知识点，即：TLA+ 函数名在使用前必须要被定义，这一点跟大多数编程语言一样。若不熟悉 TLA+ 基础语法的读者请参考：《TLA+入门教程（2）：一个简单的例子》。\n我们还是来定义状态机的三个要素：变量、初始状态和次态关系。\n首先变量就两个，一个 3 加仑的桶和一个 5 加仑的桶，定义如下：\nVARIABLES big, \\* The number of gallons of water in the 5 gallon jug. small \\* The number of gallons of water in the 3 gallon jug. TypeOK == /\\ small \\in 0..3 /\\ big \\in 0..5 TypeOK 部分叫做不变式（invariant），可以理解为断言（assert）变量的值必须在这个范围，模型检验工具在运行时会去检查变量是否满足不变式。\n第二步是初始值，大小桶的初始值都为零。\nInit == /\\ big = 0 /\\ small = 0 最后是次态关系，次态关系可能由一系列动作所产生的，包括：填满小桶（FillSmallJug）、填满大桶（FillBigJug）、倒掉小桶（EmptySmallJug）、倒掉大桶（EmptyBigJug）、将小桶的水倒入大桶（SmallToBig）、将大桶的水倒入小桶（BigToSmall）。\n这里我们只需要把这一系列动作枚举出来，不需要去写具体的流程和逻辑（这个工作我们交给模型检验工具）。所以可以得到次态关系：\nNext == \\/ FillSmallJug \\/ FillBigJug \\/ EmptySmallJug \\/ EmptyBigJug \\/ SmallToBig \\/ BigToSmall 当然，我们需要弄清楚每个实际的动作中的变量的变化，举个例子，填满小桶会让 small' = 3，而大桶不变。\n注意：这里非常重要，按照写代码的思维这里也许不需要管大桶变量 big 的变化，但是按照 TLA+ 模型，每个变量的状态变化都要反映出来。\n于是，我们如下定义次态关系中的动作：\nFillSmallJug == /\\ small\u0026#39; = 3 /\\ big\u0026#39; = big FillBigJug == /\\ big\u0026#39; = 5 /\\ small\u0026#39; = small EmptySmallJug == /\\ small\u0026#39; = 0 /\\ big\u0026#39; = big EmptyBigJug == /\\ big\u0026#39; = 0 /\\ small\u0026#39; = small Min(m,n) == IF m \u0026lt; n THEN m ELSE n SmallToBig == /\\ big\u0026#39; = Min(big + small, 5) /\\ small\u0026#39; = small - (big\u0026#39; - big) BigToSmall == /\\ small\u0026#39; = Min(big + small, 3) /\\ big\u0026#39; = big - (small\u0026#39; - small) 注意 SmallToBig 和 BigToSmall 需要考虑当前大小桶的容量，来决定可以倒入的水量。从小桶倒入大桶时，大桶最多到 5 加仑就不能再倒了；从大桶倒入小桶时，小桶最多到 3 加仑也不能再倒了。\n最后，当我们抽象完系统的变量和状态转移后，TLA+ 中把一个系统表达为一个形如 Spec == Init /\\ [][Next]_vars^L 的时序公式，来描述这个系统的规约（可以看做一个固定的格式）。其中 Init 是初始状态，Next 定义了系统的次态关系，[] 表示“总是”的时序操作符，vars 指系统中的所有变量构成的元组，L 表示系统的公平(fairness)性。这里我们暂且不管 [] 和 L，这并不影响我们理解模型。\nSpec == Init /\\ [][Next]_\u0026lt;\u0026lt;big, small\u0026gt;\u0026gt; 到现在为止，我们还没有确保整个模型可以得到 4 加仑的水。因此，我们还要额外加入一个限制，即：\nNotSolved == big # 4 显然，我们要得到的 4 加仑的水肯定装在 5 加仑的大桶中，所以我们定义 big 不等于 4 这一不变式，意思是，只要出现了 big 等于 4 的情况，模型检验工具就会报错而退出。如果运行模型检验工具一直不会退出，那就说明我们抽象的模型有问题。\n完整的代码参见：https://github.com/tlaplus/Examples/blob/master/specifications/DieHard/DieHard.tla\n下一步，我们来运行模型。\n2、使用 TLA+ 工具箱运行模型 # 我们先新建一个名为 DieHard.tla 的规约，然后将上述地址的完整代码复制进去，接着新建一个模型，在 Model Overview 中进行如下配置。\n现在，让我们拯救主角！点击绿色运行按钮，等一段时间后，如果 TLA+ 代码没问题，检验程序会退出，并在 Error-Trace 一栏显示类似下图的信息。\n观察最后一行，big = 4，与条件冲突，整个检验结束。同时我们也得到了整个状态转换流程，即怎么装水、倒水才能准确得到 4 加仑的水。\n试着运行你的模型，看看能否得出其他不一样的解。\n下一节，我们将学习在婚姻和数据库中经常使用的经典分布式算法的 TLA 规约。\n公平性 L 是系统活性（Liveness）的基础，如果 Server 为某个进程一直提供服务，剩下的进程都会陷入无限等待，这对该进程之外的进程来说就是不公平的。 公平性根据强弱程度分为强公平性(strong fairness)和弱公平性(weak fairness)。具体来说，强公平性要求系统在 enable 时动作能够无限次执行（not enable 时无要求）；弱公平性不管之前如何，系统从某个时间点开始持续 enable，那么动作能够无限次执行。\n参见：Fairness and Liveness：https://www.ccs.neu.edu/home/wahl/Publications/fairness.pdf\n","date":"2022-07-11","permalink":"/posts/202207-tla-3/","section":"Posts","summary":"1、倒水问题的 TLA+ 描述 # 本节我们来看一个更有趣、更具体的例子，","title":"TLA+入门教程（3）：《虎胆龙威3》经典倒水问题"},{"content":"本次 TLA+ 入门教程系列将分为几个部分，帮助你从零掌握 TLA+ 语言的基本知识，欢迎关注公众号和知乎“多颗糖”。\n1、形式化方法 # 上一节我们介绍了形式化方法和 TLA+，Get Your Hands Dirty！让我们小试牛刀，动动手，用 TLA+ 语言对一个简单的程序进行抽象。以下示例来自 Lamport 亲自出的教程1。\n首先，我们有以下简单 C 程序，someNumber() 函数返回一个 0 到 1000 中的随机值，然后对变量 i 自增 +1。\nint i; void main() { // someNumber() 返回 0 到 1000 中的一个数字 i = someNumber(); i = i + 1; } 上一节我们介绍了，TLA+ 通常按照状态机进行抽象，我们可以按照以下步骤进行抽象：\n首先是变量，很显然，这段代码只有一个变量 i。 接着是初始值，也很明显，i 的初始值为 0。 下一个状态的转换关系（也称为“次态关系”）。问题是 i 的下一个状态是什么呢？ 我们假设某次运行程序时 someNumber() 返回 42，那么状态变换就是 [i : 0] → [i : 42] → [i : 43]，对于 i = 42，i 的下一个状态是 i = 43。接下来，程序结束，i 就没有下一个状态了。\n但假设另一种情况， someNumber() 返回 43，那么 i 的下一个状态是 i = 44，和上面的情况冲突，我们得到了不一样的次态关系。\n对于同样的状态 i = 43，下一个状态却有两种不同的可能，我们没法找到一个固定的次态关系。显然，这样建模是行不通的。\n虽然这一代码逻辑非常简单，但抽象为状态机模型却有违我们平常写代码的逻辑。这里的问题在于，我们缺少了对程序控制的抽象，导致我们无法判断 i 的下一个状态是什么。因此，我们需要增加一个变量 pc 来代表控制流状态。还是如上代码，我们定义 pc 的值如下：\nint i; void main() { i = someNumber(); // pc = \u0026#34;start\u0026#34; i = i + 1; // pc = \u0026#34;middle\u0026#34; } // pc = \u0026#34;done\u0026#34; 那么，增加程序控制的变量 pc 后，我们再次抽象为：\n变量： i 和 pc。 初始值是 i = 0 and pc = \u0026quot;start\u0026quot;。 次态关系我们用伪代码表示，如下： if current value of pc equals \u0026#34;start\u0026#34; then next value of i in {0,1,...,1000} next value of pc equals \u0026#34;middle\u0026#34; else if current value of pc equals \u0026#34;middle\u0026#34; then next value of i equals current value of i + 1 next value of pc equals \u0026#34;done\u0026#34; else no next values 翻译过来就是：当 pc = \u0026quot;start\u0026quot; 时，i 的下一个状态是 0 到 1000 的随机值，pc 的下一个状态是 \u0026ldquo;middle\u0026rdquo;；当 pc = \u0026quot;middle\u0026quot; 时，i 的下一个状态 i + 1，pc 的下一个状态是 \u0026ldquo;done\u0026rdquo;；之后结束，没有下一个状态。\n但是这样写的缺点是不够简洁，也不够数学。TLA+ 就是用来描述这类状态机模型的语言。\n我们使用 TLA+ 语言一步步重写上面的伪代码。首先，pc 就意味着 current value of pc（即，pc 的当前状态）这里类似冗余的句子可以删去：\n接着，TLA+ 使用 pc' 表示 next value of pc（即，pc 的下一个状态），使用 i' 表示 next value of i，那么又可以简化为：\n然后，我们用 = 来表示 equals，用集合符号 $\\in$ 来表示 i in {0,1,...,1000}，同时将 {0,1,...,1000} 简写为 0..1000。我们得到：\n这样看起来简洁一些了，但还不够。我们可以用 and 把 i 和 pc 的下一个状态连接起来（即，集合中的“与”），同时使用 /\\ 符号来表示 and 关系。可以得到：\n最后，no next values 我们直接表示为 FALSE 即可，所以有：\n注意，以上代码是以 PDF 格式显示的，但是 TLA+ 源代码是 ASCII 码，其中并没有一些特殊数学符号，所以如果编写 TLA+ 的时候，用 /\\ 和 / 来代替关系“与”和“或”，用 \\in 来代替包含关系，写为：\nIF pc = \u0026#34;start\u0026#34; THEN (i\u0026#39; \\in 0..1000) /\\ (pc\u0026#39; = \u0026#34;middle\u0026#34;) ELSE IF pc = \u0026#34;middle\u0026#34; THEN (i\u0026#39; = i+1) /\\ (pc\u0026#39; = \u0026#34;done\u0026#34;) ELSE FALSE 经常写数学公式的同学会发现，这里的语法其实和 LaTeX 是一样的，因为它们都是 Lamport 发明的。\n现在，我们进一步分析和化简上面的代码。仔细观察，上面的条件判断语句其实可以再简化为：\n( pc = \u0026#34;start\u0026#34; /\\ i\u0026#39; \\in 0..1000 /\\ pc\u0026#39; = \u0026#34;middle\u0026#34;) \\/ ( pc = “middle” /\\ i\u0026#39; = i + 1 /\\ pc\u0026#39; = \u0026#34;done\u0026#34;) 如果你也是个熟练的 if else 工程师，我想你应该能理解为什么可以写成以上形式，这就是一些布尔条件的化简和位置互换。\n我们还可以再进行简写，为了排版美观，我们可以去掉括号，改为在空缺的语句（即第一行的第一列）前面补上同样缩进位置的布尔符号。那么，上面的式子写为：\n\\/ /\\ pc = \u0026#34;start\u0026#34; /\\ i\u0026#39; \\in 0..1000 /\\ pc\u0026#39; = \u0026#34;middle\u0026#34; \\/ /\\ pc = “middle” /\\ i\u0026#39; = i + 1 /\\ pc\u0026#39; = \u0026#34;done\u0026#34; 这样，我们就去掉了括号得到了上面的式子。注意，这里只是我们状态机的最后一部分，即次态关系。我们还要补全状态机的前面部分。完整的 TLA+ 程序如下：\n--------------------------- MODULE SimpleProgram --------------------------- EXTENDS Integers VARIABLES i, pc Init == (pc = \u0026#34;start\u0026#34;) /\\ (i = 0) Pick == \\/ /\\ pc = \u0026#34;start\u0026#34; /\\ i\u0026#39; \\in 0..1000 /\\ pc\u0026#39; = \u0026#34;middle\u0026#34; Add1 == \\/ /\\ pc = \u0026#34;middle\u0026#34; /\\ i\u0026#39; = i + 1 /\\ pc\u0026#39; = \u0026#34;done\u0026#34; Next == Pick \\/ Add1 ============================================================================= 至此，读者想必能够大致理解这行代码想要表述的内容，只需要再补充一些前面没涉及到的内容。让我们从头看一下。\n首先是开头和结尾，每个 TLA+ 程序都包裹在 MODULE xxx 这个块中，这是一个固定格式。当你通过 TLA+ 工具箱创建一个新的规约，开头和结尾这个框应该就已经创建好了。\n其次，EXTENDS Integers 可以理解为 import，就是导入 Integers 这个包，用来处理一些整型。\n接着，VARIABLES 这行即第 0 步，声明 i 和 pc 是变量；Init 这行是 i 和 pc 的初始值。\n最后，Next 这行表示状态机的次态关系，只不过这里将我们前面编写的次态关系拆成两个部分，分为 Pick 和 Add1。\n至此，按照前面的状态机抽象步骤，我们的第一个 TLA+ 程序就写完了。下面我们使用 TLA+ Toolbox 验证一下该模型。\n2、使用 TLA+ 工具箱验证模型 # 首先你需要前往官网下载 TLA+ Toolbox2，或者从 Github 进行下载3。\n打开工具箱，点击 “File -\u0026gt; Open Spec -\u0026gt; Add New Spec\u0026hellip;” 创建一个新的 TLA+ 规约，起名为 SimpleProgram.tla，并保存在你想保存的位置。\n你可以在打开的文件中写入前面学习的规约，或者直接复制。注意，不要超出 MODULE 的范围，否则 IDE 会报错。\n接下来我们点击 “TLC Model Checker -\u0026gt; New Model\u0026hellip;” 创建一个新的模型。不出意外的话会弹出一个模型相关的窗口，我们需要手动填入模型的初始值和次态关系，如下图所示：\n我们点击绿色播放按钮运行模型，你会发现模型报错了。这是因为工具箱默认创建的模型是检查持续运行的系统，而我们的这个模型运行一次就退出了，并不会一直运行下去。仔细观察，在模型视图中有一个 Deadlock 选项被勾上了，我们取消这个选项再运行一次。\n如果你的规约没有问题，应该可以看到模型运行成功。\n总结一下，上一节我们介绍过，TLC 检测工具就是遍历所有的状态，然后对其中的可能路径都进行检查，大致原理就是在遍历整个转换图，同时检查我们的不变式是否成立。\n此外，在 tla 文件的页面，点击 “File -\u0026gt; Produce PDF Version”可以生成 PDF 格式的 TLA+ 代码，如下图所示：\n例如，我们的简单程序导出为 PDF 格式后如下：\nTLA+ 跟 LaTeX 排版工具一样，有源代码 ASCII 码格式，和渲染后的 PDF 格式。\n3、小结 # 通过这个例子，我们可以发现 TLA+ 语言和编程语言的一些不同，TLA+ 语言包括了：变量、控制流、调用栈甚至堆，后面这些内容通常编程语言都会屏蔽掉。除此之外，TLA+ 语言还需要一些集合论、数理逻辑等数学知识。\nTLA+ 难吗？确实有点难，以至于工程师们往往只有在设计很关键、很基础以及难以预测的系统才会去使用它。但 TLA+ 是分布式系统避不开的一环，通过 TLA+ 可以实现：\n正确设计分布式系统，避免改一个 bug，搞出 10 个 bug； 精确理解各种（分布式）算法：Paxos、Raft 都比较复杂，但都给出了 TLA+ 描述，通过学习这些算法的 TLA+ 可以更准确理解算法，验证算法； 不只是文档。有了 TLA+，并发和分布式系统可以以更直观、更准确的方式来表达，如果从一开始设计时就是错的，那么剩下的工作都是徒劳。 本节我们浅尝了一下 TLA+ 语言。下一节，我们将使用 TLA+ 来计算电影《虎胆龙威3》中经典的倒水问题。\n参考资料 # Lamport 亲自出的教程：http://lamport.azurewebsites.net/video/videos.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n下载 TLA+ Toolbox：https://tla.msr-inria.inria.fr/tlatoolbox/products/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n从 Github 下载：https://github.com/tlaplus/tlaplus/releases/tag/v1.7.2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2022-07-06","permalink":"/posts/202207-tla-2/","section":"Posts","summary":"本次 TLA+ 入门教程系列将分为几个部分，帮助你从零掌握 TLA+ 语言的基本","title":"TLA+ 入门教程（2）：一个简单的例子"},{"content":"本次 TLA+ 入门教程系列将分为几个部分，帮助你从零掌握 TLA+ 语言的基本知识，欢迎关注公众号和知乎“多颗糖”。\n前言 # 1960 年代中后期，软件行业开始爆发“软件危机（Software Crisis）”1。所谓“软件危机”，是指在软件开发及维护的过程中所遇到的一系列严重问题，这些问题皆可能导致软件寿命缩短、甚至夭折。随着急速增长的算力和软件越来越复杂，软件开发成为一项难以管理、高风险、高失败率的活动。从本质上来说，软件危机还意味着写出正确、可理解、可验证的程序十分困难。\n软体危机的主要原因，很不客气地说：在没有机器的时候，编程根本不是问题；当我们有了电脑，编程开始变成问题；而现在我们有巨大的电脑，编程就成为了一个同样巨大的问题。—— Edsger Dijkstra 2\n针对“软件危机”，人们提出了种种解决方案，归纳起来基本有两类：\n1、采用工程方法来组织、管理软件的开发过程； 2、深入探讨程序和程序开发过程的规律，建立严密的理论，推演和验证理论的正确性，用来指导软件开发实践。 前者即“软件工程”，而后者则是本文要讨论的“形式化方法”。\n对于任何一个软件，正确性永远摆在第一位。但是，一般来说，软件的正确性难以证明，软件通常会经过层层测试，却依然无法避免错误发生。\n按照我们以往开发软件的经验，首先会先弄个系统设计文档（有时候甚至没有），然后照着需求、流程图或消息结构体等材料开始写代码，最后进行大量的测试去验证功能是否正确。这种情况下往往考验的是经验，因此越有经验的人往往能想到比较多的细节和可能性，设计出来的系统通常越稳定。这当然也取得了一些效果，尤其是业务软件开发，大部分都维持着这样的流程。\n但是对于并发和分布式系统等基础软件，往往具备很高的复杂度，其行为、状态的可能性太多，难以复现，正确性也更难以验证和测试。\n包括图灵奖得主 Leslie Lamport 在内的多个计算机科学家都认为，采用形式化方法（Formal Methods）对系统进行形式化验证和分析，是构建正确、可靠和安全的软件的一个重要途径。\n形式化方法，就是采用数学与逻辑的方法描述和验证系统，包括对系统行为和性质进行描述和验证，可以用一种或多种语言来描述，例如一阶逻辑、高阶逻辑、时态逻辑、状态机、代数、Pi 演算、特殊的程序语言等等。在并发和分布式系统领域，最常用的是由 Leslie Lamport 提出的行为时序逻辑 TLA+ 语言。\n大佬为了让你学习 TLA+，还扮成了小丑\nLeslie Lamport 在并发和分布式系统领域做出开创性贡献，并因此获得图灵奖，我想大家对这位老爷子并不陌生。Lamport 自称是一个理论家(theoretician)，但他理解工程师构建真实系统的需求，因此他自己的研究也是以实用为目标，其中 Paxos 最为著名。\n1、什么是 TLA+ # 用 Lamport 自己的话来说，TLA+ 是一种用于数字系统(digital systems)高级(high-level)建模的语言，并且有相应的工具对模型进行检验。\n这里的关键词是数字系统和 high-level，数字系统包括算法、程序和各种软硬件在内的系统，high-level 意味着是在代码级别之上，位于设计级别进行思考。\nTLA+ 按照官方文档3，应该写作 $TLA^+$，即 + 号在 TLA 的右上方，但是这给网页排版造成了一定困难，因此常常写为 TLA+ 即可。\nTLA+ 将一个系统抽象为一系列离散的事件，我们熟知的第一个数字系统是时钟，它以离散的滴答声模拟时钟；计算机也是数字系统，由一系列离散的程序步骤组成。TLA+ 将离散的事件描述为状态变化，系统中的变量会因为事件从一个状态变为另一个状态——这就是状态机模型。\n为什么要通过状态机的方法来抽象呢？因为绝大多数时候，我们的程序和算法是长期运行的，对于这种一直运行的程序，通过状态机来抽象是一种有效的方式。再加上，状态机在分布式系统有着广泛的应用（例如，RSM），TLA+ 正好是用来描述并发和分布式系统全局状态转换的最佳语言。\n如果我们把系统中的一个状态描述为一个变量，那么状态机可以被描述为：\n系统所有的变量 这些变量的初始值是什么 哪些动作让变量的状态发生改变，这些变量在当前状态下的值与它们在下一个状态下的可能值之间的关系 即，TLA+ 主要描述系统状态的转变关系。记住这个描述方法，我们会按照此方法来编写和阅读所有的 TLA+ 程序。\nTLA+ 如此有用的另一个关键点是，TLA+ 有一个被称为 TLC 的模型检验工具，你可以认为这是 TLA+ 语言的 IDE，其功能简单来说，就是遍历 TLA+ 抽象出来的系统的每一个可能路径，并在每条可能的路径上对一些条件（称为“不变式”）进行检验。为了使 TLC 正确遍历所有可能的路径并进行有效的检验，我们需要告诉它应该去检验什么条件和属性。\n因此，编写 TLA+ 语言的关键就是，抽象系统的变量，描述变量的初始值，变量的转变关系，以及对每个关键路径进行校验。我们可以按照前面描述状态机的 0、1、2 三个步骤来编写 TLA+ 描述，然后进行校验，最后通过模型检验工具进行验证。\n本节作为这一系列的开篇，先不展开介绍具体的语法和工具箱使用方法，这部分内容将在下一节学习。\n为了推广 TLA+ 语言，Lamport 还发明了更为接近编程语言的 PlusCal 语言，PlusCal 不能直接运行，而是转译为 TLA+ 语言后再运行。为了不给读者增加更多心智负担，本系列不讨论 PlusCal 语言。\n2、TLA+ 真的有用吗？ # 最后，Lamport 自己也多次强调，TLA+ 始终不是可以实际运行的生产代码，主要是针对系统中关键部分进行建模并验证，让你从设计层面上就能提前发现问题。\n可见，TLA+ 并非银弹，不仅需要使用者的基础数学、抽象思维和思考能力，并且与实际代码尚有一定距离。\n许多工程师会因此产生质疑，直觉上认为这个东西投入产出效益低，平常写代码说不定连文档和单元测试都没时间写，还要额外写一个 TLA+ 去单纯验证系统，真的值得吗？\n笔者觉得这样的质疑是合理的，毕竟绩效考核都看最终产出，在国内大厂你甚至没有时间去弄这玩意。不过根据笔者的观察，现在国内基础软件创业公司越来越多，TLA+ 的使用也多了起来，笔者找到的有：TiDB4、TDengine5、KingbaseES6 等企业。\n面对这些质疑，Lamport 也找了他在 Intel、Amazon 的熟人，了解他们对于 TLA+ 的使用情况7。实际上 TLA+ 在学术界和工业界都有牛逼的人物和厂商背书，据统计，国外使用 TLA+ 的经典案例有：\nIntel 将 TLA+ 用于工业硬件建模，帮助工程师在实际构建之前进行思考； AWS 从 2011 年开始使用 TLA+，TLA+ 模型检查在 DynamoDB，S3，EBS 和内部分布式锁管理器中均检测出了难以发现的潜在错误，AWS 已经发表了数篇形式化方法的论文; Microsoft 在 Xbox 360，Azure 中都有使用 TLA+，还使用 TLA+ 设计了 Cosmos DB8； 一款以网络为中心的 RTOS 应用案例； 对各种分布式共识算法（Paxos、Raft 和 EPaxos 等）都提供其 TLA+ 验证； 可见，软件形式化方法经过几十年的研究、推广和应用，取得了大量重要成果，形式化方法不仅仅停留在学术界，已经逐渐融入软件开发过程的各个阶段，从需求分析、功能描述（规约）、体系结构、算法设计、编程、测试甚至运维阶段。\n所以，即使你不打算使用 TLA+ 来写点什么，学习 TLA+ 依然有价值，不仅能够让你快速理解别人的算法，还能提供一种新的思考方式，至少在分布式领域，能让你成为一个更好的工程师。\n况且，TLA+ 并不难学习，Amazon 分享的案例显示9，不论是老鸟还是新手，都能在几周内上手 TLA+，是非常值得投资的技术。\n笔者相信，跟着本次 TLA+ 系列，你甚至不需要几周时间，就可以入门 TLA+，从而帮助你对系统行为进行清晰思考（科学视角），看懂 Paxos 和 Raft 算法的 TLA+ 描述，验证分布式协议的正确性。\n下一节，让我们从一个最简单的例子开始。\n参考引用 # 软件危机（Software Crisis）：https://en.wikipedia.org/wiki/Software_crisis\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDijkstra, E. W. The Humble Programmer. Communications of the ACM. Aug 1972, 15 (10): 859–866. doi:10.1145/355604.361591.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTLA+ 官方主页：https://lamport.azurewebsites.net/tla/tla.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTiDB TLA+ 规约：https://github.com/pingcap/tla-plus\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTDengine 中有使用 Raft 的 TLA+ 规约：https://www.taosdata.com/techtalk/8976.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKingbaseES 使用TLA+和PlusCal增强产品的可靠性：https://bbs.kingbase.com.cn/thread-753-1-1.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIndustrial Use of TLA+： https://lamport.azurewebsites.net/tla/industrial-use.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHigh-level TLA+ specifications for the five consistency levels offered by Azure Cosmos DB：https://github.com/Azure/azure-cosmos-tla\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHow Amazon Web Services Uses Formal Methods, Chris Newcombe, Tim Rath, Fan Zhang, Bogdan\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2022-07-04","permalink":"/posts/202207-tla-1/","section":"Posts","summary":"本次 TLA+ 入门教程系列将分为几个部分，帮助你从零掌握 TLA+ 语言的基本","title":"TLA+ 入门教程（1）：形式化方法简介"},{"content":"","date":"2021-03-06","permalink":"/tags/c++/","section":"Tags","summary":"","title":"C++"},{"content":"","date":"2021-03-06","permalink":"/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/","section":"Categories","summary":"","title":"编程语言"},{"content":"C++20 带着 Coroutines 来了！\n花了一两周的时间后，我想写写 C++20 协程的基本用法，因为 C++ 的协程让我感到很奇怪，写一个协程程序十分费劲。让我们抛去复杂的东西，来看看写一个 C++ 协程需要哪些东西。\n编译器支持 # 由于 C++ 20 还没被所有编译器完全支持，首先需要确保你的编译器实现了 Coroutines，可以通过下面的网站查看编译器支持情况：https://en.cppreference.com/w/cpp/compiler_support#cpp20\n值得一提，我使用的 MacOS 自带的 Apple Clang 对 C++20 支持很弱，我选择通过 Homebrew 安装最新版的 GNU GCC (10 以上版本)来编译。\n我使用的 GNU GCC 10.2 版本编译指令：\ng++ -fcoroutines -std=c++20 Clang 支持不够好，不推荐使用。Clang 可以使用如下命令编译：\nclang++ -std=c++20 -stdlib=libc++ -fcoroutines-ts 不推荐 Clang 还有一个理由：使用 Clang 需要 include 头文件 \u0026lt;experimental/coroutine\u0026gt; 而不是 \u0026lt;coroutine\u0026gt;。此外，一些类型被命名为 std::experimental:xxx 而不是 std:xxx。\n以下示例代码只支持 GNU GCC 版本的编译器。\nC++ 协程简介 # 在正式开始之前，我们先要理解 C++20 中协程使用的一些术语。#\n首先，什么是协程？\n协程就是一个可以**挂起(suspend)和恢复(resume)**的函数(但无论如何不能是 main 函数)。你可以暂停协程的执行，去做其他事情，然后在适当的时候恢复到暂停的位置继续执行。协程让我们使用同步方式写异步代码。\n怎么挂起协程呢？C++ 提供了三个方法：co_await, co_yield 和 co_return。\n顺便说一句：coroutine 不是并行(parallelism)，和 Go 语言的 goroutine 不一样！\n与你之前接触到的协程完全不同，一个 C++ 协程一般长这样：\nstruct ReturnObject { struct promise_type { ReturnObject get_return_object() { return {}; } std::suspend_never initial_suspend() { return {}; } std::suspend_never final_suspend() { return {}; } void unhandled_exception() {} }; }; struct Awaitable { std::coroutine_handle\u0026lt;\u0026gt; *hp_; constexpr bool await_ready() const noexcept { return false; } void await_suspend(std::coroutine_handle\u0026lt;\u0026gt; h) { *hp_ = h; } void await_resume() noexcept { std::cout \u0026lt;\u0026lt; \u0026#34;Event signaled, resuming.\u0026#34; \u0026lt;\u0026lt; std::endl; } }; ReturnObject counter(std::coroutine_handle\u0026lt;\u0026gt; *continuation_out) { Awaitable a{continuation_out}; for (unsigned i = 0;; ++i) { co_await a; std::cout \u0026lt;\u0026lt; \u0026#34;counter: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; } } 这奇怪的协程代码涉及了 C++ 协程很重要的三个概念：\npromise_type Awaitable std::coroutine_handle\u0026lt;\u0026gt; 在写 C++20 的协程之前，我们必须需要先了解三个概念，可以用这三张图来形容这三个概念：\n图来源: https://www.youtube.com/watch?v=vzC2iRfO_H8\nPromise # C++ 协程的返回类型必须是 promise_type，promise_type 是一个 interface，你可以用它来控制协程，在协程的生命周期中注入自定义行为：\nget_return_object： 控制协程的返回对象 initial_suspend：在协程开始的时候挂起 final_suspend：在协程结束的时候挂起 协程的生命周期如下，用户自定义的函数 \u0026lt;function-body\u0026gt; 被包裹在下面的伪代码中(来源：http://eel.is/c++draft/dcl.fct.def.coroutine#5)：\n{ promise-type promise promise-constructor-arguments ; try { co_await promise.initial_suspend() ; \u0026lt;function-body\u0026gt; } catch ( ... ) { if (!initial-await-resume-called) throw ; promise.unhandled_exception() ; } final-suspend : co_await promise.final_suspend() ; } // coroutine 被销毁 可以看到，initial_suspend 会在进入协程(也就是函数)之前执行，final_suspend 会在协程返回之前执行。\n如果 final_suspend 真的挂起了协程，那么作为协程的调用者，你需要手动的调用 destroy 来释放协程；如果 final_suspend 没有挂起协程，那么协程将自动销毁。先记住这句话，在后面还会提到。\n除此之外，Promise 还有一些其它责任：\nreturn_void()/return_value()/yield_value() 方法: 用来控制 co_return 和 co_yield 的行为； unhandled_exception() 处理异常 创建和销毁协程的 stackframe 处理 stackframe 创建可能发生的异常 stackframe ：函数运行时占用的内存空间，是栈上的数据集合，它包括：\nLocal variables Saved copies of registers modified by subprograms that could need restoration Argument parameters Return address Awaitable # 第二个概念是 Awaitable，Awaitable 负责管理协程挂起时的行为。\n一个 Awaitable 对象可以成为 co_await 调用的对象\u0010\u0010。Awaitable 拥有以下方法：\nawait_ready()：是否要挂起，如果返回 true，那么 co_await 就不会挂起函数； await_resume()：co_await 的返回值，通常返回空； await_suspend()：协程挂起时的行为； 可以在 await_suspend 中实现 await_ready 的效果，例如直接不挂起当前的协程，但在调用 await_suspend 之前，编译器必须将所有状态捆绑到协程的 stackframe 中，这会更耗时。\n有时候我们的协程并不需要自定义复杂的行为，C++ 提供了两个默认的 Awaitable：\nnamespace std { struct suspend_never { constexpr bool await_ready() const noexcept { return true; } constexpr void await_suspend(coroutine_handle\u0026lt;\u0026gt;) const noexcept {} constexpr void await_resume() const noexcept {} }; struct suspend_always { constexpr bool await_ready() const noexcept { return false; } constexpr void await_suspend(coroutine_handle\u0026lt;\u0026gt;) const noexcept {} constexpr void await_resume() const noexcept {} }; } suspend_always::await_ready() 总是返回 false，而 suspend_always::await_ready() 总是返回 true。其他的方法都是空的，没有任何作用。\n如果没有其它多余的行为，我们可以在函数中直接调用 co_await std::suspend_always{} 来挂起一个函数。\nCoroutine Handle # co_await 挂起函数，并创建了一个可调用对象，这个对象可以用来恢复Hanns乎的执行。这个可调用对象的类型就是 std::coroutine_handle\u0026lt;\u0026gt;，最常用的两个方法是：\nhandle.resume()：恢复协程的执行； handle.destroy()：销毁协程； Coroutine Handle 很像指针，我们可以复制它，但析构函数不会释放相关状态的内存。为了避免内存泄漏，一般要调用 handle.destroy() 来释放（尽管在某些情况下，协程会在完成后自行销毁——前文有提到）。同样像指针一样，一旦销毁了一个 Coroutine Handle ，指向同一个协程的另一个 Coroutine Handle 将指向垃圾，并在调用时表现出未定义行为。\n学习更复杂的用法之前，我们先看下示例。\n示例 # #include \u0026lt;coroutine\u0026gt; #include \u0026lt;iostream\u0026gt; struct HelloCoroutine { struct HelloPromise { HelloCoroutine get_return_object() { return std::coroutine_handle\u0026lt;HelloPromise\u0026gt;::from_promise(*this); } std::suspend_never initial_suspend() { return {}; } // 在 final_suspend() 挂起了协程，所以要手动 destroy std::suspend_always final_suspend() { return {}; } void unhandled_exception() {} }; using promise_type = HelloPromise; HelloCoroutine(std::coroutine_handle\u0026lt;HelloPromise\u0026gt; h) : handle(h) {} std::coroutine_handle\u0026lt;HelloPromise\u0026gt; handle; }; HelloCoroutine hello() { std::cout \u0026lt;\u0026lt; \u0026#34;Hello \u0026#34; \u0026lt;\u0026lt; std::endl; co_await std::suspend_always{}; std::cout \u0026lt;\u0026lt; \u0026#34;world!\u0026#34; \u0026lt;\u0026lt; std::endl; } int main() { HelloCoroutine coro = hello(); std::cout \u0026lt;\u0026lt; \u0026#34;calling resume\u0026#34; \u0026lt;\u0026lt; std::endl; coro.handle.resume(); std::cout \u0026lt;\u0026lt; \u0026#34;destroy\u0026#34; \u0026lt;\u0026lt; std::endl; coro.handle.destroy(); return 0; } 这个简短的示例展示了 C++ 实现协程 \u0026ldquo;Hello world\u0026rdquo; 程序。我们执行完 \u0026ldquo;Hello \u0026quot; 后挂起函数，又在执行 handle.resume() 后恢复函数的运行。\n非常简单，不再过多解释。\nco_yield # C++ 协程与一个 Promise 交互之所以如此笨拙，有一个特殊原因就是为了 co_yield。\n如果 promise 是当前协程的 Promise 对象，那么执行：\nco_yield \u0026lt;expression\u0026gt;; 相当于执行了：\nco_await promise.yield_value(\u0026lt;expression\u0026gt;); 所以，需要在 promise_type 中添加一个 yield_value 方法。上面的例子可以改为：\n#include \u0026lt;coroutine\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;string_view\u0026gt; struct HelloCoroutine { struct HelloPromise { std::string_view value_; HelloCoroutine get_return_object() { return std::coroutine_handle\u0026lt;HelloPromise\u0026gt;::from_promise(*this); } std::suspend_never initial_suspend() { return {}; } std::suspend_always final_suspend() { return {}; } void unhandled_exception() {} std::suspend_always yield_value(std::string_view value) { value_ = value; std::cout \u0026lt;\u0026lt; value_ \u0026lt;\u0026lt; std::endl; return {}; } }; using promise_type = HelloPromise; HelloCoroutine(std::coroutine_handle\u0026lt;HelloPromise\u0026gt; h) : handle(h) {} std::coroutine_handle\u0026lt;HelloPromise\u0026gt; handle; }; HelloCoroutine hello() { std::string_view s = \u0026#34;Hello \u0026#34;; co_yield s; std::cout \u0026lt;\u0026lt; \u0026#34;world\u0026#34; \u0026lt;\u0026lt; std::endl; } int main() { HelloCoroutine coro = hello(); std::cout \u0026lt;\u0026lt; \u0026#34;calling resume\u0026#34; \u0026lt;\u0026lt; std::endl; coro.handle.resume(); std::cout \u0026lt;\u0026lt; \u0026#34;destroy\u0026#34; \u0026lt;\u0026lt; std::endl; coro.handle.destroy(); return 0; } 可以用 co_yield 实现 Python 中的生成器，参考：https://lewissbaker.github.io/2018/09/05/understanding-the-promise-type\nco_return # 执行 co_return 语句时：\nco_return \u0026lt;expression\u0026gt;; 相当于执行了：\nco_return promise.return_value(\u0026lt;expression\u0026gt;); goto end; 下面再给出示例加上 co_return 的版本：\n#include \u0026lt;coroutine\u0026gt; #include \u0026lt;iostream\u0026gt; struct HelloCoroutine { struct HelloPromise { HelloCoroutine get_return_object() { return std::coroutine_handle\u0026lt;HelloPromise\u0026gt;::from_promise(*this); } std::suspend_never initial_suspend() { return {}; } std::suspend_always final_suspend() { return {}; } void unhandled_exception() {} void return_value(int value) { std::cout \u0026lt;\u0026lt; \u0026#34;got co_return value \u0026#34; \u0026lt;\u0026lt; value \u0026lt;\u0026lt; std::endl; } }; using promise_type = HelloPromise; HelloCoroutine(std::coroutine_handle\u0026lt;HelloPromise\u0026gt; h) : handle(h) {} std::coroutine_handle\u0026lt;HelloPromise\u0026gt; handle; }; HelloCoroutine hello() { std::cout \u0026lt;\u0026lt; \u0026#34;Hello \u0026#34; \u0026lt;\u0026lt; std::endl; co_await std::suspend_always{}; std::cout \u0026lt;\u0026lt; \u0026#34;world!\u0026#34; \u0026lt;\u0026lt; std::endl; co_return 42; } int main() { HelloCoroutine coro = hello(); std::cout \u0026lt;\u0026lt; \u0026#34;calling resume\u0026#34; \u0026lt;\u0026lt; std::endl; coro.handle.resume(); std::cout \u0026lt;\u0026lt; \u0026#34;destroy\u0026#34; \u0026lt;\u0026lt; std::endl; coro.handle.destroy(); return 0; } 复杂一些 # 到此， Awaitable 和 Coroutine Handle 好像还没有发挥什么作用，我写的示例程序都非常简单。\n如果我们想在协程挂起的时候，做更多的动作，一般将 Coroutine Handle 传到 Awaitable 的 await_suspend() 中，用一个官网的例子展示一下：\n#include \u0026lt;coroutine\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;stdexcept\u0026gt; #include \u0026lt;thread\u0026gt; auto switch_to_new_thread(std::jthread\u0026amp; out) { struct awaitable { std::jthread* p_out; bool await_ready() { return false; } void await_suspend(std::coroutine_handle\u0026lt;\u0026gt; h) { std::jthread\u0026amp; out = *p_out; if (out.joinable()) throw std::runtime_error(\u0026#34;Output jthread parameter not empty\u0026#34;); out = std::jthread([h] { h.resume(); }); // Potential undefined behavior: accessing potentially destroyed *this // std::cout \u0026lt;\u0026lt; \u0026#34;New thread ID: \u0026#34; \u0026lt;\u0026lt; p_out-\u0026gt;get_id() \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; std::cout \u0026lt;\u0026lt; \u0026#34;New thread ID: \u0026#34; \u0026lt;\u0026lt; out.get_id() \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // this is OK } void await_resume() {} }; return awaitable{\u0026amp;out}; } struct task{ struct promise_type { task get_return_object() { return {}; } std::suspend_never initial_suspend() { return {}; } std::suspend_never final_suspend() noexcept { return {}; } void return_void() {} void unhandled_exception() {} }; }; task resuming_on_new_thread(std::jthread\u0026amp; out) { std::cout \u0026lt;\u0026lt; \u0026#34;Coroutine started on thread: \u0026#34; \u0026lt;\u0026lt; std::this_thread::get_id() \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; co_await switch_to_new_thread(out); // awaiter destroyed here std::cout \u0026lt;\u0026lt; \u0026#34;Coroutine resumed on thread: \u0026#34; \u0026lt;\u0026lt; std::this_thread::get_id() \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } int main() { std::jthread out; resuming_on_new_thread(out); } 小结 # 本文简单介绍了 C++ 协程，希望下次你写 C++ 协程的时候，首先想到这三个东西：\n我不是编程语言专家，对于 C++ 也没有很深入的研究，C++ 在万众期待下终于支持了协程，但用了一下发现，C++ 的协程显得有些\u0010繁琐、怪异，或许是我不清楚 C++ 在原有情况下支持协程的困难，但我依然觉得 C++ 团队可以做得更好。\n我本人确实还没明白到底该如何在项目中使用这臃肿的协程。\n不过，可以预见到的是，我们会在越来越多的 C++ 项目中看到协程的身影。比如 facebook folly 就已经实现了一个实验阶段的协程框架: https://github.com/facebook/folly/tree/master/folly/experimental/coro\n也许等我再研究一段时间，会写一篇到底该如何使用 C++ 协程。\nReference # C++ Coroutine definitions: http://eel.is/c++draft/dcl.fct.def.coroutine#5 C++ draft expr.await: http://eel.is/c++draft/expr.await C++ Coroutines: Understanding the promise type: https://lewissbaker.github.io/2018/09/05/understanding-the-promise-type 官网的例子：https://en.cppreference.com/w/cpp/language/coroutines My tutorial and take on C++20 coroutines：https://www.scs.stanford.edu/~dm/blog/c++-coroutines.html#coroutine-handles ","date":"2021-03-06","permalink":"/posts/202103-cpp-coroutine/","section":"Posts","summary":"C++20 带着 Coroutines 来了！ 花了一两周的时间后，我想写写 C++20 协程的基本用法，","title":"如何编写 C++ 20 协程(Coroutines)"},{"content":"","date":"2021-02-19","permalink":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F/","section":"Tags","summary":"","title":"分布式"},{"content":"","date":"2021-02-19","permalink":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/","section":"Categories","summary":"","title":"计算机科学"},{"content":"在上篇《条分缕析 Raft 算法》中推导和梳理了 Raft 算法，但仍有一些细节没有包含到，这篇文章作为补充。\n1 日志压缩 # 随着时间推移，存储的日志会越来越多，不但占据很多磁盘空间，服务器重启做日志重放也需要更多的时间。如果没有办法来压缩日志，将会导致可用性问题：要么磁盘空间被耗尽，要么花费太长时间启动。所以日志压缩是必要的。\n日志压缩的一般思路是，日志中的许多信息随着时间推移会变成过时的，可以丢弃。例如：一个将 x 设置为 2 的操作，如果在未来将 x 设置为了 3，那么 x=2 这个操作就过时了，可以丢弃。\n一旦日志记录被提交并应用于状态机，那么用于到达当前状态的中间状态和操作就不再需要了，它们可以被压缩掉。\n和配置变化不同，不同的系统有不同的日志压缩方式，取决于你的性能考量，以及基于硬盘还是基于内存。日志压缩的大部分责任都落在状态机上。\n不同的压缩方法有几个核心的共同点：\n不将压缩决定集中在 Leader 上，每个服务器独立地压缩其已提交的日志。这就避免了 Leader 将日志传递给已有该日志的 Follower，同时也增强了模块化，减少交互，将整个系统的复杂性最小化。（对于非常小的状态机，基于 Leader 的日志压缩也许更好。） 将之前的 log 的维护责任从 Raft 转移到状态机。Raft 要保存最后被丢弃的记录的index和term，用于 AppendEntries RPC 一致性检查。同时，也需要保存最新的配置信息：成员变更失败需要回退配置，最近的配置必须保存。 一旦丢弃了前面部分的日志，状态机就承担两个新的责任：1. 如果服务器重启了，需要将最新的快照加载到状态机后再接受 log\u0010\u0010\u0010\u0010；此外，2. 需要向较慢的 follower(日志远落后于 Leader)发送一致的状态镜像。 1.1 基于内存的状态机的快照 # 状态机的数据集小于 10GB 的时候选择 memory-based 状态机是合理的。\n上图显示了 memory-based 状态机的基本想法：对内存的数据结构(树形或哈希等)进行序列化并存储，同时存储 Raft 重启需要的状态：最后一条记录的 index 和 term 以及该索引处的最新配置，然后这个 index 之前的日志和快照都可以丢弃了。\nmemory-based 状态机的快照的大部分工作是序列化内存中的数据结构。\n通过上面的介绍，Leader 可能偶尔需要把它的状态发送给慢 Followers 或新加入集群的服务器。快照信息通过 InstallSnapshot RPC 来传输。你肯定在论文中看过下图：\n1.1.1 快照的并发性 # 创建一个快照需要耗费很长时间，包括序列化和写入磁盘。\n例如，在今天的服务器上拷贝 10GB 的内存花费大约1秒钟，序列化它通常将花费更长的时间：即使 SSD 1秒钟也仅能写入大约 100MB。\n因此，序列化和写快照都要与常规操作并发进行，避免服务不可用。\ncopy-on-write 技术允许进行新的更新而不影响写快照。有两个方法来实现：\n状态机可以用不可变的(immutable)数据结构来实现。因为状态机命令不会 in-place 的方式来修改状态(通常使用追加的方式)，快照任务可以引用之前状态的并把状态一致地写入到快照。 另外，也可以使用操作系统的 copy-on-write。例如，在 Linux 上可以使用 fork 来复制父进程的整个地址空间，然后子进程就可以把状态机的状态写出并退出，整个过程中父进程都可以持续地提供服务。LogCabin中当前使用的就是这种方法。 1.1.2 何时做快照 # 服务器需要决定什么时候做快照。太过频繁地做快照，将会浪费磁盘带宽和其他资源；太不频繁地做快照，则有存储空间耗尽的风险，并且重启服务需要更长的重放日志时间。\n一个简单的策略是设置一个阈值，当日志大小超过阈值则做快照。然而，这会导致对于小型状态机时有着不必要的大日志。\n一个更好的方法是引入快照大小和日志大小的对比，如果日志超过快照好几倍，可能就需要做快照。但是在做快照之前计算快照的大小是困难并且繁重的，会引入额外负担。所以使用前一个快照的大小是比较合理的行为，一旦日志大小超过之前的快照的大小乘以扩展因子(expansion factor)，服务器就做快照。\n这个扩展因子权衡空间和带宽利用率。例如，扩展因子为 4 的话会有 20% 的带宽用于快照(每1byte 的快照写入有对应的 4bytes 的 log 写入)和大约 6 倍的硬盘空间使用(旧的快照+日志+新的快照)。\n快照仍然会导致 CPU 和磁盘的占用率突发，可以增加额外的磁盘来减轻该现象。\n同时，可以通过调度使得做快照对客户端请求没有影响。服务器需要协调保证在某一时刻集群只有小部分成员集同时在做快照。由于 Raft 是多数派成员构成的 commit，所以这样就不会影响请求的提交了。当 Leader 想做快照的时候，首先要先下台，让其他服务器选出另一个 Leader 接替工作。如果这个方法充分地可行，就可能消除快照的并发，服务器在快照期间其实是不可用的(这可能会造成集群的容错能力降低的问题)。这是一个令人兴奋的提升集群性能并降低实现机制的机会。（这里其实可以通过实现指定服务器做快照来优化，braft 里就有提到这点。）\n1.1.3 实现的关注点 # 这一节回顾快照的主要组件的实现并讨论实现的难点：\n保存和加载快照：保存快照需要对其序列化并写入磁盘，而加载则是反序列化。通过流式接口(streaming interface)可以避免将整个快照缓冲到内存中。可能对流进行压缩并附带一个 checksum 比较好。LogCabin 先把快照写入一个临时文件，当写完并且刷到磁盘后，再把文件改名。这是为了避免server启动的时候加载到部分的快照。 传输快照：传输快照牵涉到如何实现 InstallSnapshot RPC。传输的性能通常不是非常重要(一个需要这种动作的 Follower 不会参与到日志的 commit 决策中，因此不需要立即完成)。 消除不安全的日志访问和丢弃日志条目：最初设计 LogCabin 的时候没有考虑日志压缩，因此代码上假定了如果 entry i 在日志中，那么 entry 1 到 i - 1 也一定在日志中。有了日志压缩，这就不再成立了，前面的 entry 可能已经被丢弃了。这里需要仔细推理和测试。可能对一些强类型的系统做这些是简单的，编译器会强制检查日志访问并处理越界的问题。一旦我们使得所有的日志访问都是安全的，丢弃前面的日志就很直接了。在这之前，我们都只能单独地测试保存、加载和传输快照。 通过 copy-on-write 并发地做快照：可能需要重新设计状态机或利用操作系统的 fork。LogCabin 当前使用的是 fork，相比于线程交互性很差，要使其正确工作也有一定的难度。然而，它的代码量很小，而且不需要修改状态机数据结构。 决定何时做快照：我们建议在开发的过程中每应用一条日志就做一个快照，这样便于快速定位问题。一旦实现完成，就需要增加一个更有效的机制选择什么时候做快照。 1.2 基于磁盘的状态机的快照 # 对于几十或上百 GB 的状态机，需要使用磁盘作为主要存储。对于每一条记录，当其被提交并应用到状态机后，其实就可以被丢弃了，因为磁盘已经持久化存储了，可以理解为每条日志就做了一个快照。\nDisk-based 状态机的主要问题是，磁盘会导致性能不佳。在没有写缓冲的情况下，每应用一条命了都需要进行一次或多次随机磁盘写入，这会限制系统的整体吞吐量。\nDisk-based 状态机仍然需要支持向日志落后的 Follower 提供最新的快照，而写快照也要继续提供服务，所以仍然需要 copy-on-write 技术以在一定期间内保持一个一致地快照传输。幸运的是，磁盘总是被划分为逻辑块，因此在状态机中实现应该是直接的。基于磁盘的状态机也可以依靠操作系统的支持，例如 Linux 的 LVM 也可以用来创建快照。\n1.2.1 增量清理的方法 # 增量的方法做压缩如 log cleaning 或 LSM tree，是可能的。他们快照的实现会更复杂，但有如下优点：\n每次只操作数据的一部分，所以压缩的负载随着时间来看是均匀的。 写入磁盘的效率更高。它们使用大范围的、连续的写入。递增清理的方法可以有选择的压缩磁盘中拥有最多可重复使用空间的部分，可以写入更少的数据。 传递快照更为简单，因为它们不会 in-place 地修改磁盘的区域。 1.2.2 Log cleaning # 来自于 The Design and Implementation of a Log-Structured File System。\nLog cleaning 写入时直接追加，日志被切分为多个连续的 Segments。每一个 segment 通过以下三个步骤进行压缩：\n首先选择要清理的段，这些段累积了大量废弃的记录； 把有效的记录(live entry)从那些段中拷贝到日志的开头 释放那些段的空间 为了最小化对正常操作的影响，这个过程应该并发地做。\n由于将有效的记录转存到日志的头部，日志出现乱序，可以包含附加的信息(比如 version number)以在日志应用的时候重建正确的顺序。\n选择哪些段做清理的策略对性能有非常大的影响。Log cleaning 建立了一个模型，不仅考虑live entry 的占比，同时考虑这些 entry 会存活多长时间。但不幸的是，每个状态机的 live entry 会有所不同。\n1.2.3 LSM tree # LSM tree 由于 BigTable 的提出被广泛使用。\nLSM tree 是树型的数据结构，存储有序的键值对。在高层次上和 Log cleaning 一样：大的顺序写并且不 in-place 地修改磁盘上的数据。。然而，LSM tree 并没有在日志中维护所有状态，而是重新组织状态以便更好地进行随机读。\n典型的 LSM tree 将最近的写入在磁盘上保持一份小的 log。当 log 达到一定的大小后，对 key 进行排序并且写入一个叫做 run 的文件中。Runs 不会 in-place 修改，但是会周期性地对 runs 进行 merge，产生新的 runs 并丢弃旧的，merge 的过程像 merge sort。\n在正常操作期间，状态机可以直接在这些数据上操作。对于读一个 key 来说，首先检查是否在最近的 log 中有修改，之后检查每一个 run。为了避免对每一个 run 做 key 的检查，一些系统对每一个 run 创建了 bloom filter。\n1.2.4 Raft 中的 Log cleaning 和 LSM tree # LogCabin 还未实现 Log cleaning 或 LSM tree，把 LSM tree 应用到 Raft 是直截了当的，因为 Raft 日志已经将最近的记录持久地存储在磁盘上，LSM tree 可以将最近的数据以更方便的树型保存在内存中，这将提高查找的性能。并且当 Raft 日志达到指定大小的时候，这个树按顺序写到磁盘作为一个新的 run。传输状态的时候 Leader 需要把所有的 run 发送给Follower(不包含内存中的树)；幸运的是，runs 都是不可变的，所以不必担心传输过程中 runs 被修改。\n把 Log cleaning 应用到 Raft 就不是这么明显了。\n1.3 其它日志压缩 # 略。\n2 性能优化 # 2.1 Writing to the leader’s disk in parallel # 在前面的实现中，Leader 将日志写到磁盘后，再将该日志复制到它的 Follower，然后等待 Follower 将该日志写到他们的磁盘上。这里出现了两次连续的磁盘写入等待，这将导致显著的延迟。\nLeader 可以在向 Follower 并行复制日志的同时写入自己的磁盘，如图：\na 是没有并行优化的，而 b 是进行并行优化的。\n如果多数派 Follower 已经写入磁盘，Leader 甚至可以在该记录写入自己的磁盘之前就提交，这仍然是安全的。\n2.2 Batch 和 Pipeline # Raft 支持 Batch 和 Pipeline，这两者对性能提升都很重要。\nBatch：Leader 可以一次收集多个客户端 requests，然后一批发送给 Follower。当然，我们也需要有一个最大发送 size 来限制每次最多可以发送多少数据，LogCabin 使用 1M 大小。 Pipeline：如果只是用 batch，Leader 还是需要等待 Follower 返回才能继续后面的流程，我们这里还可以使用 Pipeline 来进行加速。Leader 会维护一个 nextIndex 的变量来表示下一个给 Follower 发送的 log 位置，通常情况下，只要 Leader 跟 Follower 建立起了连接，我们都会认为网络是稳定互通的。所以当 Leader 给 Follower 发送了一批 log 之后，它可以直接更新 nextIndex，并且立刻发送后面的 log，不需要等待 Follower 的返回。如果网络出现了错误，或者 Follower 返回一些错误，Leader 就重新调整 nextIndex，然后重新发送 log。 AppendEntries RPC 一致性检查保证了 pipeline 的安全性，但是，如果 RPC 失败/超时了，Leader 就要将 nextIndex 递减回到初始值重来。如果 AppendEntries RPC 一致性检查还是失败，Leader 可能进一步递减 nextIndex 重试发送前一个记录，或者等待前一个记录被确认。\n最初的线程架构阻碍了 pipeline，因为它只能支持每个 Follower 一个 RPC。这里 Leader 必须多线程地与一个 Follower 建立多个连接。\n如果 Leader 与一个 Follower 共用一个连接使用 pipeline 的话, 那么效果会是怎样的呢?其实这样和 Batch 没有多大区别，tcp 层面已经是串行的了，tcp 有滑动窗口来做 batch，同时单条连接保证了消息很少会乱序。\n那么，如果使用多线程连接的话可能存在什么问题？即使因为在多个连接中不能保证有序，但是大部分情况还是先发送的先到达；即使后发送的先到达了，由于有 AppendEntries RPC 一致性检查的存在，后发送的自然会失败，失败后重试即可。\nRaft 系统的整体性能在很大程度上取决于如何安排 batch 和 pipeline。如果在高负载的情况下，一个 batch 中积累的请求数量不够，整体处理效率就会很低，导致低吞吐量和高延迟。另一方面，如果在一个 batch 中积累了太多的请求，延迟将不必要地变高，因为早期的请求要等待后来的请求到达。\n2.3 pre-vote # 网络分区会导致某个节点的数据与集群最新数据差距拉大，但是 term 因为不断尝试选主而变得很大。网络恢复之后，Leader 向其进行日志复制时，就会导致 Leader 因为 term 较小而下台。这种情况可以引入 pre-vote 来避免。Follower 在转变为 Candidate 之前，先与集群节点通信，获得集群 Leader 是否存活的信息，如果当前集群有 Leader 存活，Follower 就不会转变为 Candidate，也不会增加term。\n2.4 MultiRaft # 来自 CockroachDB 的优化：https://www.cockroachlabs.com/blog/scaling-RAFT/\nRaft 的 Leader 向 Follower 的心跳间隔一般都较小，在 100ms 粒度，当复制实例数较多的时候，心跳包的数量就呈指数增长。如图：\n这里将复制组之间的心跳合并到节点之间的心跳。如图：\nbraft 提供了静默模式：通常复制组不需要频繁的切换 Leader，我们可以将主动 Leader Election 的功能关闭，这样就不需要维护 Leader Lease 的心跳了。复制组依靠业务 Master 进行被动触发 Leader Election，这个可以只在 Leader 节点宕机时触发，整体的心跳数就从复制实例数降为节点数。\nReference # CONSENSUS BRIDGING THEORY AND PRACTICE: https://web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf\n理论基础 · Raft phd 论文中的pipeline 优化: http://mysql.taobao.org/monthly/2019/03/08/\nTiKV 源码解析系列 - Raft 的优化: https://zhuanlan.zhihu.com/p/25735592\nScaling Raft: https://www.cockroachlabs.com/blog/scaling-RAFT/\nRAFT介绍: https://github.com/baidu/braft/blob/master/docs/cn/raft_protocol.md\n相关阅读 # ","date":"2021-02-19","permalink":"/posts/202102-raft-extension/","section":"Posts","summary":"在上篇《条分缕析 Raft 算法》中推导和梳理了 Raft 算法，但仍有一些细节","title":"条分缕析 Raft 算法(续)：日志压缩和性能优化"},{"content":"本文整理自 Ongaro 在 Youtube 上的视频。\n目标 # Raft 的目标（或者说是分布式共识算法的目标）是：保证 log 完全相同地复制到多台服务器上。\n只要每台服务器的日志相同，那么，在不同服务器上的状态机以相同顺序从日志中执行相同的命令，将会产生相同的结果。\n共识算法的工作就是管理这些日志。\n系统模型 # 我们假设：\n服务器可能会宕机、会停止运行过段时间再恢复，但是非拜占庭的（即它的行为是非恶意的，不会篡改数据等）； 网络通信会中断，消息可能会丢失、延迟或乱序；可能会网络分区； Raft 是基于 Leader 的共识算法，故主要考虑：\nLeader 正常运行 Leader 故障，必须选出新的 Leader 优点：只有一个 Leader，简单。\n难点：Leader 发生改变时，可能会使系统处于不一致的状态，因此，下一任 Leader 必须进行清理；\n我们将从 6 个部分解释 Raft：\nLeader 选举； 正常运行：日志复制（最简单的部分）； Leader 变更时的安全性和一致性（最棘手、最关键的部分）； 处理旧 Leader：旧的 Leader 并没有真的下线怎么办？ 客户端交互：实现线性化语义(linearizable semantics)； 配置变更：如何在集群中增加或删除节点； 开始之前 # 开始之前需要了解 Raft 的一些术语。\n服务器状态 # 服务器在任意时间只能处于以下三种状态之一：\nLeader：处理所有客户端请求、日志复制。同一时刻最多只能有一个可行的 Leader； Follower：完全被动的（不发送 RPC，只响应收到的 RPC）——大多数服务器在大多数情况下处于此状态； Candidate：用来选举新的 Leader，处于 Leader 和 Follower 之间的暂时状态； 系统正常运行时，只有一个 Leader，其余都是 Followers.\n状态转换图：\n任期 # 时间被划分成一个个的任期(Term)，每个任期都由一个数字来表示任期号，任期号单调递增并且永远不会重复。\n一个正常的任期至少有一个 Leader，通常分为两部分：\n任期开始时的选举过程； 正常运行的部分； 有些任期可能没有选出 Leader（如图 Term 3），这时候会立即进入下一个任期，再次尝试选出一个 Leader。\n每个节点维护一个 currentTerm 变量，表示系统中当前任期。currentTerm 必须持久化存储，以便在服务器宕机重启时将其恢复。\n**任期非常重要！任期能够帮助 Raft 识别过期的信息。**例如：如果 currentTerm = 2 的节点与 currentTerm = 3 的节点通信，我们可以知道第一个节点上的信息是过时的。\n我们只使用最新任期的信息。后面我们会遇到各种情况，去检测和消除不是最新任期的信息。\n两个 RPC # Raft 中服务器之间所有类型的通信通过两个 RPC 调用：\nRequestVote：用于选举； AppendEntries：用于复制 log 和发送心跳； 1. Leader 选举 # 启动 # 节点启动时，都是 Follower 状态； Follower 被动地接受 Leader 或 Candidate 的 RPC； 所以，如果 Leader 想要保持权威，必须向集群中的其它节点发送心跳包（空的 AppendEntries RPC）； 等待选举超时(electionTimeout，一般在 100~500ms)后，Follower 没有收到任何 RPC： Follower 认为集群中没有 Leader 开始新的一轮选举 选举 # 当一个节点开始竞选：\n增加自己的 currentTerm 转为 Candidate 状态，其目标是获取超过半数节点的选票，让自己成为 Leader 先给自己投一票 并行地向集群中其它节点发送 RequestVote RPC 索要选票，如果没有收到指定节点的响应，它会反复尝试，直到发生以下三种情况之一： 获得超过半数的选票：成为 Leader，并向其它节点发送 AppendEntries 心跳； 收到来自 Leader 的 RPC：转为 Follower； 其它两种情况都没发生，没人能够获胜(electionTimeout 已过)：增加 currentTerm，开始新一轮选举； 流程图如下： 选举安全性 # 选举过程需要保证两个特性：安全性(safety)和活性(liveness)。\n安全性(safety)：一个任期内只会有一个 Leader 被选举出来。需要保证：\n每个节点在同一任期内只能投一次票，它将投给第一个满足条件的投票请求，然后拒绝其它 Candidate 的请求。这需要持久化存储投票信息 votedFor，以便宕机重启后恢复，否则重启后 votedFor 丢失会导致投给别的节点； 只有获得超过半数节点的选票才能成为 Leader，也就是说，两个不同的 Candidate 无法在同一任期内都获得超过半数的票； 活性(liveness)：确保最终能选出一个 Leader。\n问题是：原则上我们可以无限重复分割选票，假如选举同一时间开始，同一时间超时，同一时间再次选举，如此循环。\n解决办法很简单：\n节点随机选择超时时间，通常在 [T, 2T] 之间（T = electionTimeout） 这样，节点不太可能再同时开始竞选，先竞选的节点有足够的时间来索要其他节点的选票 T \u0026raquo; broadcast time(T 远大于广播时间)时效果更佳 2. 日志复制 # 日志结构 # 每个节点存储自己的日志副本(log[])，每条日志记录包含：\n索引：该记录在日志中的位置 任期号：该记录首次被创建时的任期号 命令 **日志必须持久化存储。**一个节点必须先将记录安全写到磁盘，才能向系统中其他节点返回响应。\n如果一条日志记录被存储在超过半数的节点上，我们认为该记录已提交(committed)——这是 Raft 非常重要的特性！如果一条记录已提交，意味着状态机可以安全地执行该记录。\n在上图中，第 1-7 条记录被提交，第 8 条尚未提交。\n提醒：多数派复制了日志即已提交，这个定义并不精确，我们会在后面稍作修改。\n正常运行 # 客户端向 Leader 发送命令，希望该命令被所有状态机执行； Leader 先将该命令追加到自己的日志中； Leader 并行地向其它节点发送 AppendEntries RPC，等待响应； 收到超过半数节点的响应，则认为新的日志记录是被提交的： Leader 将命令传给自己的状态机，然后向客户端返回响应 此外，一旦 Leader 知道一条记录被提交了，将在后续的 AppendEntries RPC 中通知已经提交记录的 Followers Follower 将已提交的命令传给自己的状态机 如果 Follower 宕机/超时：Leader 将反复尝试发送 RPC； 性能优化：Leader 不必等待每个 Follower 做出响应，只需要超过半数的成功响应（确保日志记录已经存储在超过半数的节点上）——一个很慢的节点不会使系统变慢，因为 Leader 不必等他； 日志一致性 # Raft 尝试在集群中保持日志较高的一致性。\nRaft 日志的 index 和 term 唯一标示一条日志记录。（这非常重要！！！）\n如果两个节点的日志在相同的索引位置上的任期号相同，则认为他们具有一样的命令；从头到这个索引位置之间的日志完全相同； 如果给定的记录已提交，那么所有前面的记录也已提交。 AppendEntries 一致性检查 # Raft 通过 AppendEntries RPC 来检测这两个属性。\n对于每个 AppendEntries RPC 包含新日志记录之前那条记录的索引(prevLogIndex)和任期(prevLogTerm)； Follower 检查自己的 index 和 term 是否与 prevLogIndex 和 prevLogTerm 匹配，匹配则接收该记录；否则拒绝； 3. Leader 更替 # 当新的 Leader 上任后，日志可能不会非常干净，因为前一任领导可能在完成日志复制之前就宕机了。Raft 对此的处理方式是：无需采取任何特殊处理。\n当新 Leader 上任后，他不会立即进行任何清理操作，他将会在正常运行期间进行清理。\n原因是当一个新的 Leader 上任时，往往意味着有机器故障了，那些机器可能宕机或网络不通，所以没有办法立即清理他们的日志。在机器恢复运行之前，我们必须保证系统正常运行。\n**大前提是 Raft 假设了 Leader 的日志始终是对的。**所以 Leader 要做的是，随着时间推移，让所有 Follower 的日志最终都与其匹配。\n但与此同时，Leader 也可能在完成这项工作之前故障，日志会在一段时间内堆积起来，从而造成看起来相当混乱的情况，如下所示：\n因为我们已经知道 index 和 term 是日志记录的唯一标识符，这里不再显示日志包含的命令，下同。\n如图，这种情况可能出现在 S4 和 S5 是任期 2、3、4 的 Leader，但不知何故，他们没有复制自己的日志记录就崩溃了，系统分区了一段时间，S1、S2、S3 轮流成为了任期 5、6、7 的 Leader，但无法与 S4、S5 通信以进行日志清理——所以我们看到的日志非常混乱。\n唯一重要的是，索引 1-3 之间的记录是已提交的(已存在多数派节点)，因此我们必须确保留下它们。\n其它日志都是未提交的，我们还没有将这些命令传递给状态机，也没有客户端会收到这些执行的结果，所以不管是保留还是丢弃它们都无关紧要。\n安全性 # 一旦状态机执行了一条日志里的命令，必须确保其它状态机在同样索引的位置不会执行不同的命令。\nRaft 安全性(Safety)：如果某条日志记录在某个任期号已提交，那么这条记录必然出现在更大任期号的未来 Leader 的日志中。\n这保证了安全性要求：\nLeader 不会覆盖日志中的记录； 只有 Leader 的日志中的记录才能被提交； 在应用到状态机之前，日志必须先被提交； 这决定我们要修改选举程序：\n如果节点的日志中没有正确的内容，需要避免其成为 Leader； 稍微修改 committed 的定义（即前面提到的要稍作修改）：前面说多数派存储即是已提交的，但在某些时候，我们必须延迟提交日志记录，直到我们知道这条记录是安全的，所谓安全的，就是我们认为后续 Leader 也会有这条日志。 延迟提交，选出最佳 Leader # 问题来了：我们如何确保选出了一个很好地保存了所有已提交日志的 Leader ？\n这有点棘手，举个例子：假设我们要在下面的集群中选出一个新 Leader，但此时第三台服务器不可用。\n这种情况下，仅看前两个节点的日志我们无法确认是否达成多数派，故无法确认第五条日志是否已提交。\n那怎么办呢？\n通过比较日志，在选举期间，选择最有可能包含所有已提交的日志：\nCandidate 在 RequestVote RPCs 中包含日志信息（最后一条记录的 index 和 term，记为 lastIndex 和 lastTerm）； 收到此投票请求的服务器 V 将比较谁的日志更完整：(lastTermV \u0026gt; lastTermC) || (lastTermV == lastTermC) \u0026amp;\u0026amp; (lastIndexV \u0026gt; lastIndexC) 将拒绝投票；（即：V 的任期比 C 的任期新，或任期相同但 V 的日志比 C 的日志更完整）； 无论谁赢得选举，可以确保 Leader 和超过半数投票给它的节点中拥有最完整的日志——最完整的意思就是 index 和 term 这对唯一标识是最大的。 举个例子 # Case 1: Leader 决定提交日志 # 任期 2 的 Leader S1 的 index = 4 日志刚刚被复制到 S3，并且 Leader 可以看到 index = 4 已复制到超过半数的服务器，那么该日志可以提交，并且安全地应用到状态机。\n现在，这条记录是安全的，下一任期的 Leader 必须包含此记录，因此 S4 和 S5 都不可能从其它节点那里获得选票：S5 任期太旧，S4 日志太短。\n只有前三台中的一台可以成为新的 Leader——S1 当然可以，S2、S3 也可以通过获取 S4 和 S5 的选票成为 Leader。\nCase 2: Leader 试图提交之前任期的日志 # 如图所示的情况，在任期 2 时记录仅写在 S1 和 S2 两个节点上，由于某种原因，任期 3 的 Leader S5 并不知道这些记录，S5 创建了自己的三条记录然后宕机了，然后任期 4 的 Leader S1 被选出，S1 试图与其它服务器的日志进行匹配。因此它复制了任期 2 的日志到 S3。\n此时 index=3 的记录时是不安全的。\n因为 S1 可能在此时宕机，然后 S5 可能从 S2、S3、S4 获得选票成为任期 5 的 Leader。一旦 S5 成为新 Leader，它将覆盖 index=3-5 的日志，S1-S3 的这些记录都将消失。\n我们还要需要一条新的规则，来处理这种情况。\n新的 Commit 规则 # 新的选举不足以保证日志安全，我们还需要继续修改 commit 规则。\nLeader 要提交一条日志：\n日志必须存储在超过半数的节点上； Leader 必须看到：超过半数的节点上还必须存储着至少一条自己任期内的日志； 如图，回到上面的 Case 2: 当 index = 3 \u0026amp; term = 2 被复制到 S3 时，它还不能提交该记录，必须等到 term = 4 的记录存储在超过半数的节点上，此时 index = 3 和 index = 4 可以认为是已提交。\n此时 S5 无法赢得选举了，它无法从 S1-S3 获得选票。\n结合新的选举规则和 commit 规则，我们可以保证 Raft 的安全性。\n日志不一致 # Leader 变更可能导致日志的不一致，这里展示一种可能的情况。\n可以从图中看出，Raft 集群中通常有两种不一致的日志：\n缺失的记录(Missing Entries)； 多出来的记录(Extraneous Entries)； 我们要做的就是清理这两种日志。\n修复 Follower 日志 # 新的 Leader 必须使 Follower 的日志与自己的日志保持一致，通过：\n删除 Extraneous Entries； 补齐 Missing Entries； Leader 为每个 Follower 保存 nextIndex：\n下一个要发送给 Follower 的日志索引； 初始化为： 1 + Leader 最后一条日志的索引； Leader 通过 nextIndex 来修复日志。当 AppendEntries RPC 一致性检查失败，递减 nextIndex 并重试。如下图所示：\n对于 a：\n一开始 nextIndex = 11，带上日志 index = 10 \u0026amp; term = 6，检查失败； nextIndex = 10，带上日志 index = 9 \u0026amp; term = 6，检查失败； 如此反复，直到 nextIndex = 5，带上日志 index = 4 \u0026amp; term = 4，该日志现在匹配，会在 a 中补齐 Leader 的日志。如此往下补齐。 对于 b： 会一直检查到 nextIndex = 4 才匹配。值得注意的是，对于 b 这种情况，当 Follower 覆盖不一致的日志时，它将删除所有后续的日志记录（任何无关紧要的记录之后的记录也都是无关紧要的）。如下图所示：\n4. 处理旧 Leader # 实际上，老的 Leader 可能不会马上消失，例如：网络分区将 Leader 与集群的其余部分分隔，其余部分选举出了一个新的 Leader。问题在于，如果老的 Leader 重新连接，也不知道新的 Leader 已经被选出来，它会尝试作为 Leader 继续提交日志。此时如果有客户端向老 Leader 发送请求，老的 Leader 会尝试存储该命令并向其它节点复制日志——我们必须阻止这种情况发生。\n任期就是用来发现过时的 Leader(和 Candidates)：\n每个 RPC 都包含发送方的任期； 如果发送方的任期太老，无论哪个过程，RPC 都会被拒绝，发送方转变到 Follower 并更新其任期； 如果接收方的任期太老，接收方将转为 Follower，更新它的任期，然后正常的处理 RPC； 由于新 Leader 的选举会更新超过半数服务器的任期，旧的 Leader 不能提交新的日志，因为它会联系至少一台多数派集群的节点，然后发现自己任期太老，会转为 Follower 继续工作。\n这里不打算继续讨论别的极端情况。\n5. 客户端协议 # 客户端只将命令发送到 Leader：\n如果客户端不知道 Leader 是谁，它会和任意一台服务器通信； 如果通信的节点不是 Leader，它会告诉客户端 Leader 是谁； Leader 直到将命令记录、提交和执行到状态机之前，不会做出响应。\n这里的问题是如果 Leader 宕机会导致请求超时：\n客户端重新发出命令到其他服务器上，最终重定向到新的 Leader 用新的 Leader 重试请求，直到命令被执行 这留下了一个命令可能被执行两次的风险——Leader 可能在执行命令之后但响应客户端之前宕机，此时客户端再去寻找下一个 Leader，同一个命令就会被执行两次——这是不可接受的！\n解决办法是：客户端发送给 Leader 的每个命令都带上一个唯一 id\nLeader 将唯一 id 写到日志记录中 在 Leader 接受命令之前，先检查其日志中是否已经具有该 id 如果 id 在日志中，说明是重复的请求，则忽略新的命令，返回旧命令的响应 每个命令只会被执行一次，这就是所谓的线性化的关键要素。\n6. 配置变更 # 随着时间推移，会有机器故障需要我们去替换它，或者修改节点数量，需要有一些机制来变更系统配置，并且是安全、自动的方式，无需停止系统。\n系统配置是指：\n每台服务器的 id 和地址 系统配置信息是非常重要的，它决定了多数派的组成 首先要意识到，我们不能直接从旧配置切换到新配置，这可能会导致矛盾的多数派。\n如图，系统以三台服务器的配置运行着，此时我们要添加两台服务器。如果我们直接修改配置，他们可能无法完全在同一时间做到配置切换，这会导致 S1 和 S2 形成旧集群的多数派，而同一时间 S3-S5 已经切换到新配置，这会产生两个集群。\n这说明我们必须使用一个两阶段(two-phase)协议。\n如果有人告诉你，他可以在分布式系统中一个阶段就做出决策，你应该非常认真地询问他，因为他要么错了，要么发现了世界上所有人都不知道的东西。\n共同一致(Joint Consensus) # Raft 通过共同一致(Joint Consensus)来完成两阶段协议，即：新、旧两种配置上都获得多数派选票。\n第一阶段：\nLeader 收到 $C_{new}$ 的配置变更请求后，先写入一条 $C_{old+new}$ 的日志，配置变更立即生效，然后将日志通过 AppendEntries RPC 复制到 Follower 中，收到该 $C_{old+new}$ 的节点立即应用该配置作为当前节点的配置； $C_{old+new}$ 日志复制到多数派节点上时，$C_{old+new}$ 的日志已提交； $C_{old+new}$ 日志已提交保证了后续任何 Leader 一定有 $C_{old+new}$ 日志，Leader 选举过程必须获得旧配置中的多数派和新配置中的多数派同时投票。\n第二阶段：\n$C_{old+new}$ 日志已提交后，立即写入一条 $C_{new}$ 的日志，并将该日志通过 AppendEntries RPC 复制到 Follower 中，收到 $C_{new}$ 的节点立即应用该配置作为当前节点的配置； $C_{new}$ 日志复制到多数派节点上时，$C_{new}$ 的日志已提交；在 $C_{new}$ 日志提交以后，后续的配置都基于 $C_{new}$ 了； Joint Consensus 还有一些细节：\n变更过程中，来自新旧配置的节点都有可能成为 Leader； 如果当前 Leader 不在 $C_{new}$ 配置里面，一旦 $C_{new}$ 提交，它必须下台(step down)。 如图所示，旧 Leader 不再是新配置的成员之后，还有可能继续服务一小段时间；即旧 Leader 可能在 $C_{new}$ 配置下继续当 Leader（虽然实质上并不是Leader），直到 $C_{new}$ 的日志复制到多数派上而 committed；\n相关阅读 # Raft 作者亲自出的 Raft 试题，你能做对几道？\nGolang 实现 Paxos 分布式共识算法\n漫谈分布式共识问题\n理解 Paxos（含伪代码）\n","date":"2021-02-01","permalink":"/posts/202102-raft/","section":"Posts","summary":"本文整理自 Ongaro 在 Youtube 上的视频。 目标 # Raft 的目标（或者说是分布式共识","title":"条分缕析 Raft 算法"},{"content":"","date":"2021-01-25","permalink":"/tags/%E6%9E%B6%E6%9E%84/","section":"Tags","summary":"","title":"架构"},{"content":"","date":"2021-01-25","permalink":"/categories/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/","section":"Categories","summary":"","title":"软件工程"},{"content":"架构师一般从三个方面来发现架构特性：\n领域关注(Domain Concerns) 需求 隐性领域知识 在《软件架构基础 4》中我们讨论了隐性领域知识，这里将介绍另外两种。\n搬运下 wikipedia 对于领域(Domain)的解释：\nA domain is the targeted subject area of a computer program. It is a term used in software engineering. Formally it represents the target subject of a specific programming project, whether narrowly or broadly defined. For example, a particular programming project might have had as a goal the creation of a program for a particular hospital, and that hospital would be the domain.\n简而言之，领域就是指程序的目标主题领域，假如为医院开发某个程序，那么医院就是领域。Domain 这个词常常出现在软件工程中，例如领域驱动架构(\u0010DDD)、领域模型（Domain model）。\n领域关注(Domain Concerns) # 架构师必须能够将领域关注的问题转化为确定、正确的架构特性。例如，可扩展性是最重要的关注点，还是容错、安全或性能？也许系统需要所有四个特性的结合。\n有一个小窍门，那就是努力让最终的清单尽可能的简短。架构中一个常见的反模式是试图设计一个通用架构，支持所有的架构特性。架构所支持的每一个架构特性都会使整个系统设计变得复杂，支持过多的架构特性会导致架构师和开发人员在还没有开始解决问题之前就变得越来越复杂。\n不要纠结于架构特性的数量，而是要保持设计简单的动机。\n一开始就考虑系统最终的架构特性列表是愚蠢的，这不仅浪费时间还会产生挫败感和分歧。更好的办法是选出最重要的三个特性（顺序不限）。\n架构师谈论的是可扩展性、互操作性、容错性、可学习性和可用性，业务利益相关者谈论的是合并和收购、用户满意度、销售时间和竞争优势。两者之间可能互相不理解，下表提供一个领域关注到架构特性的转换。\n领域关注 架构特性 兼并和收购 互操作性、可扩展性、适应性、可扩展性。 销售时间 敏捷性、可测试性、可部署性 用户满意度 性能、可用性、容错性、可测试性、可部署性、敏捷性、安全性。 竞争优势 敏捷性、可测试性、可部署性、可扩展性、可用性、容错性。 时间和预算 简单性、可行性。 需要注意的一个重要问题是，敏捷性不等于销售时间。相反，销售时间是敏捷性+可测试性+可部署性。这是很多架构师在翻译领域关注点时陷入的陷阱——只关注其中一个点。\n例如，一个业务利益相关者可能会说：“由于监管要求，我们必须按时完成基金的日终计价”。一个无效的架构师可能只会关注性能，因为这似乎是该领域关注的主要焦点。然而，该架构师会因为很多原因而失败：\n首先，如果系统在需要的时候无法使用，那么系统的速度有多快都不重要； 第二，随着业务的发展和更多资金，系统必须也能扩展到及时完成日终处理； 第三，系统不仅要可用，而且要可靠，以便在计算日终基金价格时不会崩溃； 第四，如果日终基金定价完成了 85%，系统崩溃了怎么办？它必须能够恢复并继续计算。 最后，系统可能很快，但基金价格是否计算正确？ 所以，除了性能之外，架构师还必须同样关注可用性、可扩展性、可靠性、可恢复性和可审计性。\n从需求提取架构特 # 性\n一些架构特征来自需求文档中的明确声明。例如，明确的预期用户数和规模。另一些则来自架构师的领域知识，这是领域知识对架构师总是有益的众多原因之一。\n例如，假设一个架构师设计了一个为大学生处理班级注册的应用程序，为了便于计算，假设学校有 1000 名学生，注册时间为 10 小时。架构师在设计系统时，是否应该假设注册过程中的学生会随着时间推移均匀分布？或者，基于对大学生习惯和癖好的了解，架构师是否应该设计一个能够处理所有 1000 名学生在最后 10 分钟注册的系统？任何一个了解学生有多么拖延的人都知道这个问题的答案! 像这样的细节很少会出现在需求文档中，但它们确实为设计决策提供了信息。\n案例学习：三明治店网上订餐系统 # 描述 # 一家全国性的三明治店想实现网上订餐（除了目前的呼叫服务外）。\n用户 # 数千人，也许有一天会到数百万人。\n需求 # 用户下单，然后会得到一个领取三明治的时间和到店的方向（必须与几个外部地图服务整合，包括交通信息）； 如果店家提供送餐服务，就派司机带着三明治给用户送去； 移动设备的可访问性； 提供全国性的每日促销/特价活动； 提供本地每日促销/特价活动； 接受在线、当面或货到付款； 其它需求 # 三明治店都是加盟店，每个店都有不同的老板； 母公司近期有海外扩张计划； 企业的目标是雇佣廉价的劳动力，以实现利润最大化\u0026quot;； 鉴于以上这种情况，架构师将如何推导出架构特征？\n需求的每一部分都可能有助于架构的一个或多个方面，架构师在这里并不设计整个系统，相反，架构师要寻找影响或冲击设计的东西，特别是结构性的。\n首先，将候选架构特性分为显性和隐性特性。\n显性架构特性 # 明确的架构特性作为必要的一部分出现在需求规范中，例如，一个购物网站可能支持的并发用户数量，需求中明确了这一点。架构师应该考虑需求的每个部分，看看它是否有助于架构特性。但首先，架构师应该考虑领域级的关于预期指标的估计。\n首先应该引起架构师注意的细节之一是用户数量：目前是数千人，也许有一天会到数百万人（这是一家雄心勃勃的三明治商店！）。因此，可扩展性\u0026ndash;处理大量并发用户而不严重降低性能的能力\u0026ndash;是架构的首要特性之一。请注意，需求陈述并没有明确要求可扩展性，而是将该要求表述为预期的用户数。架构师必须经常将业务语言解码成工程等价物。\n然而，我们可能还需要弹性\u0026ndash;处理突发请求的能力。这两个特性经常出现在一起，但它们有不同的限制。\n可扩展性看起来像下图所示的图形：\n弹性则是流量的爆发，如下图所示：\n有些系统是可扩展的，但不是弹性的。例如，考虑一个酒店预订系统，如果没有特殊的销售或活动，并发用户数量可能是一致的。相反，考虑一个音乐会门票预订系统，随着新票的发售，狂热的粉丝会涌入网站，这就需要高度的弹性。通常情况下，弹性系统需要可扩展性：处理突发事件和大量并发用户的能力。\n弹性的要求并没有出现在需求中，然而架构师应该将其确定为一个重要的考虑因素。一个三明治店的流量是全天一致的吗？还是在用餐时间前后会有爆棚的客流？几乎可以肯定是后者。因此，一个好的架构师应该识别这种潜在的架构特征。\n架构师应该依次考虑这些业务需求中的每一个，看看是否存在架构特征。\n用户下单，然后得到一个领取三明治的时间和去商店的方向（必须提供与外部地图服务整合的选项，包括交通信息）。外部地图服务意味着集成，这可能会影响可靠性等方面。例如，如果开发人员构建了一个依赖于第三方系统的系统，然而调用它却失败了，这就会影响调用系统的可靠性。但是，架构师也要警惕架构特性的过度规范，如果外部流量服务出现故障怎么办？三明治网站是否应该失败，或者只是在没有流量信息的情况下提供稍低的效率？ *如果店家提供送餐服务，就派司机带着三明治给用户送去。*看起来不需要特殊的架构特性来支持这个需求。 *移动设备的可访问性。*这一要求将主要影响到应用程序的设计，要做一个 portable web application 或是几个 native web applications。考虑到预算限制和应用程序的简单性，一个移动端优化的 web 应用更好。因此，架构师还要考虑为页面加载时间和移动端的性能架构特性。注意，架构师在这样的情况下不应该单独行动，而是应该与用户体验设计师、业务利益相关者和其他相关方合作，审核这样的决策。 提供全国性的每日促销/特价。 *提供本地每日促销/特价活动。*这两个要求都规定了促销和特价商品的可定制性。注意，需求 1 还意味着基于地址的可定制信息，基于这三个需求，架构师可以考虑将可定制性作为一种架构特性。例如，微内核架构这样的架构风格，通过定义一个插件架构，可以极好地支持自定义行为。传统的设计也可以通过设计模式（如模板）来满足这种需求。需要架构师权衡取舍。 *接受在线、当面或货到付款。网上支付意味着安全性，但这一要求没有任何内容表明安全程度特别高。 7. 三明治店都是加盟店，每个店都有不同的老板。*这个要求可能会对架构造成成本限制\u0026ndash;架构师应该检查可行性（应用成本、时间和员工技能培训等约束条件），看看是否需要一个简单性或牺牲性的架构。 *母公司近期有向海外扩张的计划。*这个要求意味着国际化，也就是 i18n。许多技术可以处理这一要求，应该不需要特殊的结构来适应。 *企业的目标是雇佣廉价的劳动力以实现利润最大化。*这个要求表明可用性将是重要的，但同样是更关注设计而不是架构特点。 还有一个架构特征是性能：没有人愿意从一个性能差的三明治店购买，尤其是在高峰期。然而，性能是一个有着不同差别的概念\u0026ndash;架构师应该为什么样的性能而设计？我们将在后续章节中介绍性能的各种细微差别。\n我们还希望将性能数字与可扩展性数字结合起来定义。换句话说，我们必须在没有特定规模的情况下建立一个性能基线，并确定在一定数量的用户下，可接受的性能水平是多少。\n隐形架构特性 # 许多没有在需求文档中指定的架构特性却非常重要。\n系统可能支持的一个隐含的架构特性是可用性：确保用户可以访问三明治网站。与可用性密切相关的是可靠性：确保网站在交互过程中保持运行\u0026ndash;没有人想从一个不断掉线的网站上购买。\n安全性在每个系统中都是一个隐含的特性：没有人愿意使用不安全的软件。然而，它可能会根据关键性来确定优先级。\n对于该三明治店的支付应该由第三方处理，因此，只要开发者遵循一般的安全（不将信用卡号码以纯文本的形式传递，不存储太多信息等），架构师应该不需要任何特殊的结构设计来适应安全问题，在应用中做好设计就足够了。\n三明治店需要支持的最后一个主要架构特性：可定制性。需求的几个部分提供了自定义行为：食谱、本地销售等。通常这应该属于应用程序的设计，这个设计元素对应用的成功并不关键。\n在选择架构特性的过程中，没有正确的答案，只有不正确的答案：\n架构没有错误的答案，只有昂贵的答案。\n架构师可以设计一个在结构上不适应可定制性的架构，要求应用本身来支持这种行为。架构师不应该过于强调发现完全正确的架构特性集\u0026ndash;开发人员可以用各种方式来实现功能。当然，正确识别重要的结构元素可能会促进更简单或更优雅的设计。\n架构师还必须优先朝着找到最简单的集合的方向发展。团队在确定架构特性方面一个有用的尝试是找到最不重要的一个特性，如果你必须消除一个，那会是哪个？一般来说，架构师更有可能剔除显性的架构特征，因为许多隐性的特征是一个应用想成功最基本的特性。\n在三明治店的案例中，哪个架构特性是最不重要的？（不存在绝对正确的答案）\n在这种情况下，解决方案可能会失去可定制性或性能。我们可以取消可定制性作为架构特性，并计划将该行为作为应用设计的一部分来实现。性能可能是成功的最不关键因素，当然，开发人员并不是要构建一个性能糟糕的应用，而是要构建一个不将性能优先于其他特性（如可扩展性或可用性）的应用。\n","date":"2021-01-25","permalink":"/posts/202101-fosa5/","section":"Posts","summary":"《软件架构基础（Fundamentals of Software Architecture）》读书笔记（5）","title":"软件架构基础 5 架构特性识别与实战: 订餐系统"},{"content":"一个公司决定用软件解决一个特定的问题，公司会收集该系统的需求清单。需求可以说是软件开发的基础，但除了需求以外，架构师有很多因素需要去考虑。\n下图在第 1 章出现过：\n架构师可能会参与收集需求，但架构师一个关键的职责是要发现、定义其它和具体需求没有直接关系的东西，这些东西称之为架构特性（architectural characteristics）。\n一个架构特性满足三个标准：\n指定了一个非领域（业务）设计的考虑因素：例如，一个重要的架构特性是应用程序的性能水平，而这一点往往不会出现在需求文档中； 影响结构上的设计：这个架构特性是否需要特殊的结构才能成功？例如，考虑要支持支付功能的系统，可以选择第三方支付（只需要符合安全即可嵌入），还是应用自己处理支付（这需要设计特定的支付模块），这将导致不同的架构设计； 对应用的成功至关重要：应用程序可以支持大量的架构特性，但每个架构特性的支持都会增加设计的复杂性，因此，架构师的一项工作是选择最少的架构特性； 架构特性主要分为显性的和隐性的。\n隐性的很少出现在需求中，但它们却是项目成功的必要条件。例如，可用性、可靠性和安全性几乎是所有应用的基础，然而它们很少在设计文档中被提及。\n架构师必须在分析阶段利用他们的知识来发现这些架构特性。\n架构特性列表 # 架构特性存在于软件系统的广泛范围内，从代码特性如模块化，到复杂的问题如可扩展性和弹性。由于软件生态系统变化如此之快，新的概念、术语、措施和验证不断出现，并不存在真正的标准。但是架构师通常将架构特性分为下面几大类：\n运行架构特性 # 运行架构特性涵盖了性能、可扩展性、弹性、可用性和可靠性等能力。\n类型 定义 可用性(Availability) 系统可用时间，如果是24/7，则需要使系统在发生任何故障时能够迅速启动和运行 持续性(Continuity) 灾难恢复能力 性能(Performance) 包括压力测试、峰值分析、分析功能的使用频率、所需容量和响应时间。性能报告有时需要自行演练，需要几个月才能完成。 可恢复性(Recoverability) 业务持续性要求(例如，在发生灾难的情况下，系统需要多快才能重新上线？)。这将影响备份策略和对冗余硬件的要求 可靠性/安全性(Reliability/safety) 评估系统是否需要具备某些安全功能，如果发生故障，是否会给公司带来大笔资金损失？ 稳健性(Robustness) 在网络连接中断、断电或硬件故障时，系统是否能够处理运行中的错误和边界条件 可扩展性(Scalability) 随着用户或请求数量的增加，系统执行和运行的能力 结构架构特性 # 架构师也必须关注代码结构。在许多情况下，架构师对代码质量问题负有单独或共同的责任，如良好的模块化、组件之间的可控耦合、可读的代码以及一系列其他内部质量评估。\n类型 定义 可配置性(Configurability) 能够轻松地变更软件配置 可扩展性(Extensibility) 添加新的功能是多么的重要 可安装性(Installability) 易于在所有必要的平台上安装 可利用性/重复使用(Leverageability/reuse) 能够重复利用通用组件 本地化(Localization) 在文字输入、显示上支持多种语言；在报表、计量单位、货币上支持各种字符 可维护性(Maintainability) 如何轻松地进行应用变更和系统维护 可移植性(Portability) 系统是否需要运行在超过一个平台上 支持性(Supportability) 应用程序需要什么级别的技术支持？需要什么级别的日志和其他设施来调试系统中的错误？ 可升级性(Upgradeability) 能够在服务器和客户端上轻松、快速地升级 交叉架构特性 # 许多特征不在分类范围内或无法分类，却形成了重要的设计限制和考虑因素。\n类型 定义 可访问性(Accessibility) 让您的所有用户，包括色盲或听障等残疾用户都能访问(例如 Iphone 就有类似的支持) 归档性(Archivability) 数据是否需要在一段时间后归档或删除？例如，客户账户在三个月后要删除或标记为过时，并归档到二级数据库，以便将来访问。 认证(Authentication) 安全要求，确保用户就是那个人。 权限(Authorization) 安全性要求，以确保用户只能访问应用程序中的某些功能 法律要求(Legal) 系统在什么法律约束下运行(数据保护、萨班斯法案、GDPR 等)？公司需要哪些保留权限？关于应用程序的构建或部署方式有什么规定？ 隐私(Privacy) 能够对公司内部员工隐藏交易(加密的交易即使是 DBA 和网络架构师也无法看到) 安全性(Security) 数据库中的数据是否需要加密？内部系统之间的网络通信是否需要加密？远程用户访问时需要进行哪种类型的认证？ 可用性/可实现性(Usability/achievability) 你的用户使用应用程序需要的培训程度，是否易用？。需要像对待其他架构问题一样认真对待可用性要求。 其它架构特性 # ISO 也发布过软件质量的列表，参见：https://iso25000.com/index.php/en/iso-25000-standards/iso-25010\n主要分为下图的各大类，在此不再展开。\n任何架构特征列表必然是不完整的，任何软件都可能根据独特的因素发明重要的架构特征。\n权衡架构 # 由于各种原因，应用程序只能支持列出的几个架构特性：\n每一个被支持的特性都需要设计上的努力，或许还需要结构上的支持； 更大的问题在于，每个架构特性往往会对其他特性产生影响。例如，如果架构师想要提高安全性，几乎肯定会对性能产生负面影响； 因此，架构师很少会遇到这样的情况：他们能够设计一个系统，并将每一个架构特性最大化。更多的情况是，决定要在几个相互竞争的问题之间进行权衡。\n过多的架构特性导致设计变得笨重。\n**架构师应该努力使架构设计尽可能地迭代。**如果能更容易地对架构进行修改，就可以减少在第一次尝试中就设计出完全正确的东西的压力。敏捷软件开发最重要的经验之一就是迭代的价值，这在软件开发的各个层面都适用，包括架构。\n（这本书一直在强调，在软件架构和开发中，快速迭代和敏捷的重要性。）\n永远不要追求最佳的架构。\n","date":"2021-01-16","permalink":"/posts/202101-fosa4/","section":"Posts","summary":"《软件架构基础（Fundamentals of Software Architecture）》读书笔记（4）","title":"软件架构基础 4: 你的架构需要考虑的架构特性"},{"content":"0. 写在前面 # 什么是好的代码？好的代码应该模块化。\n王垠在其《编程的智慧》中也提到，要“写模块化的代码”。(不对人做评价，这篇文章写得是非常好的。)\n如果你读过《代码大全》和《代码整洁之道》等书，一定对**“高内聚、低耦合”**不陌生。\n好的模块化代码就是要高内聚、低耦合。\n事实上，内聚和耦合是 1972 年就提出的概念，由于耦合不好具体的衡量，Meilir Page-Jones 在 1992 年提出了共生性（Connascence）。本章重点就是介绍如何评估模块化架构，以及引入共生性这一概念来帮助更好的模块化。\n1. 模块化 # 不同的平台、语言为代码提供了不同的复用机制，将相关代码组合成模块。\n理解模块对于架构师来说非常重要，因为用来分析架构的工具（可视化等）常常都依赖于模块化的概念。如果一个架构师在设计一个系统时，没有注意到各个部分是如何连接在一起的，那么他们最终创建的系统会带来无数的问题。\n架构师必须保持良好的结构，这不会偶然发生。\n模块的代码到底是什么？我们用模块化来描述相关代码中的逻辑分组，这些模块可以用来构造一个更复杂的结构。\n现代的语言有各种各样的封装机制，例如，许多语言可以在函数/方法、类、包/命名空间中定义行为，每个包都有不同的可见性和范围规则。（这有时候也会让开发人员选择困难）\n架构师必须意识到开发者是如何组织包的，如果几个包紧密的耦合在一起，那么重用其中一个包就变得非常困难。\n鉴于模块化的重要性，研究人员提供了各种语言无关的标准来衡量，我们专注于三个关键概念：\n内聚（Cohesion） 耦合（Coupling） 共生性（Connascence）（注：参考《UML面向对象设计基础》的翻译） 2. 内聚（Cohesion） # 内聚性是指子程序中各种操作之间联系的紧密程度，我们的目标是让每一个模块只做好一件事，不去做其他事情。\n试图分割一个内聚的模块只会导致耦合性增加和可读性降低。（Attempting to divide a cohesive module would only result in increased coupling and decreased readability.） —— Larry Constantine\n计算机科学家们已经定义了一系列的内聚的衡量标准，从最好到最坏列出如下：\n功能性内聚（Functional cohesion）：模块内所有元素都为完成同一个功能而存在，共同完成一个单一的功能，模块已不可再分，具有最高的内聚； 顺序内聚（Sequential cohesion）：模块必须顺序执行； 通信内聚（Communicational cohesion）：两个不同操作的模块使用同样的数据。例如，在数据库中添加一条记录，并根据该信息生成一封邮件； 过程内聚（Procedural cohesion）：两个模块必须以特定的次序执行。 时间内聚（Temporal cohesion）：把需要同时执行的动作组合在一起形成的模块。 逻辑内聚（Logical cohesion）：这种模块把几种相关的功能组合在一起， 每次被调用时，由传送给模块参数来确定该模块应完成哪一种功能。 巧合内聚（Coincidental cohesion）：模块内的各个元素之间没有任何联系，只是偶然地被凑到一起；内聚程度最低。 内聚不容易考量，特定的模块需要架构师来具体决定，例如，考虑一个模块定义了：\nCustomer：\nadd customer update customer get customer notify customer get customer orders cancel customer orders 或者说将后两个函数剥离出来，分成两个模块：\nCustomer：\nadd customer update customer get customer notify customer Order：\nget customer orders cancel customer orders 哪个更好？一如既往，这要看情况：\n订单只有这两个操作吗？如果是这样，将这些操作放在客户包中维护可能是有意义的； 客户包按预期是否会变得更大？ 订单是否需要如此多的客户信息？ 这些问题代表了软件架构师工作核心的权衡分析。\n由于内聚非常主观，计算机科学家制定了一个标准来衡量内聚性，其中 LCOM(Lack of Cohesion in Methods) 为著名。这里涉及到的数学公式平时很少用到，在此不再展开，只需要知道有这么一个公式，在需要的时候可以再查询拿出来用。想进一步了解的读者可以查看：https://en.wikipedia.org/wiki/Programming_complexity\n3. 耦合（Coupling） # 我们常常谈到要“解耦”，弱耦合是系统可维护的关键。\n耦合其实也有多种类型，但在此不再介绍，因为它们已经被共生性（Connascence）所取代。\n4. 共生性（Connascence） # 1996 年 Meilir Page-Jones 发表了 《What Every Programmer Should Know About Object-Oriented Design》，完善了耦合的度量，并命名为：Connascence。\n他是这样定义的：\n如果一个组件的改变会要求另一个组件进行修改，才能保持系统的整体正确性，那么这两个组件就是共生的。 —— Meilir Page-Jones\n共生性分为静态的和动态的。我们将分别介绍各种类型的共生性，对于部分重要的、不易理解的，我将补充一些代码案例，作为具体的参考来帮助理解。\n静态共生性：\n4.1 名称共生性（Connascence of Name, CoN） # methodA() 改名为 methodB() 时,　调用 methodA() 的地方都要改名，这是代码库中最常见的耦合方式，现代的 IDE 的检索功能使修改代码的名称变得很容易，这是最理想的耦合方式；\n4.2 类型共生性（Connascence of Type, CoT) # 如果一个变量从值 100 变成了一个很大的数，变量的类型可能要从 int 改成 BigInteger\n4.3 意义共生性（Connascence of Meaning, CoM） # 例如，在很多语言中，通常会把大于 0 的数字认为是 True，0 认为是 False。下面是 Java 中的一个具体例子：\na.compareTo(b) // 如果 a = b，则返回值 0； // 如果 a \u0026gt; b，则返回大于 0 的值； // 如果 a \u0026lt; b，则返回小于 0 的值。 4.4 位置共生性（Connascence of Position, CoP） # 函数的参数的位置顺序或个数耦合，例如下面的函数增加一个参数后，函数调用将会出错。\n针对这个例子，我们可以通过下面的办法，将位置共生性转为名称共生性来降低耦合性：\nclass User { FirstName, LastName, Address } void SaveUser(User); myrepo.SaveUser(new User{ FirstName = \u0026#34;bob\u0026#34;, LastName = \u0026#34;Marley\u0026#34;, Address = \u0026#34;Jamaica\u0026#34;}); 4.5 算法共生性（Connascence of Algorithm, CoA） # 多个组件必须就一个特定的算法达成一致。例如：客户端和服务端用相同的算法验证用户身份。这代表一种较高的耦合形式——如果算法细节改变，验证将不再有效。\n动态共生性：\n4.6 执行共生性（Connascence of Execution, CoE） # 代码的执行顺序上的耦合。例如下面的代码，在设置主题之前就发送了，明显在顺序上有问题。\nemail = new Email(); email.setRecipient(\u0026#34;foo@example.com\u0026#34;); email.setSender(\u0026#34;me@me.com\u0026#34;); email.send(); email.setSubject(\u0026#34;whoops\u0026#34;); 4.7 时间共生性（Connascence of Timing, CoT） # 常见情况是两个线程同时执行造成的竞赛条件。\n这里我们可以看一个有趣的例子，发生在 bootstrap 的一个 issue：https://github.com/twbs/bootstrap/issues/3902\n// using bootstrap modal $(element).modal(\u0026#39;hide\u0026#39;) $(element).modal(\u0026#39;show\u0026#39;) // Error! // 隐藏一个 modal 大约需要 500ms 的动画， // 如果你在这时候直接调用了 \u0026#39;show\u0026#39;，将会发生异常 // 我们必须这样做 $(element).modal(\u0026#39;hide\u0026#39;) $(element).on(\u0026#39;hidden.bs.modal\u0026#39;, ()=\u0026gt;{ $(element).modal(\u0026#39;show\u0026#39;) // ok }) 4.8 值共生性（Connascence of Values, CoV） # 常见的情况在分布式事务中，例如需要在多个独立的数据库中做分布式事务。\n4.9 身份共生性（Connascence of Identity, CoI） # 两个独立的模块需要共享和更新同一个数据结构，例如：分布式队列。\n5. 共生性的属性 # 5.1 强度（Strength） # Page-Jones 指出，共生性有明确的强弱谱系，如下图所示，按强度递增排序。identity 具有最强的共生性，name 具有最弱的共生性。——也就是说用 name 的方式耦合则为最弱的耦合方式。\n架构师应该倾向于静态共生性而不是动态共生性，因为开发人员可以通过现代的 IDE 来很快地确定它。\n5.2 局部性（Locality） # 局部性指两个模块的之间的远近程度。\n通常情况下，在同一模块中、距离较近的类比在不同模块中、距离距离较远的类具有更高的共生性。换句话说，随着两个模块在代码中的距离增加，共生性会减弱。\n5.3 程度（Degree） # 共生性的程度与模块的影响大小有关——它影响了几个类还是几十个类？影响较小的共生性对代码库的损坏就较小。\n6. 如何通过共生性来提高系统模块化 # 讲了这么多，我们到底如何实践共生性呢？\nPage-Jones 提供了三个使用共生性来提高系统模块化的指南： 1.通过将系统拆分成封装的元素，使得整体的共生性达到最弱 2.最大限度地减少任何跨越封装边界的共生性 3.最大限度地提高封装边界的共生性\nJim Weirich （传奇的软件架构创新者，Ruby 社区活跃人士）简化了上面较为抽象的指导，提供了两个更具体的建议：\n程度法则（Rule of Degree）：将强共生性转化为弱共生性。 局部性规则（Rule of Locality）：随着软件元素之间距离的增加，应使用较弱的共生性。 7. 耦合性和共生性 # 从架构师的角度来看，耦合和共生是有所重叠的，这是不同时代的产物，下图列出两者重叠的部分：\n共生性提供了更精细化的考量，例如左边的数据耦合，在右边的静态共生性提供了更具体的建议。\n8. 局限性 # 尽管如此，架构师在应用这些指标来分析和设计系统时，存在几个问题：\n这些度量从代码层面考察细节，关注代码质量，而不一定是架构。架构师更关注模块如何耦合，而不是耦合程度，例如，架构师关心的是同步或异步通信，而不关心如何实现。 共生性并没有真正解决许多现代架构师必须做出的一个基本决定\u0026ndash;在分布式架构（例如：微服务）中，使用同步还是异步通信？在后面会介绍新的方法来思考现代的共生性。 虽然对模块化进行了大量的介绍和思考，开发人员和架构师在实际实施过程中，还是会遇到很多的困难。\n纸上得来终觉浅，绝知此事要躬行。\n设计良好的架构，并非易事！\n","date":"2021-01-14","permalink":"/posts/202101-fosa3/","section":"Posts","summary":"《软件架构基础（Fundamentals of Software Architecture）》读书笔记（3）","title":" 软件架构基础 3: 什么是好的模块化代码？高内聚、低耦合如何衡量？"},{"content":"架构思维指用架构的眼光和观点来看待事物，主要包括：\n理解软件架构和软件设计的区别，知道与开发团队合作，并让架构发挥作用。 拥有技术广度的同时，保持一定的技术深度，看到别人看不到的解决方案和可能性。 理解、分析、协调各种解决方案和技术之间的权衡。 理解业务驱动的重要性，以及如何将其转化为架构。 架构与设计 # 软件架构和软件设计的区别往往是一个令人困惑的问题。架构在哪里结束，设计在哪里开始？架构师与开发人员的职责分别是什么？\n下图是传统的架构师和开发的关系，架构师分析业务需求，提取和定义架构特性，选择架构风格，以及创建组件，然后交给开发团队，开发团队为每个组件创建类、用户界面，以及开发和测试源码。\n这样的架构很少能够成功，因为单箭头穿过架构师和开发人员，意味着架构师的想法和决定很少传到开发团队，而开发团队碰到的架构问题也很少回到架构师那里———这样的模式是脱节的。\n想要架构发挥作用，必须打破架构师和开发人员之间的物理和虚拟的障碍，形成一种双向关系。这种模式不仅有利于架构师和开发之间的双向沟通，而且可以让架构师为团队中的开发人员提供指导和辅导。\n下图展示了一种双向沟通关系。\n与老派的瀑布流程静态且僵化的软件架构不同，当今系统的架构在项目的每一次迭代都会发生变化和发展，架构师和开发团队之间的紧密合作是任何软件项目成功的关键。\n技术广度 # 如果说开发人员必须具备很好的技术深度才能完成工作，那么架构师必须具备大量的技术广度来思考问题。\n下图代表知识金字塔，包括三类知识：\n你知道的东西(Stuff you know)：日常工作中使用的技术、框架、语言和工具； 你知道你不知道的东西(Stuff you know you don\u0026rsquo;t know)：略知一二但没有深入理解或没有专业知识的东西，例如：你可能听过 Clojure，但是不知道怎么使用这种语言进行编码； 你不知道你不知道的东西(Stuff you don\u0026rsquo;t know you don\u0026rsquo;t know)：你不知道这些东西的存在，是知识三角中最大的一部分； 对开发人员来说，最重要的是顶部的部分。\n随着开发人员向架构师角色的过渡，知识的性质发生了变化。架构师的很大一部分价值如何使用技术解决特定问题。例如，作为一名架构师，知道针对特定问题存在五种解决方案比只在一种解决方案上拥有单一的专业知识更有利。\n对架构师来说，金字塔最重要的部分是顶部和中间部分。\n对于架构师来说，明智的做法是牺牲一些很难学到（hard-won）的专业知识，利用这段时间来拓宽自己的广度。这也是一种取舍。\n如下图，“你知道的东西”变小了，只保留一些技术深度（渗透下来的绿色），用来换取技术广度。\n知识金字塔说明了架构师和开发人员的不同。\n开发人员用他们的整个职业生涯来磨练专业知识，过渡到架构师的角色意味着这种观点的转变。这对许多人来说很难，常导致两个问题：\n试图在每个领域都保持专业性，导致任何一个领域都不成功，把工作做得很粗糙； 错误地以为自己陈旧的知识仍然是前沿的，经常在大公司看到这种情况； 向架构师角色过渡的开发人员必须改变他们看待知识获取的方式，平衡关于深度与广度的知识组合是每个开发人员在整个职业生涯中应该考虑的问题。\n分析权衡 # 架构就是你没法用 Google 搜索的东西。\n架构中的一切都是权衡。\n每个架构问题的答案都包含了“这取决于……”，你没法在 Google 搜索是 REST 还是消息传递更好，是微服务好还是单体架构更好？因为它确实取决于，取决于部署环境、业务、公司文化、预算、时间、开发人员的技能组合以及其他几十个因素。\n架构之所以这么难，因为每个人的环境、情况和问题都不一样。\n在架构中没有正确或错误的答案，只有权衡。\n考虑一个拍卖系统，有以下两种数据消费模式。\n图 6: 发布-订阅（pub-sub）消息传递。 图 7：队列，点对点消息传递。 发布-订阅模型的优势：\n假如我们要增加一个“竞价历史”新服务，则完全不需要对现有系统进行任何修改；而在队列模型中，我们可能需要修改生产者添加一个队列； 解耦：生产者不需要知道数据有哪些服务在使用、如何去使用；而在队列模型中，生产者需要知道是什么类型的数据，发送给谁。 **架构师的思维需要看到方案的好处和坏处。**队列模型的优势：\n任何人都能访问发布-订阅模型的数据，存在数据访问和安全问题。 发布-订阅模型只能接受相同格式的数据，假设新的“竞价历史”服务需要当前的售价以及竞价，但其他服务原本没有这些信息，在这种情况下需要修改数据格式，并且会影响使用该数据的所有其他服务。在队列模型中，这将是一个单独的通道，因此是一个单独的格式，不影响任何其他服务。 发布-订阅模型不支持监控某个主题的消息数量，导致不支持自动缩放。在队列中很容易知道哪个队列消息量大，独立地自动伸缩。请注意，这种权衡是特定于技术的，高级消息队列协议（Advanced Message Queuing Protocol，AMQP）可以支持负载均衡和监控。 鉴于这种权衡分析，现在哪个是更好的选择？答案是什么呢? 这就要看情况了!\n理解业务 # 架构思维就是要理解系统成功所需的业务因素，并将这些需求转化为架构特性（如可扩展性、性能和可用性）。\n这是一项具有挑战性的任务，要求架构师具有一定程度的业务领域知识，并与关键业务利益相关者建立健康的协作关系。\n平衡架构和编码 # 架构师面临的困难任务之一是如何平衡架构和编码。\n每个架构师都应该进行编码，并且能够保持一定的技术深度（参见“技术广度”）。虽然这看起来似乎是一个简单的任务，但有时却相当难以完成。\n架构师需要避免瓶颈陷阱。当架构师掌握项目关键路径内的代码（通常是底层框架代码）的所有权，并成为团队的瓶颈时，就会出现瓶颈陷阱**。架构师不是全职开发人员，需要在开发人员（编写和测试代码）和架构师（画图、参加会议，以及参加更多的会议）之间取得平衡。**\n避免瓶颈陷阱的方法：将关键路径和框架代码委托给开发团队中的其他人，然后集中精力在一到三次迭代后对一个业务功能进行编码。\n架构师如何才能保持亲力亲为并保持一定的技术深度呢？有四种基本方法可以让架构师在工作中练习，而不必“在家练习编码”（尽管也建议在家里练习编码）：\n经常做 POC（proof-of-concept），通过考虑实现细节来验证架构决策。例如，如果架构师在两种缓存解决方案中无法抉择，那么可以每种缓存开发一个实例，并进行对比。 处理一些技术债务或架构问题，让开发团队腾出时间来处理关键的功能开发。这些问题通常是低优先级的，一般不会影响迭代。还可以在迭代中修复 bug，在帮助开发团队的同时也保持了编码，还可以找出代码库可能存在的问题和弱点。 通过创建简单的命令行工具和分析工具来帮助开发团队自动化完成日常任务，寻找开发团队执行的重复性任务，并将这个过程自动化。开发团队会感谢自动化。 经常做 code review。虽然并不是实际写代码，但至少参与了源代码的编写。此外，做 code review 还能确保代码符合架构的要求，帮助和指导开发。 ","date":"2021-01-08","permalink":"/posts/202101-fosa2/","section":"Posts","summary":"《软件架构基础（Fundamentals of Software Architecture）》读书笔记（2）","title":" 软件架构基础 2：架构思维"},{"content":"《软件架构基础（Fundamentals of Software Architecture）》被誉为和《设计数据密集型应用》一样经典的后端书籍，架构师的入门指南。本篇为该书第一章的读书笔记。\n如今，全球范围内“架构师”这一头衔炒得十分火热，但没有真正的指南来帮助开发人员成为软件架构师。\n这本书主要有三部分内容：基础、架构风格、技术和软技能。基础部分是关于软件体系结构的一般概念；架构风格部分介绍了不同的架构风格，并以一些架构特征标准进行评价；技术和软技能部分涵盖了很多好的概念，包括做出健康的架构决策、风险分析技术、演讲能力、管理团队关系、谈判、架构师职业规划等。\n架构就是关于重要的东西\u0026hellip;\u0026hellip;不管那是什么。—— Ralph Johnson\n1. 什么是软件架构 # 学习架构就像学习艺术一样，读者必须要在特定的背景下去理解它。\n在一个动态系统中，不存在一劳永逸的解决方案。\n学习架构时，必须放在上下文中理解。架构师做的许多决定都是基于他们所处的实际情况。\n例如，在 20 世纪末的主要目标是有效地利用共享资源，因为当时所有的基础设施都是昂贵的商业化产品：操作系统、服务器和数据库等等。如果你在 2002 年告诉主管，“我有一个革命性的架构好主意，每个服务都运行在自己隔离的机器上，有自己的专用数据库……（即描述今天的微服务架构）所以，我需要 50 个 Windows 的许可证，另外 30 个服务器许可证，以及至少 50 个数据库许可证。”在 2002 年想构建这样的微服务架构成本之高难以想象。然而这几年，随着开源运动的兴起，以及 DevOps 的出现，我们可以合理地构建一个如上所述的架构。\n整个行业都在努力精确定义“软件架构”，有些称为系统的蓝图，有些定义为开发的路线图。本书关于架构的定义主要从四个方面：\n系统的结构（Structure） 系统所支持的架构特性、能力（Architecture characteristics） 架构决策（Architecture decisions） 设计原则（Design principles） 系统的结构指的是系统实现架构风格的类型（如微服务、分层或微内核）。但仅仅通过结构来描述一个架构，并不能完全阐明一个架构。\n架构特性多以 \u0026ldquo;-ility\u0026rdquo; 结尾（例如 Availability、Scalability）\n架构决策定义了系统应该如何构建的规则。例如，架构师可能会做出一个架构决策，即在分层架构中只有业务层和服务层可以访问数据库（见图），限制表现层直接调用数据库。架构决策形成了系统的约束，并指导开发团队什么是允许的，什么是不允许的。\n设计原则与架构决策的不同之处在于，设计原则是一个指导方针，而不是一个硬性规定。例如，图示的设计原则指出，开发团队应该在微服务架构中的服务之间传递异步消息来提高性能。一个架构决策永远不可能涵盖服务之间通信的每一个条件和选项，设计原则为首选方法（在本例中，异步消息传递）提供指导，允许开发人员在特定情况下选择更合适的通信协议（如 REST 或 gRPC）。\n2. 对架构师的 8 个期待 # 2.1 做出架构决定 # 架构师应确定架构和设计原则，用于指导团队、部门或整个企业的技术决策。\n架构师应该指导而不是指定技术选择。例如，架构师应该决定开发团队使用基于响应式 (Reactive) 的框架进行开发，从而指导开发团队在 Angular、Elm、React.js、Vue 或其他任何基于响应式的 Web 框架之间做出选择。\n架构师偶尔需要做出特定的技术决策，以保留特定的架构特性，如可扩展性、性能或可用性。\n架构师经常为找到正确的界线而苦恼。\n2.2 持续分析架构 # 多数架构都会经历结构性衰变，当开发人员进行编码或设计变更时，会影响到所需的架构特性，如性能、可用性和可扩展性。架构师需要评估三年或更长时间前定义的架构在今天的可行性。\n另一方面，架构师还常常忘记测试和发布环境。敏捷性有很大的好处，如果团队需要几周的时间来测试变更，而发布又需要几个月的时间，那么架构师就无法实现整体架构的敏捷性。\n2.3 紧跟最新趋势 # 架构师要跟上最新的技术和行业趋势。\n开发人员必须掌握他们每天使用的最新技术，以保持代码能力(并保住一份工作！)。架构师有一个更关键的要求，就是要掌握最新的技术和行业趋势。架构师所做的决定往往是长期的，难以改变的，了解和跟踪关键趋势有助于架构师为未来做好准备，做出正确的决定。\n2.4 确保遵守各项决定 # 架构师要不断验证开发团队是否遵循架构师定义、记录和传达的架构决策和设计原则。\n考虑这样的场景：架构师做出一个决定，在分层架构中限制对数据库的访问，只限于业务层和服务层（而不是表现层）。这意味着表现层必须经过架构的所有层才能进行最简单的数据库调用。用户界面开发人员可能不同意这个决定，出于性能考虑而直接访问数据库。然而，架构师做出这个架构决策是有特定原因的：控制变化。通过保持各层独立，可以在不影响表现层的情况下进行数据库变更。如果不确保架构决策的合规性，就会发生类似这样的违规行为，架构将无法满足所需的架构特性，应用程序或系统将无法按预期工作。\n2.5 多样化的接触和经验 # 架构师要接触多种多样的技术、框架、平台和环境。\n现在的大多数环境都是异构的，一个架构师至少应该知道如何与多个系统和服务对接，不管这些系统或服务是用什么语言、平台和技术编写的。\n最好方法之一是让架构师延伸自己的舒适区，只关注单一技术或平台是一个安全的避风港，一个好的软件架构师应该积极寻找机会，以获得多种语言、平台和技术的经验，关注技术广度而不只是技术深度。\n2.6 具备业务领域知识 # 一个架构师要有一定的业务领域专业知识。\n有效的软件架构师不仅了解技术，还了解业务问题。如果没有业务领域的知识，就很难理解业务问题、目标和需求，很难设计出满足业务需求的有效架构。\n最成功的架构师是那些拥有广泛的、实践性的技术知识，再加上对某一特定领域的深刻了解的人。这些软件架构师能够使用这些利益相关者所能理解的领域知识和语言，与主管和业务用户进行有效的沟通。\n2.7 具备人际交往能力 # 架构师应具备卓越的人际交往能力，包括团队合作、促进和领导能力。\n拥有卓越的领导力和人际交往能力是大多数开发人员和架构师难以企及的期望。作为技术专家，开发人员和架构师喜欢解决技术问题，而不是人的问题。\n架构师不仅要为团队提供技术指导，还要带领开发团队完成架构的实施。无论架构师的角色和头衔是什么，领导能力是成为一个软件架构师不可或缺的。\n2.8 理解和驾驭企业政治 # 架构师要了解企业的政治氛围，并能驾驭政治。\n在一本关于软件架构的书中谈论谈判和驾驭办公室政治，可能看起来比较奇怪。主要的一点是，几乎架构师的每一个决策都会受到挑战。由于涉及到成本或工作量（时间）的增加，架构决策会受到产品、项目经理、开发和业务利益相关者的挑战，因为他们觉得自己的方法更好。无论在哪种情况下，架构师都必须驾驭公司的政治，并应用基本的谈判技巧来获得批准。\n架构师不像开发，可以自行设计代码结构、类、设计模式甚至是语言而不需要批准，架构师做出广泛而重要的决策，必须为几乎每一个决策进行论证和争取。\n3. 架构师和其它的交集 # 3.1 工程实践 # 将软件开发过程与软件工程实践分开是有益的。所谓过程，我们指的是如何组建和管理团队，如何进行会议以及工作流组织，指的是人们如何组织和互动的机制。而**软件工程实践则是指那些已经说明了的、可重复效益的与过程无关的实践。**例如，持续集成是一种经过验证的工程实践，它不依赖于特定的过程。\n注重工程实践很重要：\n首先，软件开发缺乏许多比较成熟的工程学科的特点。例如，土木工程可以比软件工程更准确地预测结构变化。 其次，**软件开发的一个致命弱点是估算\u0026ndash;多少时间，多少资源，多少钱？**这种困难一部分在于陈旧的会计无法适应软件开发的探索性；但另一部分是因为我们传统上不擅长估算，部分原因是因为 unknown unknowns。 unknown unknowns 是软件系统的克星：没有人知道会出现的东西，却又意外地出现了。例如：某个意外的 bug 出现。\n所有的架构都会因为 unknown unknowns 而变成迭代式的，敏捷只是认识到了这一点，并且更早去做了。（All architectures become iterative because of unknown unknowns, Agile just recognizes this and does it sooner.）\n迭代流程更符合软件架构的本质，试图使用像瀑布这样的陈旧流程来构建微服务这样的现代系统的团队会发现，一个陈旧的流程忽视了软件如何结合在一起的现实，会产生大量的摩擦。\n如图所示，软件系统的架构由需求和所有其他架构特征组成。\n采用敏捷工程实践，如持续集成、自动机器供应和类似的实践，使构建弹性架构变得更容易。这也说明了架构与工程实践是如何相互交织在一起的。\n3.2 运维/DevOps # 架构和相关领域之间最近最明显的交集发生在 DevOps 的出现。许多公司认为运维是与软件开发是分离的，在 20 世纪 90 年代和 2000 年代设计的架构都假设架构师无法控制运维，架构师们被迫围绕引入的限制进行防御性设计。因此，他们构建了能够在内部处理规模、性能、弹性和其他一系列能力的架构。这种设计的副作用是大大增加了架构的复杂性。\n微服务风格架构的构建者们意识到，通过在架构和运维之间建立一个联络点，架构师可以简化设计，依靠运维来处理他们最擅长的事情。因此，意识到资源的挪用导致了意外的复杂性，架构师和运维合作创建了微服务。\n3.3 流程 # 软件架构与软件开发过程大多是正交的（相互不可替代的），大多数关于软件架构的书籍都忽略了软件开发过程。例如，在过去的几十年里，由于软件的性质，许多公司都采用了敏捷开发方法。架构师在敏捷项目中得到更快的反馈，这使得架构师可以更积极地进行实验。\n所有的架构都会变成迭代式的，这只是时间问题。为此，我们要在整个过程中假设敏捷方法论的基线，并允许适当的例外。例如，许多单体架构因为年龄、政治或其他与软件无关的因素而使用旧流程的情况还是很常见的。\n3.4 数据 # 很大一部分严肃的应用程序开发包括外部数据存储，通常采用关系型(或越来越多的 NoSQL)数据库的形式。然而，许多关于软件架构的书籍只对架构的这一重要方面进行了轻描淡写的处理。代码和数据具有共生关系：两者缺一不可。\n4. 软件架构法则 # 软件架构第一定律： 软件架构中的所有东西都是一种权衡。(Everything in software architecture is a trade-off.)\n我们对软件架构的定义超越了结构的范畴，包含了原则、特性等，架构的范围比单纯的结构更广，体现在我们的软件架构第二定律中： 为什么比怎么做更重要。（Why is more important than how.）\n","date":"2021-01-07","permalink":"/posts/202101-fosa1/","section":"Posts","summary":"《软件架构基础（Fundamentals of Software Architecture）》读书笔记（1）","title":" 软件架构基础 1：基本介绍"},{"content":"前文《理解 Paxos》只包含伪代码，帮助了理解但又不够爽，既然现在都讲究 Talk is cheap. Show me the code. 这次就把文章中的伪代码用 Go 语言实现出来，希望能帮助各位朋友更直观的感受 Paxos 论文中的细节。\n但我们需要对算法做一些简化，有多简单呢？我们不持久化存储任何变量，并且用 chan 直接代替 RPC 调用。\n代码地址：https://github.com/tangwz/paxos/tree/naive\n记得切换到 naive 分支。\n定义相关结构体 # 我们定义 Proposer 如下：\ntype proposer struct { // server id id int // the largest round number the server has seen round int // proposal number = (round number, serverID) number int // proposal value value string acceptors map[int]bool net network } 这些结构体成员都很容易理解，其中 acceptors 我们主要用来存储 Acceptors 的地址，以及记录我们收到 Acceptor 的成功/失败响应。\nAcceptor 的结构体：\ntype acceptor struct { // server id id int // the number of the proposal this server will accept, or 0 if it has never received a Prepare request promiseNumber int // the number of the last proposal the server has accepted, or 0 if it never accepted any. acceptedNumber int // the value from the most recent proposal the server has accepted, or null if it has never accepted a proposal acceptedValue string learners []int net network } 主要成员解释都有注释，简单来说我们需要记录三个信息：\npromiseNumber： 承诺的提案编号 acceptedNumber： 接受的提案编号 acceptedValue： 接受的提案值 定义消息结构体 # 消息结构体定义了 Proposer 和 Acceptor 之间、Acceptor 和 Leaner 之间的通讯协议。最主要的还是 Paxos 的两阶段的四个消息。\nPhase 1 请求：提案编号 Phase 1 响应：如果有被 Accepted 的提案，返回提案编号和提案值 Phase 2 请求：提案编号和提案值 Phase 2 响应：Accepted 的提案编号和提案值 这样看，我们的消息结构体只需要提案编号和提案值，加上一个消息类型，用来区分是哪个阶段的消息。消息结构体定义在 message.go 文件，具体如下：\n// MsgType represents the type of a paxos phase. type MsgType uint8 const ( Prepare MsgType = iota Promise Propose Accept ) type message struct { tp MsgType from int to int number int // proposal number value string // proposal value } 实现网络 # 网络上可以做的选择和优化很多，但这里为了保持简单的原则，我们将网络定义成 interface。后面完全可以改成 RPC 或 API 等其它通信方式来实现（没错，我已经实现了一个 Go RPC 的版本了）。\ntype network interface { send(m message) recv(timeout time.Duration) (message, bool) } 接下里我们去实现 network 接口：\ntype Network struct { queue map[int]chan message } func newNetwork(nodes ...int) *Network { pn := \u0026amp;Network{ queue: make(map[int]chan message, 0), } for _, a := range nodes { pn.queue[a] = make(chan message, 1024) } return pn } func (net *Network) send(m message) { log.Printf(\u0026#34;net: send %+v\u0026#34;, m) net.queue[m.to] \u0026lt;- m } func (net *Network) recvFrom(from int, timeout time.Duration) (message, bool) { select { case m := \u0026lt;-net.queue[from]: log.Printf(\u0026#34;net: recv %+v\u0026#34;, m) return m, true case \u0026lt;-time.After(timeout): return message{}, false } } 就是用 queue 来记录每个节点的 chan，key 则是节点的 server id。\n发送消息则将 Message 发送到目标节点的 chan 中，接受消息直接从 chan 中读取数据，并等待对应的超时时间。\n不需要做其它网络地址、包相关的东西，所以非常简单。具体在 network.go 文件。\n实现单元测试 # 这个项目主要使用 go 单元测试来检验正确性，我们主要测试两种场景：\nTestSingleProposer（单个 Proposer） TestTwoProposers（多个 Proposer） 测试代码通过运行 Paxos 后检查 Chosen 返回的提案值是否符合预期。\n实现算法流程 # 按照角色将文件分为 proposer.go, acceptor.go 和 learner.go，每个文件都有一个 run() 函数来运行程序，run() 函数执行条件判断，并在对应的阶段执行对应的函数。\n按照伪代码描述，我们很容易实现 Phase 1 和 Phase 2，把每个阶段的请求响应都作为一个函数，我们一步步来看。\n第一轮 Prepare RPCs 请求阶段： # // Phase 1. (a) A proposer selects a proposal number n // and sends a prepare request with number n to a majority of acceptors. func (p *proposer) prepare() []message { p.round++ p.number = p.proposalNumber() msg := make([]message, p.majority()) i := 0 for to := range p.acceptors { msg[i] = message{ tp: Prepare, from: p.id, to: to, number: p.number, } i++ if i == p.majority() { break } } return msg } // proposal number = (round number, serverID) func (p *proposer) proposalNumber() int { return p.round\u0026lt;\u0026lt; 16 | p.id } Prepare 请求阶段我们将 round+1 然后发送给多数派 Acceptors。\n注：这里很多博客和教程都会将 Prepare RPC 发给所有的 Acceptors，6.824 的 paxos 实验就将 RPC 发送给所有 Acceptors。这里保持和论文一致，只发送给 a majority of acceptors。\n第一轮 Prepare RPCs 响应阶段： # 接下来在 acceptor.go 文件中处理请求：\nfunc (a *acceptor) handlePrepare(args message) (message, bool) { if a.promiseNumber \u0026gt;= args.number { return message{}, false } a.promiseNumber = args.number msg := message{ tp: Promise, from: a.id, to: args.from, number: a.acceptedNumber, value: a.acceptedValue, } return msg, true } 如果 args.number 大于 acceptor.promiseNumber，则承诺将不会接收编号小于 args.number 的提案（即 a.promiseNumber = args.number）。如果之前有提案被 Accepted 的话，响应还应包含 a.acceptedNumber 和 a.acceptedValue。 否则忽略，返回 false。 第二轮 Accept RPCs 请求阶段： # func (p *proposer) accept() []message { msg := make([]message, p.majority()) i := 0 for to, ok := range p.acceptors { if ok { msg[i] = message{ tp: Propose, from: p.id, to: to, number: p.number, value: p.value, } i++ } if i == p.majority() { break } } return msg } 当 Proposer 收到超过半数 Acceptor 的响应后，Proposer 向多数派的 Acceptor 发起请求并带上提案编号和提案值。\n第二轮 Accept RPCs 响应阶段： # func (a *acceptor) handleAccept(args message) bool { number := args.number if number \u0026gt;= a.promiseNumber { a.acceptedNumber = number a.acceptedValue = args.value a.promiseNumber = number return true } return false } Acceptor 收到 Accept() 请求，在这期间如果 Acceptor 没有对比 a.promiseNumber 更大的编号另行 Promise，则接受该提案。\n别忘了：Learning a Chosen Value # 在 Paxos 中有一个十分容易混淆的概念：Chosen Value 和 Accepted Value，但如果你看过论文，其实已经说得非常直接了。论文的 2.3 节 Learning a Chosen Value 开头就说：\nTo learn that a value has been chosen, a learner must find out that a proposal has been accepted by a majority of acceptors.\n所以 Acceptor 接受提案后，会将接受的提案广播 Leaners，一旦 Leaners 收到超过半数的 Acceptors 的 Accepted 提案，我们就知道这个提案被 Chosen 了。\nfunc (l *learner) chosen() (message, bool) { acceptCounts := make(map[int]int) acceptMsg := make(map[int]message) for _, accepted := range l.acceptors { if accepted.number != 0 { acceptCounts[accepted.number]++ acceptMsg[accepted.number] = accepted } } for n, count := range acceptCounts { if count \u0026gt;= l.majority() { return acceptMsg[n], true } } return message{}, false } 运行和测试 # 代码拉下来后，直接运行：\ngo test 写在后面 # 为什么不用 mit 6.824 的课程代码？ # 之前我曾把 mit 6.824 的 Raft 答案推到自己的 Github，直到 2020 开课的时候 mit 的助教发邮件让我将我的代码转为 private，因为这样会导致学习课程的人直接搜到代码，而无法保证作业独立完成。\n确实，实验是计算机最不可或缺的环节，用 mit 6.824 2015 的 paxos 代码会导致很多学习者不去自己解决困难，直接上网搜代码，从而导致学习效果不好，违背了 mit 的初衷。\n当然，你也可以说现在网上以及很容易搜到 6.824 的各种代码了，但出于之前 mit 助教的邮件，我不会将作业代码直接发出来。\n感兴趣的同学可以到 2015 版本学习：http://nil.csail.mit.edu/6.824/2015/\n未来计划 # 实现一个完整的（包含网络和存储的） Paxos 基于 Paxos 实现一个 Paxos KV 存储 实现其它 Paxos 变种 欢迎各位朋友催更……\n结语 # 本文代码在 Github 上，如本文有什么遗漏或者不对之处，或者各位朋友有什么新的想法，欢迎提 issue 讨论。\n","date":"2020-12-13","permalink":"/posts/202012-impl-basic-paxos/","section":"Posts","summary":"前文《理解 Paxos》只包含伪代码，帮助了理解但又不够爽，既","title":"Golang 实现 Paxos 分布式共识算法"},{"content":"leveldb 是一个持久化的 key/value 存储，key 和 value 都是任意的字节数组(byte arrays)，并且在存储时，key 值根据用户指定的 comparator 函数进行排序。\n作者是大名鼎鼎的 Jeff Dean 和 Sanjay Ghemawat.\n基本介绍 # 特性 # keys 和 values 是任意的字节数组。 数据按 key 值排序存储。 调用者可以提供一个自定义的比较函数来重写排序顺序。 提供基本的 Put(key,value)，Get(key)，Delete(key) 操作。 多个更改可以在一个原子批处理中生效。 用户可以创建一个瞬时快照(snapshot)，以获得数据的一致性视图。 在数据上支持向前和向后迭代。 使用 Snappy 压缩库对数据进行自动压缩 与外部交互的操作都被抽象成了接口(如文件系统操作等)，因此用户可以根据接口自定\u0010义的操作系统交互。 局限性 # 这不是一个 SQL 数据库，它没有关系数据模型，不支持 SQL 查询，也不支持索引。 同时只能有一个进程(可能是具有多线程的进程)访问一个特定的数据库。 该程序库没有内置的 client-server 支持，有需要的用户必须自己封装。 性能 # 下面是运行 db_bench 程序的性能报告。结果有一些噪声(noisy)，但足以得到一个大概的性能估计。\n配置 # 我们使用的是一个有一百万个条目的数据库，其中每个条目的 key 是 16 字节，value 是 100 字节，value 压缩后大约是原始大小的一半，测试配置如下:\nLevelDB: version 1.1 Date: Sun May 1 12:11:26 2011 CPU: 4 x Intel(R) Core(TM)2 Quad CPU Q6600 @ 2.40GHz CPUCache: 4096 KB Keys: 16 bytes each Values: 100 bytes each (50 bytes after compression) Entries: 1000000 Raw Size: 110.6 MB (estimated) File Size: 62.9 MB (estimated) 具体 benchmark 源码见：https://github.com/google/leveldb/blob/master/benchmarks/db_bench.cc\n写性能 # “fill” 基准测试创建了一个全新的数据库，以顺序(下面的 “fillseq”)或者随机(下面的 “fillrandom”)方式写入。\n“fillsync” 基准测试每次写操作都将数据从操作系统刷到磁盘; 其它的操作会将数据保存在系统中一段时间。\n“overwrite” 基准测试做随机写，会更新数据库中已有的 key。\nfillseq : 1.765 micros/op; 62.7 MB/s fillsync : 268.409 micros/op; 0.4 MB/s (10000 ops) fillrandom : 2.460 micros/op; 45.0 MB/s overwrite : 2.380 micros/op; 46.5 MB/s 上述每个 “op” 对应一个 key/value 对的写操作。即，一个随机写基准测试每秒约四十万次写操作(1,000,000/2.46)。\n每个 “fillsync” 操作耗时(大约 0.3 毫秒)少于一次磁盘搜索(大约 10 毫秒)。我们怀疑这是因为磁盘本身将更新操作缓存到了内存，并在数据落盘之前返回响应。这可能是安全的，也可能是不安全的，取决于硬盘是否有足够的电力在断电时保存其内存。\n读性能 # 我们列出了正向顺序读、反向顺序读以及随机查询的性能。注意，基础测试创建的数据库很小，因此该性能报告描述的是 leveldb 的全部数据集能放入到内存的场景，如果数据不在操作系统缓存中，读取的性能消耗主要在于一到两次的磁盘搜索，写性能基本不会受数据集是否能放入内存的影响。\nreadrandom : 16.677 micros/op; (approximately 60,000 reads per second) readseq : 0.476 micros/op; 232.3 MB/s readreverse : 0.724 micros/op; 152.9 MB/s leveldb 会在后台 compact 其底层存储的数据来改善读性能。上面列出的结果是在大量随机写操作后得出的，经过 compact 后的性能指标（通常是指动出发的）会更好：\nreadrandom : 11.602 micros/op; (approximately 85,000 reads per second) readseq : 0.423 micros/op; 261.8 MB/s readreverse : 0.663 micros/op; 166.9 MB/s 读操作消耗高的成本一部分来自于重复解压从磁盘读取的数据库，如果我们能够提供足够的缓存给 leveldb 来将解压后的数据保存在内存中，读性能会进一步改善：\nreadrandom : 9.775 micros/op; (approximately 100,000 reads per second before compaction) readrandom : 5.215 micros/op; (approximately 190,000 reads per second after compaction) 编译 # 项目支持 Cmake 开箱即用。编译非常简单：\ngit clone --recurse-submodules https://github.com/google/leveldb.git mkdir -p build \u0026amp;\u0026amp; cd build cmake -DCMAKE_BUILD_TYPE=Release .. \u0026amp;\u0026amp; cmake --build . 头文件介绍 # leveldb 对外暴露的接口都在 include/*.h 中，用户不应该依赖任何其它目录下的头文件，这些内部 API 可能会在没有警告的情况下被改变。\ninclude/leveldb/db.h：主要的 DB 接口，从这开始。 include/leveldb/options.h： 控制数据库的行为，也控制当个读和写的行为。 include/leveldb/comparator.h： 比较函数的抽象。如果你只想对 key 逐字节比较，可以直接使用默认的比较器。如果你想要自定义排序（例如处理不同的字符编码、解码等），可以实现自己的比较器。 include/leveldb/iterator.h：迭代数据的接口，你可以从一个 DB 对象获取到一个迭代器。 include/leveldb/write_batch.h：原子地将多个操作应用到数据库。 include/leveldb/slice.h：类似 string，维护着指向字节数组的指针和对应的长度。 include/leveldb/status.h：许多公共接口都会返回 Status，用于报告成功或各种错误。 include/leveldb/env.h：操作系统环境的抽象，该接口的 posix 实现位于 util/env_posix.cc 中. include/leveldb/table.h, include/leveldb/table_builder.h：底层的模块，大多数用户可能不会直接用到。 使用 # 编译以后我们可以使用 cmake 来小试牛刀，首先在 leveldb 目录创建文件夹 app/ 来单独存放我们的练习文件，然后创建一个文件例如：main.cc，接着我们修改 CMakeLists.txt 文件，增加一行：\n347 if(NOT BUILD_SHARED_LIBS) + 348 leveldb_test(\u0026#34;app/main.cc\u0026#34;) 349 leveldb_test(\u0026#34;db/autocompact_test.cc\u0026#34;) 编写完代码后，只需回到 build/ 目录执行：\ncmake .. \u0026amp;\u0026amp; cmake --build . 即可编译出 main 可执行文件。\n打开一个数据库 # leveldb 数据库都有一个名字，该名字对应文件系统上的一个目录，该数据库内容全都存在该目录下。下面的例子显示了如何打开一个数据库，必要时创建数据库：\n#include \u0026lt;cassert\u0026gt; #include \u0026#34;leveldb/db.h\u0026#34; int main() { leveldb::DB* db; leveldb::Options options; options.create_if_missing = true; leveldb::Status status = leveldb::DB::Open(options, \u0026#34;/tmp/testdb\u0026#34;, \u0026amp;db); assert(status.ok()); } 如果你想在数据库已存在的时候触发一个异常，将下面这行配置加到 leveldb::DB::Open 调用之前：\noptions.error_if_exists = true; Status # 你也许注意到上面的 leveldb::Status 返回类型，leveldb 中大部分方法在遇到错误的时候会返回该类型的值，你可以检查它是否 ok，然后打印相关的错误信息：\nleveldb::Status s = ...; if (!s.ok()) cerr \u0026lt;\u0026lt; s.ToString() \u0026lt;\u0026lt; endl; 尝试输出数据库已存在的错误信息吧！\n关闭数据库 # 当数据库不再使用的时候，像下面这样直接删除数据库对象就可以了：\ndelete db; 是不是很简单？后面我们具体源码分析时会看到，DB 类是基于 RAII 实现的，在 delete 时会触发析构函数自动清理。\n数据库读写 # leveldb 提供了 Put、Delete 和 Get 方法来修改/查询数据库，下面的代码展示了将 key1 对应的 value 移动到 key2 下。\nstd::string value; leveldb::Status s = db-\u0026gt;Get(leveldb::ReadOptions(), key1, \u0026amp;value); if (s.ok()) s = db-\u0026gt;Put(leveldb::WriteOptions(), key2, value); if (s.ok()) s = db-\u0026gt;Delete(leveldb::WriteOptions(), key1); 原子更新 # 需要注意的是，在上一小节中如果进程在 Put key2 后 Delete key1 之前挂了，那么同样的 value 将被存储在多个 key 下。可以通过使用 WriteBatch 原子地应用一组操作来避免类似的问题。\n#include \u0026#34;leveldb/write_batch.h\u0026#34; ... std::string value; leveldb::Status s = db-\u0026gt;Get(leveldb::ReadOptions(), key1, \u0026amp;value); if (s.ok()) { leveldb::WriteBatch batch; batch.Delete(key1); batch.Put(key2, value); s = db-\u0026gt;Write(leveldb::WriteOptions(), \u0026amp;batch); } WriteBatch 保存着一系列将被应用到数据库的操作，这些操作会按照添加的顺序依次被执行。注意，我们先执行 Delete 后执行 Put，这样如果 key1 和 key2 一样的情况下我们也不会错误地丢失数据。\n除了原子性，WriteBatch 也能加速更新过程，因为可以把一大批独立的操作添加到同一个 batch 中然后一次性执行。\n同步写操作 # 默认情况下，leveldb 每个写操作都是异步的：进程把要写的内容丢给操作系统后立即返回，从操作系统内存到底层持久化存储的传输是异步进行的。\n可以为某个特定的写操作打开同步标识：write_options.sync = true，以等到数据真正被记录到持久化存储后再返回（在 Posix 系统上，这是通过在写操作返回前调用 fsync(...) 或 fdatasync(...) 或 msync(..., MS_SYNC) 来实现的）。\nleveldb::WriteOptions write_options; write_options.sync = true; db-\u0026gt;Put(write_options, ...); **异步写通常比同步写快 1000 倍。**异步写的缺点是，一旦机器崩溃可能会导致最后几个更新操作丢失。注意，仅仅是写进程崩溃（而非机器重启）不会造成任何损失，因为哪怕 sync 标识为 false，在进程退出之前，写操作也已经从进程内存推到了操作系统。\n异步写通常可以安全使用。比如你要将大量的数据写入数据库，如果丢失了最后几个更新操作，也可以重做整个写过程。如果数据量非常大，一个优化点是采用混合方案，每进行 N 个异步写操作则进行一次同步写，如果期间发生了崩溃，重启从上一个成功的同步写操作开始即可。（同步写操作可以同时更新一个标识，描述崩溃时重启的位置）\nWriteBatch 可以作为异步写操作的替代品，多个更新操作可以放到同一个 WriteBatch 中然后通过一次同步写(即 write_options.sync = true)一起落盘。\n并发 # 一个数据库同时只能被一个进程打开。leveldb 会从操作系统获取一把锁来防止多进程同时打开同一个数据库。在单个进程中，同一个 leveldb::DB 对象可以被多个并发线程安全地使用，也就是说，不同的线程可以在不需要任何外部同步原语的情况下，写入、获取迭代器或者调用 Get（leveldb 实现会确保所需的同步）。但是其它对象，比如 Iterator 或者 WriteBatch 需要外部自己提供同步保证，如果两个线程共享此类对象，需要使用自己的锁进行互斥访问。具体见对应的头文件。\n迭代数据库 # 下面的用例展示了如何打印数据库中全部的 (key, value) 对。\nleveldb::Iterator* it = db-\u0026gt;NewIterator(leveldb::ReadOptions()); for (it-\u0026gt;SeekToFirst(); it-\u0026gt;Valid(); it-\u0026gt;Next()) { cout \u0026lt;\u0026lt; it-\u0026gt;key().ToString() \u0026lt;\u0026lt; \u0026#34;: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;value().ToString() \u0026lt;\u0026lt; endl; } assert(it-\u0026gt;status().ok()); // Check for any errors found during the scan delete it; 下面的用例展示了如何打印 [start, limit) 范围内的数据：\nfor (it-\u0026gt;Seek(start); it-\u0026gt;Valid() \u0026amp;\u0026amp; it-\u0026gt;key().ToString() \u0026lt; limit; it-\u0026gt;Next()) { ... } 当然你也可以反向遍历（注意，反向遍历可能要比正向遍历慢一些，具体见前面的读性能基准测试）：\nfor (it-\u0026gt;SeekToLast(); it-\u0026gt;Valid(); it-\u0026gt;Prev()) { ... } 快照 # 快照提供了针对整个 KV 存储的一致性只读视图（consistent read-only views）。ReadOptions::snapshot 不为 null 表示读操作应该作用在 DB 的某个特定版本上；若为 null，则读操作将会作用在当前版本的一个隐式的快照上。\n快照通过调用 DB::GetSnapshot() 方法创建：\nleveldb::ReadOptions options; options.snapshot = db-\u0026gt;GetSnapshot(); ... apply some updates to db ... leveldb::Iterator* iter = db-\u0026gt;NewIterator(options); ... read using iter to view the state when the snapshot was created ... delete iter; db-\u0026gt;ReleaseSnapshot(options.snapshot); 注意，当一个快照不再使用的时候，应该通过 DB::ReleaseSnapshot 接口进行释放。\nSlice # it-\u0026gt;key() 和 it-\u0026gt;value() 调用返回的值是 leveldb::Slice 类型。熟悉 Go 的同学应该对 Slice 不陌生。Slice 是一个简单的数据结构，包含一个长度和一个指向外部字节数组的指针，返回一个 Slice 比返回一个 std::string 更高效，因为不需要隐式地拷贝大量的 keys 和 values。另外，leveldb 方法不返回 \\0 截止符结尾的 C 风格字符串，因为 leveldb 的 keys 和 values 允许包含 \\0 字节。\nC++ 风格的 string 和 C 风格的空字符结尾的字符串很容易转换为一个 Slice：\nleveldb::Slice s1 = \u0026#34;hello\u0026#34;; std::string str(\u0026#34;world\u0026#34;); leveldb::Slice s2 = str; 一个 Slice 也很容易转换回 C++ 风格的字符串：\nstd::string str = s1.ToString(); assert(str == std::string(\u0026#34;hello\u0026#34;)); 在使用 Slice 时要小心，要由调用者来确保 Slice 指向的外部字节数组有效。例如，下面的代码就有 bug ：\nleveldb::Slice slice; if (...) { std::string str = ...; slice = str; } Use(slice); 当 if 语句结束的时候，str 将会被销毁，Slice 的指向也随之消失，后面再用就会出问题。\n比较器（Comparator） # 前面的例子中用的都是默认的比较函数，即逐字节按字典序比较。你可以自定义比较函数，然后在打开数据库的时候传入，只需要继承 leveldb::Comparator 然后定义相关逻辑即可，下面是一个例子：\nclass TwoPartComparator : public leveldb::Comparator { public: // Three-way comparison function: // if a \u0026lt; b: negative result // if a \u0026gt; b: positive result // else: zero result int Compare(const leveldb::Slice\u0026amp; a, const leveldb::Slice\u0026amp; b) const { int a1, a2, b1, b2; ParseKey(a, \u0026amp;a1, \u0026amp;a2); ParseKey(b, \u0026amp;b1, \u0026amp;b2); if (a1 \u0026lt; b1) return -1; if (a1 \u0026gt; b1) return +1; if (a2 \u0026lt; b2) return -1; if (a2 \u0026gt; b2) return +1; return 0; } // Ignore the following methods for now: const char* Name() const { return \u0026#34;TwoPartComparator\u0026#34;; } void FindShortestSeparator(std::string*, const leveldb::Slice\u0026amp;) const {} void FindShortSuccessor(std::string*) const {} }; 在打开数据库的时候，传入上面定义的比较器：\n// 实例化比较器 TwoPartComparator cmp; leveldb::DB* db; leveldb::Options options; options.create_if_missing = true; // 将比较器赋值给 options.comparator options.comparator = \u0026amp;cmp; // 打开数据库 leveldb::Status status = leveldb::DB::Open(options, \u0026#34;/tmp/testdb\u0026#34;, \u0026amp;db); ... 向后兼容性 # 比较器 Name() 方法返回的结果在创建数据库时会被绑定到数据库上，后续每次打开都会进行检查，如果名称改了，对 leveldb::DB::Open 的调用就会失败。因此，当且仅当在新的 key 格式和比较函数与已有的数据库不兼容而且已有数据不再被需要的时候再修改比较器名称。总而言之，一个数据库只能对应一个比较器，而且比较器由名字唯一确定，一旦修改名称或比较器逻辑，数据库的操作逻辑统统会出错，毕竟 leveldb 是一个有序的 KV 存储。\n如果非要修改比较逻辑呢？你可以根据预先规划一点一点的演进你的 key 格式，注意，事先的演进规划非常重要。比如，你可以在每个 key 的结尾存储一个版本号（大多数场景，一个字节足矣），当你想要切换到新的 key 格式的时候（比如上面的例子 TwoPartComparator 处理的 keys 中），那么需要做的是：\n保持相同的比较器名称 递增新 keys 的版本号 修改比较器函数以让其使用版本号来决定如何进行排序 性能调优 # 通过修改 include/leveldb/options.h 中定义的类型的默认值来对 leveldb 的性能进行调优。\nBlock 大小 # leveldb 把相邻的 keys 组织在同一个 block 中(具体见后续文章针对 sstable 文件格式的描述)，block 是数据在内存和持久化存储传输之间的基本单位。默认的未压缩 block 大小大约为 4KB，经常批量扫描大量数据的应用可能希望把这个值调大，而针对数据只做“点读”的应用则可能希望这个值小一些。但是，没有证据表明该值小于 1KB 或者大于几个 MB 的时候性能会表现得更好。另外要注意的是，使用较大的 block size，压缩效率会更高效。\n压缩 # 每个 block 在写入持久化存储之前都会被单独压缩。压缩默认是开启的，因为默认的压缩算法非常快，而且对于不可压缩的数据会自动关闭压缩功能，极少有场景会让用户想要完全关闭压缩功能，除非基准测试显示关闭压缩会显著改善性能。按照下面方式即可关闭压缩功能：\nleveldb::Options options; options.compression = leveldb::kNoCompression; ... leveldb::DB::Open(options, name, ...) .... 缓存 # 数据库的内容存储在文件系统中的一组文件中，每个文件都存储了一系列压缩后的 blocks，如果 options.block_cache 是非 NULL，则用于缓存经常使用的已解压缩 block 内容。\n#include \u0026#34;leveldb/cache.h\u0026#34; leveldb::Options options; options.block_cache = leveldb::NewLRUCache(100 * 1048576); // 100MB cache leveldb::DB* db; leveldb::DB::Open(options, name, \u0026amp;db); ... use the db ... delete db delete options.block_cache; 注意 cache 保存的是未压缩的数据，因此应该根据应用程序所需的数据大小来设置它的大小。（已压缩数据的缓存工作交给操作系统的 buffer cache 或者用户自定义的 Env 实现去干。）\n当执行一个大块数据读操作时，应用程序可能想要取消缓存功能，这样读进来的大块数据就不会导致当前 cache 中的大部分数据被置换出去，我们可以为它提供一个单独的 iterator 来达到该目的：\nleveldb::ReadOptions options; options.fill_cache = false; leveldb::Iterator* it = db-\u0026gt;NewIterator(options); for (it-\u0026gt;SeekToFirst(); it-\u0026gt;Valid(); it-\u0026gt;Next()) { ... } Key 的布局 # 注意，磁盘传输和缓存的单位都是一个 block，相邻的 keys（已排序）总在同一个 block 中，因此应用可以通过把需要一起访问的 keys 放在一起，同时把不经常使用的 keys 放到一个独立的键空间区域来提升性能。\n举个例子，假设我们正基于 leveldb 实现一个简单的文件系统。我们打算存储到这个文件系统的数据类型如下：\nfilename -\u0026gt; permission-bits, length, list of file_block_ids file_block_id -\u0026gt; data 我们可以给上面表示 filename 的 key 增加一个字符前缀，例如 \u0026lsquo;/\u0026rsquo;，然后给表示 file_block_id 的 key 增加另一个不同的前缀，例如 \u0026lsquo;0\u0026rsquo;，这样这些不同用途的 key 就具有了各自独立的键空间区域，扫描元数据的时候我们就不用读取和缓存大块文件内容数据了。\n过滤器 # 鉴于 leveldb 数据在磁盘上的组织形式，一次 Get() 调用可能涉及多次磁盘读操作，可配置的 FilterPolicy 机制可以用来大幅减少磁盘读次数。\nleveldb::Options options; // 设置启用基于布隆过滤器的过滤策略 options.filter_policy = NewBloomFilterPolicy(10); leveldb::DB* db; // 用该设置打开数据库 leveldb::DB::Open(options, \u0026#34;/tmp/testdb\u0026#34;, \u0026amp;db); ... use the database ... delete db; delete options.filter_policy; 上述代码将一个基于布隆过滤器的过滤策略与数据库进行了关联，基于布隆过滤器的过滤方式依赖于如下事实，在内存中保存每个 key 的部分位（在上面例子中是 10 位，因为我们传给 NewBloomFilterPolicy 的参数是 10），这个过滤器将会使得 Get() 调用中非必须的磁盘读操作大约减少 100 倍，每个 key 用于过滤器的位数增加将会进一步减少读磁盘次数，当然也会占用更多内存空间。我们推荐数据集无法全部放入内存同时又存在大量随机读的应用设置一个过滤器策略。\n如果你在使用自定义的比较器，应该确保你在用的过滤器策略与你的比较器兼容。举个例子，如果一个比较器在比较 key 的时候忽略结尾的空格，那么 NewBloomFilterPolicy 一定不能与此比较器共存。相反，应用应该提供一个自定义的过滤器策略，而且它也应该忽略 key 的尾部空格，示例如下：\nclass CustomFilterPolicy : public leveldb::FilterPolicy { private: FilterPolicy* builtin_policy_; public: CustomFilterPolicy() : builtin_policy_(NewBloomFilterPolicy(10)) {} ~CustomFilterPolicy() { delete builtin_policy_; } const char* Name() const { return \u0026#34;IgnoreTrailingSpacesFilter\u0026#34;; } void CreateFilter(const Slice* keys, int n, std::string* dst) const { // Use builtin bloom filter code after removing trailing spaces std::vector\u0026lt;Slice\u0026gt; trimmed(n); for (int i = 0; i \u0026lt; n; i++) { trimmed[i] = RemoveTrailingSpaces(keys[i]); } return builtin_policy_-\u0026gt;CreateFilter(\u0026amp;trimmed[i], n, dst); } }; 当然也可以自己提供非基于布隆过滤器的过滤器策略，具体见 leveldb/filter_policy.h。\n校验和（Checksums） # leveldb 将校验和与它存储在文件系统中的所有数据进行关联，对于这些校验和，有两个独立的控制：\nReadOptions::verify_checksums 可以设置为 true，以强制对所有从文件系统读取的数据进行校验。默认为 false，即，不会进行这样的校验。\nOptions::paranoid_checks 在数据库打开之前设置为 true ，以使得数据库一旦检测到数据损毁立即报错。根据数据库损坏的部位，报错可能是在打开数据库后，也可能是在后续执行某个操作的时候。该配置默认是关闭状态，即，持久化存储部分损坏数据库也能继续使用。\n如果数据库损坏了(当开启 Options::paranoid_checks 的时候可能就打不开了)，leveldb::RepairDB() 函数可以用于对尽可能多的数据进行修复。\n近似空间大小 # GetApproximateSizes 方法用于获取一个或多个键区间占据的文件系统近似大小(单位, 字节)\nleveldb::Range ranges[2]; ranges[0] = leveldb::Range(\u0026#34;a\u0026#34;, \u0026#34;c\u0026#34;); ranges[1] = leveldb::Range(\u0026#34;x\u0026#34;, \u0026#34;z\u0026#34;); uint64_t sizes[2]; db-\u0026gt;GetApproximateSizes(ranges, 2, sizes); 上述代码结果是，size[0] 保存 [a..c) 区间对应的文件系统大致字节数。size[1] 保存 [x..z) 键区间对应的文件系统大致字节数。\n环境变量 # 由 leveldb 发起的全部文件操作以及其它的操作系统调用最后都会被路由给一个 leveldb::Env 对象。用户也可以提供自己的 Env 实现以达到更好的控制。比如，如果应用程序想要针对 leveldb 的文件 IO 引入一个人工延迟以限制 leveldb 对同一个系统中其它应用的影响：\n// 定制自己的 Env class SlowEnv : public leveldb::Env { ... implementation of the Env interface ... }; SlowEnv env; leveldb::Options options; // 用定制的 Env 打开数据库 options.env = \u0026amp;env; Status s = leveldb::DB::Open(options, ...); 可移植 # 如果某个特定平台提供 leveldb/port/port.h 导出的类型/方法/函数实现，那么 leveldb 可以被移植到该平台上，更多细节见 leveldb/port/port_example.h。\n另外，新平台可能还需要一个新的默认的 leveldb::Env 实现。具体可参考 leveldb/util/env_posix.h 实现。\n","date":"2020-11-30","permalink":"/posts/202011-leveldb-index/","section":"Posts","summary":"leveldb 是一个持久化的 key/value 存储，key 和 value 都是任意的字节数组(byt","title":"Leveldb 基本介绍和使用指南"},{"content":"","date":"2020-11-30","permalink":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/","section":"Tags","summary":"","title":"分布式存储"},{"content":"数据库系统总会涉及非易失性存储，我们需要知道一个典型的计算机系统是如何进行存储管理的。时至今日，虽然 SSD 已经成为很多数据库管理员的选择，但传统 HDD 还是有着广泛的应用，文件系统和存储引擎大部分设计和发展还是基于 HDD 的行为；过去数十年来，HDD 一直是计算机系统中持久存储的主要形式。\n本文回顾硬盘的物理特性，硬盘的主要性能指标，以及操作是如何进行硬盘 I/O 性能优化的，最后参考开源系统来讨论如何根据硬盘特性进行系统设计。\n硬盘的物理特性 # 硬盘（Hard Disk Drive，HDD，有时为了与固态硬盘相区分称“机械硬盘”）是计算机最基础的非易失性存储，它在平整的磁性表面存储和检索数据，数据通过离磁性表面很近的磁头由电磁流来改变极性的方式被写入到磁盘上。数据可以通过盘片被读取，原理是磁头经过盘片的上方时盘片本身的磁场导致读取线圈中电气信号改变1。\n硬盘主要包括一至数片高速转动的盘片(platter)以及放在传动手臂上的读写磁头(read–write head)，每个盘片都有两面，都可记录信息，因此也会相对应每个盘片有 2 个磁头。物理结构如下图所示：\n我们通常更关注硬盘内部的结构：\n——图源自《数据库系统概念》\n磁道（Track）：当硬盘旋转时，磁头若保持在一个位置上，则每个磁头都会在磁盘表面划出一个圆形轨迹，这些圆形轨迹就叫做磁道； 柱面（Cylinder）：在有多个盘片构成的盘组中，由不同盘片的面，但处于同一半径圆的多个磁道组成的一个圆柱面； 扇区（Sector）：磁盘上的每个磁道被等分为若干个弧段，这些弧段便是硬盘的扇区（Sector）。硬盘的第一个扇区，叫做引导扇区； 硬盘性能的度量 # 硬盘常规的一次 I/O 需要 3 步，每一步都有相关的延迟，可以将 I/O 访问时间（access time）表示为 3 部分之和：\n寻道时间（seek time）：将读写磁头组合定位在访问块所在磁道的柱面上所需要的时间 旋转延迟（rotational latency）：等待访问块的第一个扇区旋转到磁头下的时间； 传输时间（transfer time）：完成数据传输需要的时间，取决于硬盘数据传输率； 为了更好理解寻道时间和旋转延迟，可以参考下图：\n值得一提的是，硬盘的趋势是传输速率相当快，因为硬盘制造商擅长将更多位填塞到同一表面。但驱动器的机械方面与寻道相关（传动手臂速度和旋转速度），改善相当缓慢2。因此，为了摊销 I/O 成本，必须在寻道之间传输尽可能多的数据。\n操作系统中的硬盘 # 就像进程是 CPU 的抽象、地址空间是内存的抽象一样，存储在操作系统的抽象是文件（目录也是一种文件）。\n如果算上内核中的文件系统、驱动等，Linux 的存储架构大体如下：\n一个具体的读流程3：\n系统调用 read（） 会触发相应的 VFS（Virtual Filesystem Switch）函数，传递参数有文件描述符和文件偏移量； VFS 确定请求的数据是否已经在内存缓存中；若数据不在内存中，内核需要通过块设备层从物理设备上读取数据； 通过通用块设备层（Generic Block Layer）在块设备上执行读操作，启动I/O 操作，传输请求的数据； 在通用块设备层之下是 I/O 调度（I/O Scheduler），根据内核的调度策略，将对应的 I/O 插入队列； 最后，块设备驱动（Block Device Driver）通过向磁盘控制器发送相应的命令，执行真正的数据传输； 我们从上到下来看一些关键的点。\nVFS（Virtual File Systems） # VFS 为多种不同的文件系统提供一个通用的接口，通常包含四个部分：\nSuperblock：包含关于特定文件系统的信息，例如文件系统中有多少个 Inode 和数据块、Inode 表的开始位置等等； Inode(Index node)：描述文件的元数据的结构，包括：文件类型（例如，常规文件、目录等）、大小、权限、一些时间信息、分配给它的块数，以及有关其数据块驻留在磁盘上的位置的信息； Dentry(Directory Entries)：目录。VFS 是以完整的路径名作为参数，需要遍历路径的目录读取 Inode 信息，一般放到内存中； File：进程打开的文件； 这里我们不讨论这些数据结构是如何具体实现的，我们重点关注操作系统如何对读写 I/O 进行优化的，这些优化常常启发人们后续的软件设计。\nPage Cache # Linux 2.2版本之前内核同时有 Page Cache 和 Buffer Cache 两个 cache，到了 2.4 版本后这两个 cache 被合在了一起，现在内核只有 Page Cache4\n倘若没有任何缓存的情况下：\n对于打开文件，每次都需要对目录层次结构中的每个级别至少进行两次读取（一次读取相关目录的 inode，并且至少有一次读取其数据）。 我们要创建一个新的文件，至少需的 I/O 有：一次查找空闲的 inode，一次写入 inode 的存储（将其标记为已分配），一次写入新的 inode 本身（初始化它），一次写入目录的数据，一次读写目录的 inode 以便更新它，最后一次写入真正的数据块——所有这些只是为了创建一个文件！5 Page Cache 位于 VFS 和文件系统之间6，在内存中保存常用的块，如果所需的页面已经存在，则根本不需要调用文件系统代码。第一次打开可能会产生很多 I/O 来读取目录的 inode 和数据，但是根据局部性原理，大部分时候会命中缓存。\n如果写入数据，则首先将其写入 Page Cache，然后作为脏页（dirty pages）进行管理，这些脏页会定期（也会与系统调用 sync 或 fsync 一起）传输到存储设备。这里也常被称为写缓冲（write buffering），主要有以下三个好处：\n通过延迟写入，将许多小的 I/O 成批写入到磁盘； 通过将一些写入缓存在内存中，系统可以调度后续的 I/O，从而提高性能； 一些写入可以通过拖延来完全避免。例如，如果应用程序创建文件并将其删除，则可以通过延迟写入完全避免写入磁盘。 有些系统（如数据库）不喜欢这种折中，因此，为了避免由于写入缓冲导致的意外数据丢失，它们就强制写入磁盘，通过调用 fsync()，使用绕过缓存的直接 I/O（direct I/O） 接口，或者使用原始磁盘（raw disk）接口完全避免使用文件系统。\n通用块层（Generic Block layer） # 对于 VFS 来说，块（block）是基本的数据传输单元；但对于块设备（硬盘也是块设备中的一种）来说，扇区是最小寻址单元，块设备无法对比扇区还小的单元进行寻址和操作。通用块设备层（Generic Block Layer）就是这一转换的中间层，也是内核的一个组成部分，它处理系统所有对块设备的请求。有通用块设备层后，内核可以方便地：\n为所有的块设备管理提供一个抽象视图，隐藏硬件块设备的差异性； 提供不同的 I/O 调度策略，能够优化性能，减少磁头移动次数，减少磁盘擦写次数，延长磁盘寿命； 扇区大小是设备的物理属性，一般大小是 512 字节。由于扇区是块设备的最小可寻址单元，所以块不能比扇区还小，只能整数倍于扇区大小，一般是 4K。\n但是，在更新磁盘时，驱动器制造商唯一保证的是单个 512 字节的写入是原子的（具体情况参见制造商说明书）。因此，如果发生不合时宜的掉电，则可能只完成部分写入。\n块设备驱动层（Block Device Driver） # I/O 调度后的请求会交给相应的设备驱动程序去进行读写，驱动层中的驱动程序对应具体的物理存储设备，向控制器发出具体的指令来读写数据。由于不是做驱动开发，这里不是我们关注的重点。\n常见的硬盘 I/O 优化 # 通过上面的分析，我们知道 Linux 一次读写请求到达磁盘的过程，为了降低文件系统的 I/O 成本，Linux 主要：\n通过缓存来提高读写性能，本质就是减少磁盘寻道次数； 同时根据磁盘顺序读写快、随机读写慢的特点，尽量做追加写； 这些设计思想也被许多开源软件广泛采用。\n追加写 # Google BigTable 7的论文把 LSM-Tree（Log Structured-Merge Tree）8 这个古老的数据结构带回前沿，基于 LSM-Tree 的存储引擎有：Leveldb、Rocksdb、HBase、Cassandra 等等。不同于传统的 B 树类存储引擎，基于 LSM-Tree 的存储引擎尤其适合写多读少的场景。\n当一个写请求到达时，它会被写到 memtable 中，memtable 在内存里维护一个平衡二叉树或者跳表来保持 key 有序（memtable 同时会写 WAL 来备份数据到磁盘，以便崩溃恢复），当 memtable 达到既定规模时，就会转换为 immutable memtable（不可变 memtable，顾名思义，只读的），然后后台进程会将 immutable memtable 压缩成 SSTable(Sorted String Table，即有序的) 写到磁盘。\n存储引擎只做了顺序磁盘读写，因为没有文件被编辑，增加、修改或删除操作都用简单的生成新的文件来存储。旧的文件不会被更新，重复的记录只会通过创建新的纪录来覆盖，这当然也就产生了一些冗余的数据。显然随着数据的不断修改，SSTable 的文件数量会不断的增加，\n所以，系统会定期的执行合并（compaction)操作，即把多个 SSTable 归并为一个大的 SSTable，移除重复的更新或者删除纪录，同时也会删除上述的冗余。通过这样的方式减少了文件个数的增长，保证读操作的性能。因为 SSTable 文件都是有序结构的，所以合并操作也是非常高效的。\n当然 LSM-Tree 实现还有很多具体的细节，例如：快照、SSTable 索引、如何组织合并后的 SSTable 等内容，这里我们暂且不表，后面我们会专注于分析 LSM-Tree 的具体实现（leveldb、rocksdb）。\n总之，LSM-Tree 充分利用了内存随机读写 + 顺序落盘 + 定期归并来获取最大性能。\n较大的文件 # 硬盘最适合顺序的大文件 I/O 读写，在硬盘上分散的多个小文件会损害性能；同时，元数据过多也会带来很多 I/O 开销（请求很多次 inode）影响性能，所以我们尽量：\n将小文件合并为大文件 优化元数据存储和管理 Google File System9 和 Facebook Haystack10 是两个典型的案例：\nGFS 选择了当时看来相当大的 64M 作为数据存储的基本单位，就是为了减少大量元数据； Facebook Haystack 同样将小文件集合成大文件来减少了元数据数目；同时精简元数据，去掉一切 Facebook 场景中不需要的元数据，压缩元信息到足够小并全部加载到内存中，避免请求 inode 带来的开销。 Reference # https://en.wikipedia.org/wiki/Disk_storage\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n“Hardware Technology Trends and Database Opportunities” David A.PattersonKeynote Lecture at the ACM SIGMOD Conference (SIGMOD ’98) June, 1998\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.ilinuxkernel.com/files/Linux.Generic.Block.Layer.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://books.google.de/books?id=lZpW6xmXrzoC\u0026amp;pg=PA348\u0026amp;dq=linux+buffer+cache+page+cache\u0026amp;cd=1#v=onepage\u0026amp;q=linux%20buffer%20cache%20page%20cache\u0026amp;f=false\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026ldquo;Operating Systems: Three Easy Pieces\u0026rdquo; Peter Reiher\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026ldquo;The future of the page cache\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026ldquo;Bigtable:A distributed storage system for structured data\u0026rdquo; Chang F;Dean J;Ghemawat S;Hsieh WC,Wallach DA,Burrows M,Chandra T,Fikes A,Gruber RE, 2006\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPatrick O\u0026rsquo;Neil, Edward Cheng, Dieter Gawlick, and Elizabeth O\u0026rsquo;Neil, The Log-Structured Merge-Tree. Acta Informatica 33, June 1996.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGhemawat, S., Gobioff, H., and Leung, S.-T. 2003. The Google file system In 19th Symposium on Operating Systems Principles. Lake George, NY. 29-43.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBeaver D, Kumar S, Li HC, Sobel J, Vajgel P et al (2010) Finding a needle in haystack: facebook’s photo storage. In OSDI, vol 10. pp 1–8\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2020-11-26","permalink":"/posts/202011-disk/","section":"Posts","summary":"数据库系统总会涉及非易失性存储，我们需要知道一个典型的计算机","title":"系统设计那些事儿：硬盘 I/O"},{"content":"Multi-Paxos 在文献中并没有准确的实现细节，这里提供一个相对完整的规范，保持接近 Leslie Lamport 在 “The Part-Time Parliament.” 中给出的算法。\n这里描述的 Multi-Paxos 尚未经过实践证明其正确性。\n众所周知 Raft 是更易理解的，所以我参照 Raft 的风格，将 Paxos 转换成了下图。\n1. 基础 # 提案编号 n = (round number, server ID) T： 固定的超时时间，用于选举算法 α：并发限制，用于配置变更 1.1 Leader 选举算法 # 每个节点每隔 T（ms） 向其它服务器发送心跳 如果一个节点在 2（Tms） 时间内没有收到比自己 server ID 更大的心跳，那它自己就转为 Leader 2. 持久化 # 2.1 Acceptor 上的持久化状态 # lastLogIndex：已经接受的最大的日志index minProposal：已经接收提案中的最小提案编号，如果还未收到 Prepare 请求，则为 0 每个 Acceptor 上还会存储一个日志，日志索引 i ∈ [1, lastLogIndex]，每条日志记录包含以下内容：\nacceptedProposal[i]：第 i 条日志最后接受的提案编号。初始化时为 0；如果提案被 chosen，则 acceptedProposal[i] = 无穷大 acceptedValue[i]：第 i 条日志最后接受的 value，初始化时为 null firstUnchosenIndex：i \u0026gt; 0 且 acceptedProposal[i] \u0026lt; ∞ 的最小日志 index 2.2 Proposer 上的持久化状态 # maxRound：Proposer 已知的最大 round number 2.3 Proposer 上的易失性状态 # nextIndex：客户端请求要写的下一个日志 index prepared：如果 prepared 为 True，那么 Proposer 不再需要发起 Prepare 请求（超过半数的 Acceptor 回复了 noMoreAccepted）；初始化为 False 3 流程 # 3.1 Prepare（阶段 1） # 请求： # n：提案编号 index：Proposer 的提案对应的日志 index 接受者处理： # 收到 Prepare 请求后，如果 request.n \u0026gt;= minProposal，则 Acceptor 设置 minProposal = request. proposalId；同时承诺拒绝所有提案编号 \u0026lt; request.n 的 Accept 请求。\n响应： # acceptedProposal：Acceptor 的 acceptedProposal[index] acceptedValue：Acceptor 的 acceptedValue[index] noMoreAccepted：Acceptor 遍历 \u0026gt;= index 的日志记录，如果之后没有接受过任何值（都是空的记录），那么 noMoreAccepted = True；否则设为 False 3.2 Accept（阶段 2） # 请求： # n：和 Prepare 阶段一样的提案编号 index：日志 index v：提案的值，如果 Prepare 阶段收到一个更大的提案编号，那么就是该最大的提案的值，否则 Proposer 使用来自 Client 的值 firstUnchosenIndex：节点日志上第一个没有被 chosen 的日志 index 接受者处理： # 收到 Accept 请求后，如果 n \u0026gt;= minProposal, 则：\nacceptedProposal[index] = n acceptedValue[index] = v minProposal = n 对于每个 index \u0026lt; request.firstUnchosenIndex，如果 acceptedProposal[index] = n，则 acceptedProposal[index] = ∞ 响应： # n：Acceptor 的 minProposal 值 firstUnchosenIndex：Acceptor 的 firstUnchosenIndex 值 3.3 Success（阶段 3） # 请求： # index：日志的索引 v：log[index] 已 chosen 的提案值 接受者处理： # Acceptor 收到 Success RPC 后，更新已经被 chosen 的日志记录：\nacceptedValue[index] = v acceptedProposal[index] = 无穷大 响应： # firstUnchosenIndex：Acceptor 的 firstUnchosenIndex 值 当发送者收到响应后，如果 reply.firstUnchosenIndex \u0026lt; firstUnchosenIndex，则发送者再发生请求： Success(index = reply.firstUnchosenIndex, value = acceptedValue[reply.firstUnchosenIndex])\n3.4 Proposer 算法：write(inputValue) → bool # 如果不是 Leader，或者 Leader 还没有初始化完成，直接返回 False 如果 prepared == True： index = nextIndex, nextIndex++ goto 7 index = firstUnchosenIndex，nextIndex = index + 1 生成一个新的提案编号 n（maxRound++，并持久化保存） 广播 Prepare(n, index) 给所有 Acceptor 一旦收到超过半数 Acceptor 的 Prepare 响应（reply.acceptedProposal,reply.acceptedValue,reply.noMoreAccepted）： 如果所有响应中最大的 reply.acceptedProposal 不等于 0，那么使用它的 reply.acceptedValue，否则使用自己的 inputValue 如果超过半数的 Acceptor 回复了 reply.noMoreAccepted = True，那么 prepared = true 广播 Accept(index, n, v) 到所有的 Acceptor 一旦收到一个 Acceptor 的响应（reply.n, reply.firstUnchosenIndex） 如果 reply.n \u0026gt; n，则从 reply.n 中修改 maxRound，修改 prepared = False，跳转到 1 如果 reply.firstUnchosenIndex ≤ lastLogIndex 并且 acceptedProposal[reply.firstUnchosenIndex] == ∞，就发送 Success(index = reply.firstUnchosenIndex, value = acceptedV alue[reply.firstUnchosenIndex]) 一旦收到超过半数 Acceptor 的 Accept 响应：修改 acceptedP roposal[index] = ∞ 和 acceptedValue[index] = v 如果 v == inputValue， 返回 True 跳转到 2 4. 配置变更（成员变更） # 配置通常一个列表，每一项存着一台服务器的 id 和 ip 地址，作为一条特殊的记录存储在日志中 𝛼 表示配置多少条记录后才能生效。第 i 条记录 chosen 时的配置存储在第 i-𝛼 条或 i-𝛼 条之前 α 用作并发限制：在 i 这个位置的值被 chosen 之前，我们不能 chosen i+α 这个位置的值 ","date":"2020-11-15","permalink":"/posts/202011-understanding-multi-paxos/","section":"Posts","summary":"Multi-Paxos 在文献中并没有准确的实现细节，这里提供一个相对完整的规范，","title":"用 Raft 的方式理解 Multi-Paxos"},{"content":"试题 # 1. # (4 分）下面的每张图都显示了一种 Multi-Paxos 服务器上可能的日志（每个条目中的数字代表 acceptedProposal 值）。考虑每份日志都是独立的，下列日志是否可能发生在正确实现的 Multi-Paxos 中？\na. b. c. d. 2. # (6 分) 对于 Basic Paxos，假设一个集群有 5 台服务器，其中 3 台接受了(accepted)提案编号 5.1 和对应的提案值 X，在这种情况下，集群中的任意服务器是否有可能接受不同的值 Y ？解释你的答案。\n3. # (10 分) 假设 Multi-Paxos 集群选出了一个节点作为 Leader，而且没有其它 Leader。此外，假设该节点继续担任一段时间的 Leader，为日志 chosen 了很多命令，并且在这期间依然没有其它节点试图担任 Leader。\na. 在此期间，该节点最少要发送多少轮 Prepare RPC？给出解释，且尽可能精确。\nb. 在此期间，该节点最多要发送多少次 Prepare RPC？给出解释，且尽可能精确。\n4. # (5 分) 当一个 Acceptor 使用 Proposer 提供的 firstUnchosenIndex 来标记被 chosen 的日志记录时，它必须先检查日志记录中的提案编号（acceptedProposal[i] == request.proposal）。假设它跳过了这一检查：请描述一个系统异常的情况。\n5. # (5 分) 假设提案编号的两个部分（自增 id 和唯一 server_id）进行了互换，即 server_id 位于高位。 a. 这会影响 Paxos 的安全性（Safety）吗？请简单解释你的答案。 b. 这会影响 Paxos 的活性（Liveness）吗？请简单解释你的答案。\n6. # (10 分) 假设一个 Proposer 以初始值 v1 运行 Basic Paxos，但是它在协议执行过程中或执行后的某个（未知）时间点宕机了。假设该 Proposer 重新启动并从头开始运行协议，使用之前使用的相同的提案编号，但初始值为 v2，这样安全吗？请解释你的答案。\n7. # (10 分) 在一个成功的 Accept RPC 中，Acceptor 将其 minProposal 设为 n（Accept RPC 中的提案编号）。描述一个这样做实际上改变了 minProposal 值的场景（即 minProposal 还没有等于 n）。描述如果没有这段代码，系统将出现异常行为的场景。\n8. # (10 分) 考虑 Multi-Paxos 的配置变更，旧配置由服务器 1、2 和 3 组成，新配置由服务器 3、4 和 5 组成。假设新配置在日志中第 N 条被 chosen，同时日志记录 N 到 N+α (含)也都被 chosen。假设此时旧服务器 1 和 2 被关闭，因为它们不属于新配置。描述下这可能在系统中引起的问题。\n答案 # 1. # a. 是。\nb. 是。\nc. 是。\nd. 是。\n2. # 是。如果 S1，S2 和 S3 接受了提案 \u0026lt;5.1, X\u0026gt;，其它服务器仍然可能接受更早的提案编号的提案值 Y。\n例如，S4 先发送 Prepare(3.4) 发现并没有已接受的提案值，接着 S1 发送 Prepare（5.1）到 S1，S2，S3，然后 S1，S2，S3 接受了\u0026lt;5.1, X\u0026gt;，此时 S4 仍然可能在 S4，S5 完成提案 \u0026lt;3.4, Y\u0026gt;\n3. # a. 最少只发送 1 轮 Prepare RPC，如果多数派 Prepare 都立即返回了具有 noMoreAccepted=true 的响应。\nb. 最多是： Leader 节点上每有一个未 chosen 但是 Acceptor 已经接受的日志记录，就会有一轮 Prepare RPC。这发生在如果每次 Leader 为其没有 chosen 的日志发送 Prepare 请求，都发现有一个 Acceptor 已经接受了该提案值，那它就会在该条目位置采用这个提案值，然后继续尝试下一个日志条目。这样就会发生最多的轮次。\n4. # 可能出现的异常行为是：服务器会标记两个不同的 chosen 值。用 2 个竞争的提案，3 节点集群和 2 个日志来举例：\nS1 完成一轮 Prepare 发送提案编号 n=1.1, index=1 给 S1，S2 S1 只完成 S1（它自己）的 Accept 提案 n=1.1, value = X, index = 1 S2 完成一轮 Prepare 发送提案编号 n=2.2, index=1 给 S2，S3，收到二者包含 noMoreAccepted=true 的响应 S2 完成一轮 Accept，S2、S3 收到 n=2.2, value=Y, index=1 S2 标记 index 1 的日志为 chosen S2 完成一轮 Accept，S1, S2, 和 S3 收到 n=2.2, value=Z, index=2, firstUnchosenIndex=2，此时，S1 将会发生异常：将 n=1.1, value=X 的日志设为 chosen，然后将 X 应用到状态机。这是不正确的，因为实际上是 Y 被 chosen。 5. # a. 不会。因为安全性只需要提案编号唯一，每台服务器的 server_id 是唯一的，并且有自增 id，所以唯一性得到保证。\nb. 会。例如，server_id 最大的服务器向集群中每一台服务器发出的 Prepare RPC 将会永远失败。然后，其它 Proposer 无法继续运行，因为其它服务器的 minProposal 对于 Proposer 来说太大了。\n6. # 不安全。不同的提案必须具有不同的提案编号。下面是一个 3 节点集群的例子：\nS1 发送 Prepare(n=1.1) 至 S1，S2 S1 发送 Accept(n=1.1, v=v1) 至 S1 S1 重启 S1 发送 Prepare(n=1.1) 至 S2，S3（并且发现还没有被接受的提案） S1 发送 Accept(n=1.1, v=v2) 与 S2，S3 S1 将 v2 被 chosen 返回给客户端 S2 发送 Prepare(n=2.2) 至 S1，S2 并收到响应： 来自 S1: acceptedProposal=1.1, acceptedValue=v1 来自 S2: acceptedProposal=1.1, acceptedValue=v2 S2 直接选择了 v1 作为提案值 S2 发送 Accept(n=2.2, v=v1)至S1，S2，S3 S2 将 v1 被 chosen 返回给客户端 可能出现的另一个问题是，崩溃前的请求在崩溃之后才被送到：\nS1 发送 Prepare(n=1.1) 至 S1，S2 S1 发送 Accept(n=1.1, v=v1) 至 S1 S1 发送 Accept(n=1.1) 至 S2 和 S3，但是它们并没有收到 S1 重启 S1 发送 Prepare(n=1.1) 至 S2，S3（并且发现还没有被接受的提案） S1 发送 Accept(n=1.1, v=v2) 至 S2 和 S3 S1 将 v2 被 chosen 返回给客户端 现在，S2 和 S3 收到了（之前的） Accept(n=1.1, v=v1) 请求，并且覆盖了 acceptedValue 设为 v1。现在集群的状态是 v1 被 chosen，但是客户端收到 v2 被 chosen。 7. # 用 5 个节点的 Basic Paxos 举例：\nS1 发送 Prepare(n=1.1) 至 S1, S2, S3（并且发现没有接受的提案） S5 发送 Prepare(n=2.5) 至 S3, S4, S5（并且发现没有接受的提案） S5 发送 Accept(n=2.5, v=X) 至 S2, S3, S5，这时 S2 的 minProposal 应该是 2.5 S5 返回 X 被 chosen 给客户端 S1 发送 Accept(n=1.1, v=Y) 至 S2，这通常会被拒绝，但是如果 Accept 阶段未更新 S2 的 minProposal，这会被接受 S3 发送 Prepare(n=3.3) 至 S1, S2, S4（并且发现 n=1.1, v=Y） S3 发送 Accept(n=3.3, v=Y)至 S1, S2, S3, S4, S5 S3 返回 Y 被 chosen 给客户端 8. # 这将导致新集群的活性(liveness)问题，因为新集群服务器上的 firstUnchosenIndex 可能小于 N+α。\n例如，在最坏情况下，S3 可能永久故障了，而 S1 和 S2 则可能没有尝试将任何值同步到 S4 和 S5（仅使用本讲义中介绍的算法）。然后，S4 和 S5 将永远无法学习到日志记录第 1 到 N+α-1 所 chosen 的值，因为它们无法和 S1、S2 或 S3 进行通信。S4 和 S5 的状态机将永远无法超越其初始状态。\n","date":"2020-10-26","permalink":"/posts/202010-paxos-exam/","section":"Posts","summary":"试题 # 1. # (4 分）下面的每张图都显示了一种 Multi-Paxos 服务器上可能的日志","title":"Raft 作者出的 Paxos 的试题"},{"content":"试题 # 1. # （4 分）下面的每张图都显示了一台 Raft 服务器上可能存储的日志（日志内容未显示，只显示日志的 index 和任期号）。考虑每份日志都是独立的，下面的日志可能发生在 Raft 中吗？如果不能，请解释原因。\na. b. c. d. 2. # （6 分）下图显示了一个 5 台服务器集群中的日志（日志内容未显示）。哪些日志记录可以安全地应用到状态机？请解释你的答案。\n3. # （10 分）考虑下图，它显示了一个 6 台服务器集群中的日志，此时刚刚选出任期 7 的新 Leader（日志内容未显示，只显示日志的 index 和任期号）。对于图中每一个 Follower，给定的日志是否可能在一个正常运行的 Raft 系统中存在？如果是，请描述该情况如何发生的；如果不是，解释为什么。\n4. # （5 分）假设硬件或软件错误破坏了 Leader 为某个特定 Follower 存储的 nextIndex 值。这是否会影响系统的安全？请简要解释你的答案。\n5. # （5 分）假设你实现了 Raft，并将它部署在同一个数据中心的所有服务器上。现在假设你要将系统部署到分布在世界各地的不同数据中心的每台服务器，与单数据中心版本相比，多数据中心的 Raft 需要做哪些更改？为什么？\n6. # （10 分）每个 Follower 都在其磁盘上存储了 3 个信息：当前任期（currentTerm）、最近的投票（votedFor）、以及所有接受的日志记录（log[]）。 a. 假设 Follower 崩溃了，并且当它重启时，它最近的投票信息已丢失。该 Follower 重新加入集群是否安全（假设未对算法做任何修改）？解释一下你的答案。 b. 现在，假设崩溃期间 Follower 的日志被截断（truncated）了，日志丢失了最后的一些记录。该 Follower 重新加入集群是否安全（假设未对算法做任何修改）？解释一下你的答案。\n7. # （10 分）如视频中所述，即使其它服务器认为 Leader 崩溃并选出了新的 Leader 后，（老的）Leader 依然可能继续运行。新的 Leader 将与集群中的多数派联系并更新它们的任期，因此，老的 Leader 将在与多数派中的任何一台服务器通信后立即下台。然而，与此期间，它也可以继续充当 Leader，并向尚未被新 Leader 联系到的 Follower 发出请求；此外，客户端可以继续向老的 Leader 发送请求。我们知道，在选举结束后，老的 Leader 不能提交（commit）任何新的日志记录，因为这样做需要联系选举多数派中的至少一台服务器。但是，老的 Leader 是否有可能执行一个成功 AppendEntries RPC，从而完成在选举开始前收到的旧日志记录的提交？如果可以，请解释这种情况是如何发生的，并讨论这是否会给 Raft 协议带来问题。如果不能发生这种情况，请说明原因。\n8. # （10 分）在配置变更过程中，如果当前 Leader 不在 C-new 中，一旦 C-new 的日志记录被提及，它就会下台。然而，这意味着有一段时间，Leader 不属于它所领导的集群（Leader 上存储的当前配置条目是 C-new，它不包括 Leader）。假设修改算法，如果 C-new 不包含 Leader，则使 Leader 在其日志存储了 C-new 时就立即下台。这种方法可能发生的最坏情况是什么？\n答案 # 1. # a. 不能：任期在日志里必须单调递增。\n具体来说，写入日志 \u0026lt;4, 2\u0026gt; 的 Leader1 只能从当前任期 \u0026gt;= 3 的 Leader2 那里接收到日志 \u0026lt;3, 3\u0026gt; ，所以 Leader1 当前任期也将 \u0026gt;= 3，那么它就不能写入 \u0026lt;4, 2\u0026gt;\nb. 可以\nc. 可以\nd. 不能：日志不允许空洞。 具体来说，Leader 只能追加日志，AppendEntries 中的一致性检查永远不会允许空洞。\n2. # 日志记录 \u0026lt;1,1\u0026gt; 和 \u0026lt;2,1\u0026gt; 可以安全应用（到状态机）：\n如果一条日志记录没有存储在多数派上，它就不能被安全地应用。这是因为少数服务器可能故障，并且其它服务器（构成多数派）可以在不知道该日志记录的情况下继续运行。\n因此，我们只需要考虑记录 \u0026lt;1,1\u0026gt;, \u0026lt;2,1\u0026gt;, \u0026lt;3,2\u0026gt;, \u0026lt;4,2\u0026gt;, \u0026lt;5,2\u0026gt;。\n我们必须弄清楚哪些节点可以当选 Leader，然后看看它们是否会导致这些日志记录被删除。(Leader 处理不一致是通过强 Follower 直接复制自己的日志来解决的)\nS2 可以被选为 Leader，因为它的日志至少和 S3、S4 和 S5 一样完整。那么它可能导致 \u0026lt;3,2\u0026gt;, \u0026lt;4,2\u0026gt; 和 \u0026lt;5,2\u0026gt; 被删除，所以这些日志记录不能被安全地应用。\n所以我们只剩下 \u0026lt;1,1\u0026gt; 和 \u0026lt;2,1\u0026gt; 可能安全地应用（到状态机）。\nS3 和 S4 不能被选为 Leader，因为它们的日志不够完整。S5 能被选举为 LEader，但是它包含了 \u0026lt;1,1\u0026gt; 和 \u0026lt;2,1\u0026gt; 。\n因此，只有记录 \u0026lt;1,1\u0026gt; 和 \u0026lt;2,1\u0026gt; 可以被安全地应用（到状态机）\n3. # (a) 不能。如果在不同的日志中的两条记录拥有相同的索引和任期号，那么他们之前的所有日志条目也全部相同。 \u0026lt;5, 3\u0026gt; 在 Leader 和 a 中都存在，但是前面的日志却不相同。\n(b) 不能。同上，\u0026lt;6, 5\u0026gt; 在 Leader 和 b 中都存在，但是前面的日志却不相同。\n(c) 可能。c 可能是\u0010\u0010\u0010\u0010\u0010第 6 任期的 Leader，其起始日志为 \u0026lt;1,1\u0026gt;, \u0026lt;2,1\u0026gt; 并且可能在其日志中写了一堆记录，而没有与我们第 7 任期的当前 Leader 进行通信。这也假设当前 Leader 的 \u0026lt;3,3\u0026gt;、\u0026lt;4,3\u0026gt;、\u0026lt;5,3\u0026gt;、\u0026lt;6,5\u0026gt; 这几个日志记录在第 5 任期没有被写入，这是可能的。\n(d) 不能。在一个日志中，任期只能是单调递增的。\n(e) 可能。例如，e 是任期 1 的 Leader，提交了日志 \u0026lt;1,1\u0026gt; 和 \u0026lt;2,1\u0026gt;，然后与其它服务器失联（网络分区），但还在继续处理客户端请求。\n4. # 不会。\n如果 nextIndex 的值太小，Leader 将发送额外的 AppendEntries 请求。每个请求都不会对 Follower 的日志产生任何影响（它们将进行一致性检查，但不会和 Follower 日志中的记录产生冲突，也不会向 Follower 提供该 Follower 没有存储的任何日志记录），成功的响应将告诉 Leader 应该增加其 nextIndex。\n如果 nextIndex 的值太大，Leader 也将发送额外的 AppendEntries 请求，对此，一致性检查将会失败，从而导致 Follower 拒绝该请求，Leader 将会递减 nextIndex 值并重试。\n无论哪种方式，这都是安全的行为，因为两种情况下都不会修改关键的状态。\n5. # 我们需要将选举超时(election timeouts)时间设置得更长：预期的广播时间更长，选举超时时间应该比广播时间长得多，以便候选人有机会在再次超时之前完成一次选举。该算法其余部分不需要任何修改，因为它不依赖于时序。\n6. # a. 不安全。这将允许一个服务器在同一任期内投票两次，这样以来，每个任期就可以有多个 Leader 参与，这几乎破坏了一切。\n例如，对于 3 台服务器：\nS1 获得 S1 和 S2 的投票，并且成为任期 2 的 Leader S2 重启，丢失了它在任期 2 中投过的票(votedFor) S3 获得 S2 和 S3 的选票，并且成为任期 2 的第二任 Leader 现在 S1 和 S3 都可以在任期 2 同一 index 的日志记录上提交不同的值。 b. 不安全。这将允许已提交的日志不被存储在多数派上，然后将允许同一 index 提交其它不同的值。 例如，对于 3 台服务器：\nS1 成为任期 2 的 Leader，并在自己和 S2 上追加写了 index=1, term=2, value=X，并设置 committedIndex=1，然后返回已提交的值 X 给客户端 S2 重启，并且丢失了其日志中的记录 S3（具有空的日志）成为任期 3 的 Leader，因为它的空日志也至少与 S2 一样完整。S3 在自己和 S2 上追加写 index=1, term=3, value=Y，并设置committedIndex=1，然后返回已提交的值 Y 给客户端 7. # 可能\u0010\u0010\u0010。仅当新 Leader 也包含正在提交的日志时，才会发生这种情况，所以不会引起问题。\n下面是一个在 5 台服务器发生这种情况的例子：\n带有空日志的 S1 成为任期 2 的 Leader，票选来自 S1，S2 和 S3 S1 将 index=1, term=2, value=X 追加写到它自己和 S2 S2 的日志中包含 index=1, term=2, value=X，S2 成为任期 3 的 Leader，票选来自 S2，S4 和 S5 S1 将 index=1, term=2, value=X 追加写到 S3 此时，S1 已经完成了对 index=1, term=2, value=X 的提交，即使它不再是当前任期的 Leader 这种行为是安全的，因为任何新的 Leader 也必须包含该日志记录，因此它将永远存在。\n该日志记录必须存储在给新 Leader（记为 L）投票的服务器 S 上，并且必须在 S 给新 Leader 投票之前存储在 S 上，日志完整性会检测，S 只能在以下情况投票给 L： L.lastLogTerm \u0026gt; S.lastLogTerm 或者 (L.lastLogTerm == S.lastLogTerm and L.lastLogIndex \u0026gt;= S.lastLogIndex)\n如果 L 是 S 之后的第一任 Leader，那么我们必须处于第二个条件下，那么 L 一定包含了 S 拥有的所有日志记录，包括我们担心\u0010的那个记录。\n如果 L\u0026rsquo; 是 S 之后的第二任 Leader，那么 L\u0026rsquo; 只有从 L 那里接收到了日志，它最新的任期号才可能比 S 大。但是 L 在把自己的日志复制到 L\u0026rsquo; 时也一定已经把我们担心的那条日志复制到 L\u0026rsquo; 了，所以这也是安全的。\n而且，这个论点对未来所有的 Leader 都成立。\n8. # 根据对算法的理解，有两种可能的正确答案。\n答案 1: 假设一个不错的实现——一旦一个服务器不再属于其当前配置，它就不会再成为 Candidate。问题在于，C-old 中的另一台服务器可能会被选为 Leader，在其日志中追加 C-new，然后立即下台。\n更糟糕的是，这种情况可能会在 C-old 的多数派服务器上重复。一旦超过半数 C-old 存储了 C-new 条目，它就不能再重复了\u0010\u0010\u0010\u0010。\u0010由于日志完整性检查，没有 C-new 这条日志记录的 C-old 中的任何服务器都不能当选（超过半数的 C-old 需要日志 C-old+new，不会再给没有 C-new 这条日志记录的服务器投票。）\n在这之后，C-new 中的某台服务器必须当选，集群就会继续运行。所以最坏的情况其实只是跑了最多大约 |C-old|/2 次额外的选举和选举超时。\n答案 2: 假设一个朴素的（naive）实现，仍允许一个不属于其当前配置的服务器成为 Candidate，在这种情况下，最坏的情况是， Leader 一下台就再次当选（它的日志仍然是完整的），然后再下台，然后无限重复。\n","date":"2020-10-26","permalink":"/posts/202010-raft-exam/","section":"Posts","summary":"试题 # 1. # （4 分）下面的每张图都显示了一台 Raft 服务器上可能存储","title":"Raft 作者亲自出的 Raft 试题，你能做对几道？"},{"content":"分布式系统为了实现多副本状态机（Replicated state machine），常常需要一个多副本日志（Replicated log）系统，这个原理受到简单的经验常识启发：如果日志的内容和顺序都相同，多个进程从同一状态开始，并且以相同的顺序获得相同的输入，那么这些进程将会生成相同的输出，并且结束在相同的状态。\nReplicated log =\u0026gt; Replicated state machine\n问题是：\n如何保证日志数据在每台机器上都一样？\n当然是一直在讨论的 Paxos。一次独立的 Paxos 代表日志中的一条记录，重复运行 Paxos 即可创建一个 Replicated log。\n但是如果每一组提案值都通过一次 Paxos 算法实例来达成共识，每次都要两轮 RPC，会产生大量开销。所以需要对 Paxos 做一些调整解决更实际的问题，并提升性能。经过一系列优化后的 Paxos 我们称之为 Multi-Paxos。\nMulti-Paxos 的目标就是实现 Replicated log.\n下面我们从第一个问题开始。\n如何确定是哪条日志记录？ # 首先，Replicated log 类似一个数组，我们需要知道当次请求是在写日志的第几位。因此，Multi-Paxos 做的第一个调整就是要添加一个日志的 index 参数到 Prepare 和 Accept 阶段，表示这轮 Paxos 正在决策哪一条日志记录。\n现在流程大致如下，当收到客户端带有提案值的请求时：\n找到第一个没有 chosen 的日志记录 运行 Basic Paxos，对这个 index 用客户端请求的提案值进行提案 Prepare 是否返回 acceptedValue？ 是：用 acceptedValue 跑完这轮 Paxos，然后回到步骤 1 继续处理 否：chosen 客户端提案值 举个例子 # 如图所示，首先，服务器上的每条日志记录可能存在三种状态：\n已经保存并知道被 chosen 的日志记录，例如 S1 方框加粗的第 1、2、6 条记录（后面会介绍服务器如何知道这些记录已经被 chosen） 已经保存但不知道有没有被 chosen，例如 S1 第 3 条 cmp 命令。观察三台服务器上的日志，cmp 其实已经存在两台上达成了多数派，只是 S1 还不知道 空的记录，例如 S1 第 4、5 条记录，S1 在这个位置没有接受过值，但可能在其它服务器接受过：例如 S2 第 4 条接受了 sub，S3 第 5 条接受了 cmp 我们知道三台机可以容忍一台故障，为了更具体的分析，我们假设此时是 S3 宕机的情况。同时，这里的提案值是一条具体的命令。当 S1 收到客户端的请求命令 jmp 时，：\n找到第一个没有 chosen 的日志记录：图示中是第 3 条 cmp 命令。 这时候 S1 会尝试让 jmp 作为第 3 条的 chosen 值，运行 Paxos。 因为 S1 的 Acceptor 已经接受了 cmp，所以在 Prepare 阶段会返回 cmp，接着用 cmp 作为提案值跑完这轮 Paxos，s2 也将接受 cmp 同时 S1 的 cmp 变为 chosen 状态，然后继续找下一个没有 chosen 的位置——也就是第 4 位。 S2 的第 4 个位置接受了 sub，所以在 Prepare 阶段会返回 sub，S1 的第 4 位会 chosen sub，接着往下找。 第 5 位 S1 和 S2 都为空，不会返回 acceptedValue，所以第 5 个位置就确定为 jmp 命令的位置，运行 Paxos，并返回请求。 值得注意的是，这个系统是可以并行处理多个客户端请求，比如 S1 知道 3、4、5、7 这几个位置都是未 chosen 的，就直接把收到的 4 个命令并行尝试写到这四个位置。但是，如果是状态机要执行日志时，必须是按照日志顺序逐一输入，如果第 3 条没有被 chosen，即便第 4 条已经 chosen 了，状态机也不能执行第 4 条命令。\n还记得我们之前文章里说的活锁。如果所有的 Proposer 都一起并行工作，因 Proposer 间大量的冲突而需要更多轮 RPC 才能达成共识的可能性就很大。另外，每个提案最优情况下还是需要两轮 RPC ！\n一般通过以下方式优化：\n选择一个Leader，任意时刻只有它一个 Proposer，这样可以避免冲突 减少大部分 Prepare 请求，只需要对整个日志进行一次 Prepare，后面大部分日志可以通过一次 Accept 被 chosen 下面谈谈这两个优化。\nLeader 选举 # 有很多办法可以进行选举，Lamport 提出了一种简单的方式：让 server_id 最大的节点成为Leader（在上篇说到提案编号由自增 id 和 server_id 组成，就是这个 server_id）。\n既然每台服务器都有一个 server_id，我们就直接让 server_id 最大的服务器成为 Leader，这意味着每台服务器需要知道其它服务器的 server_id 为此，每个节点每隔 Tms 向其它服务器发送心跳 如果一个节点在 2Tms 时间内没有收到比自己 server_id 更大的心跳，那它自己就转为 Leader，意味着： 该节点处理客户端请求 该节点同时担任 Proposer 和 Acceptor 如果一个节点收到比自己 server_id 更大的服务器的心跳，那么它就不能成为 Leader，意味着： 该节点拒绝掉客户端请求，或者将请求重定向到 Leader 该节点只能担任 Acceptor 值得注意的是，这是非常简单的策略，这种方式系统中同时有两个 Leader 的概率是较小的。即使是系统中有两个 Leader，Paxos 也是能正常工作的，只是冲突的概率就大了很多，效率也会降低。\n有一些基于租约的策略显得更为稳定，也更复杂，在此不表。\n减少 Prepare 请求 # 在讨论如何减少 Prepare 请求之前，先讨论下 Prepare 阶段的作用，需要 Prepare 有两个原因：\n屏蔽老的提案：但 Basic-Paxos 只作用在日志的一条记录 检查可能已经被 chosen 的 value 来代替原本的提案值：多个 Proposer 并发进行提案的时候，新的 Proposal 要确保提案的值相同 我们依然是需要 Prepare 的。我们要做的是减少大部分 Prepare 请求，首先要搞定这两个功能。\n对于 1，我们不再让提案编号只屏蔽一个 index 位置，而是让它变成全局的，即屏蔽整个日志。一旦 Prepare 成功，整个日志都会阻塞（值得注意的是，Accept 阶段还是只能写在对应的 index 位置上）。\n对于2，需要拓展 Prepare 请求的返回信息，和之前一样，Prepare 还是会返回最大提案编号的 acceptedValue，除此之外，Acceptor 还会向后查看日志记录，如果要写的这个位置之后都是空的记录，没有接受过任何值，那么 Acceptor 就额外返回一个标志位 noMoreAccepted。\n后续，如果 Leader 接收到超过半数的 Acceptor 回复了 noMoreAccepted，那 Leader 就不需要发送 Prepare 请求了，直接发送 Accept 请求即可。这样只需要一轮 RPC。\n副本的完整性 # 目前为止，通过选主和减少 Prepare 请求之后的 Multi-Paxos 依然不够完整，还需要解决：\n之前的日志只需要被多数派接受，完整的日志记录需要复制到全部节点 只有 Proposer（也就是Leader） 知道哪些记录被 chosen 了，需要所有的服务器都知道哪些记录被 chosen 换句话说，我们需要每台机的日志都完整，这样状态机执行日志后才能达到一样的状态。\n要做到这点，我们需要：\n为了让日志尽可能被复制到每台服务器：Leader 在收到多数派 Acceptor 回复后，可以继续做后面的处理，但同时在后台继续对未回复的 Acceptor 进行重试。这样不会影响客户端的响应时间，但这也不能确保完全复制了（例如，如果 Leader 在中途宕机了） 为了追踪哪些记录是被 chosen 的，我们增加一些内容： acceptedProposal 代表日志的提案编号，如果第 i 条记录被 chosen，则 acceptedProposal[i] = 无穷大（这是因为，只有提案编号更大的提案才能被接受，无穷大则表示无法再被重写了） 每个节点都维护一个 firstUnChosenIndex，表示第一个没有被 chosen 的日志位置。（即第一个 acceptedProposal[i] != 无穷大的节点） Leader 告诉 Acceptor 哪些日志被 chosen ：Leader 在向 Acceptor 发送 Accept 请求的时候带上 firstUnChosenIndex，这样 Acceptor 收到 Accept 请求的时候，如果第 i 条日志满足 i \u0026lt; request.firstUnchosenIndex \u0026amp;\u0026amp; acceptedProposal[i] == request.proposal，则标记 i 为 chosen（即设为无穷大） 用图示来说明一下，上图表示同一个 Acceptor 节点 Accept 请求前后的 ``。该 Acceptor 在 Accept 请求之前的第 6 位的提案编号为 3.4，这时它收到一个提案编号也为 3.4 的 Accept 请求，并且请求的 firstUnchosenIndex = 7，大于之前 3.4 所在的 6，所以将选中第 6 位，同时因为该请求的 index = 8，acceptedProposal[8] == 3.4\n到了这里还需要考虑，Acceptor 的日志条目中仍然可能有一些前任 Leader 留下的提案记录，还没有完成提案的复制或者 chosen 时 Leader 宕机，换了一个 Leader 节点，这时候需要： Acceptor 将其 firstUnchosenIndex 作为 Accept 请求的响应返回给 Proposer Proposer 判断如果 Acceptor.firstUnChosenIndex \u0026lt; Proposer.firstUnChosenIndex，则在后台（异步）发送 Success(index, v) RPC Acceptor 收到 Success RPC 后，更新已经被 chosen 的日志记录： acceptedValue[index] = v acceptedProposal[index] = 无穷大 return firstUnchosenIndex 如果需要(可能存在多个不确定的状态)，Proposer 发送额外的 Success RPC 总结一下，通过 4 个步骤就可以确保所有的 Acceptor 都最终知道 chosen 的日志记录。在一般的情况，并不需要额外的第 4 步，只有在 Leader 切换时才可能需要第 4 步。\n现在我们的日志已被完全复制了。因此，让我们转头看看与客户端的交互。\n客户端请求 # 接下来需要考虑客户端如何与系统交互。\n首先，当客户端第一次请求时，并不知道谁是 Leader，它任意请求一台服务器，如果该服务器不是 Leader，重定向给 Leader。\nLeader 直到日志记录被 chosen 并且被 Leader 的状态机执行才返回响应给客户端。\n客户端会一直和 Leader 交互，直到无法再联系上它（例如请求超时）。在这种情况下，客户端会联系任何其它服务器，这些服务器又在重定向到实际的 Leader。\n但这存在一个问题，如果请求提案被 chosen 后，Leader 在回复客户端之前宕机了。客户端会认为请求失败了，并重试请求。这相当于一个命令会被状态机执行两次，这是不可接受的。\n解决办法是客户端为每个请求增加一个唯一 id，服务器将该 id 与命令一起保存到日志记录中。状态机在执行命令之前，会根据 id 检查该命令是否被执行过。\n集群管理（加入或减少节点） # 最后一个非常棘手的问题，因为集群中的节点是会变更的，包括：服务器的 id、网络地址变更和节点数量等。集群节点数量改变会影响多数派数量的判断，我们必须保证不会出现两个重叠的多数派。\nLamport 在 Paxos 论文中的建议解决方案是：使用日志来管理这些变更。当前的集群配置被当作一条日志记录存储起来，并与其它的日志记录一起被复制同步。\n这里看起来会比较奇怪，如图所示，第 1、3 个位置存储了两个不同的系统配置，其它位置的日志存储了状态机要执行的命令。增加一个系统参数 𝛼 去控制当配置变更时什么时候去应用它，𝛼 表示配置多少条记录后才能生效。\n这里假设 𝛼 = 3，意味着 C1 在 3 条记录内不生效，也就是 C1 在第 4 条才会生效， C2 在第 6 条开始生效。\n𝛼 是在系统启动的时候就指定的参数。这个参数的大小会限制我们在系统可以同时 chosen 的日志条数：在 i 这个位置的值被 chosen 之前，我们不能 chosen i+α 这个位置的值——因为我们不知道中间是否有配置变更。\n所以，如果 α 值很小，假设是 1，那整个系统就是串行工作了；如果 α = 3，意味着我们可以同时 chosen 3 个位置的值；如果 α 非常大， α = 1000，那事情就会变得复杂，如果我们要变更配置，可能要等配置所在的 1000 条记录都被 chosen 以后才会生效，那要等好一阵子。这时候为了让配置更快生效，我们可以写入一些 no-op 指令来填充日志，使得迅速达到需要的条数，而不用一直等待客户端请求进来。\n总结 # 首先，描述了如何从 Basic-Paxos 到 Multi-Paxos，如何 chosen 某个位置的日志记录，接着是两个提高 Paxos 效率的办法：选定 Leader 和减少 Prepare 请求。还讲到了如何让所有的服务器都得到完整的日志，系统如何与客户端交互工作。最后，讲了通过 α 值来处理配置变更。\nBasic Paxos 流程是比较容易理解的，但 Multi-Paxos 却非常棘手，尤其是实际使用的时候，需要一系列的优化，这一系列优化又是不那么容易理解和做到的。这也是后来的分布式系系统纷纷转投 Raft 的原因之一吧，Paxos 的工程化实在令人头疼。\n但不得不说的是，大厂都有自己的 Paxos/Multi-Paxos 实现。Google 的论文 \u0026ldquo;Paxos made live\u0010\u0026rdquo; 介绍他们相关的工作，他们的 BigTable、chubby 都是基于文章描述的 Multi-Paxos；微信作为体量巨大的应用，也有开源的 paxos 实现：phxpaxos；\u0010阿里的 Oceanbase 也是使用 Paxos——Paxos 可谓分布式系统的皇冠。\n下篇文章我们还会继续介绍 Paxos 的其它变体：Fast-Paxos。\n","date":"2020-10-18","permalink":"/posts/202010-multi-paxos/","section":"Posts","summary":"分布式系统为了实现多副本状态机（Replicated state mac","title":"Paxos 的变种（一）：Multi-Paxos"},{"content":" Google Chubby 的作者 Mike Burrows 说过：There is only one consensus protocol, and that\u0026rsquo;s Paxos.\n引言 # 上文我们已经详细的阐述了共识问题并介绍了一些共识算法，其中 Paxos 算法是 Leslie Lamport 于 1990 年提出的共识算法，不幸的是采用希腊民主议会的比喻很明显失败了，Lamport 像写小说一样，把一个复杂的数学问题弄成了一篇带有考古色彩的历史小说。根据 Lamport 自己的描述，三个审阅人都认为该论文尽管并不重要但还有些意思，只是应该把其中所有 Paxos 相关的故事背景删掉才能发表。Lamport 对这些缺乏幽默感的人感到生气，他不打算对论文做任何修改，论文也没有得以发表。\n多年后，两个在 SRC(Systems Research Center，DEC 于 1984 年创立，Lamport 也曾在此工作过)工作的人需要为他们正在构建的分布式系统寻找一些合适算法，而 Paxos 恰恰提供了他们想要的。Lamport 就将论文发给他们，他们也没觉得该论文有什么问题。\n因此，Lamport 觉得论文重新发表的时间到了，\u0026quot;The Part-Time Parliament\u0026quot; 最终在 1998 年公开发表。\n可是很多人抱怨这篇论文根本看不懂啊，人们只记住了那个奇怪的故事，而不是 Paxos 算法。Lamport 走到哪都要被人抱怨一通。于是他忍无可忍，2001 年重新发表了一篇关于 Paxos 的论文——\u0026quot;Paxos Made Simple\u0026quot;，这次论文中一个公式也没有，摘要也只有一句话：\nThe Paxos algorithm, when presented in plain English, is very simple.\n满满的都是嘲讽！\n然而，可能是表述顺序的原因，这篇论文还是非常难以理解，于是人们写了一系列文章来解释这篇论文（重复造论文），以及在工程上如何实现它。\n其中，个人认为讲解 Paxos 最好的视频来自于 Raft 算法作者 Diego Ongaro，本文采用 Diego 讲义中的图片来理解 Paxos 算法，也纠正了一个个人认为 Diego 笔误的地方。\n术语 # 基本概念 # Proposal Value：提案的值； Proposal Number：提案编号； Proposal：提案 = 提案编号 + 提案的值； Chosen：批准，也叫选定。一旦某个值被 Chosen，后续 Paxos 都必须用该值进行交互。 注：Proposal 有人叫“提议”有人叫“提案”，此处和维基百科里的翻译保持一致，叫“提案”。\n角色 # Proposer：提案发起者； Acceptor：提案接收者； Learner：提案学习者； 问题描述 # 为了高可用性，一种常见的设计是用一个 master 节点来写，然后复制到各个 slave 节点。这种解决方法的问题在于，一旦 master 节点故障，整个服务将不可用或者数据不一致。\n为了克服单点写入问题，于是有了多数派（Quorum）写，思路就是写入一半以上的节点。即，如果集群中有 N 个节点，客户端需要写入 W \u0026gt;= N/2 + 1 个节点。不需要主节点。这种方法可以容忍最多 (N-1)/2 个节点故障。\n但是问题依然存在：每个接收者该如何决定是否接受这次请求的值呢？\n如果我们接受第一次收到的值，那么当出现以下情况（Split Votes），则没有出现多数派，没有一个值被 Chosen，算法无法终止，这违反了活性（liveness）。\n为了解决 Split Votes 问题，我们允许接受多个不同的值，收到的每一个请求都接受，这时候新的问题出现了，如下，可能不止一个值被 Chosen，这违反了安全性（safety）。\n注意，Paxos 强调：\nOnce a value has been chosen, future proposals must propose the same value.\n也就是说，我们讨论的 Basic-Paxos 只会 Chosen 一个值。基于此，就需要一个两阶段（2-phase）协议，对于已经 Chosen 的值，后续的提案也要使用相同的值。\n如下图这种情况，S3 直接拒绝 red 值，因为 blue 已经 Chosen，这样就可以保证成功。\n这种方式我们需要对提案进行排序。如果你熟悉分布式系统，应该能想到 \u0026ldquo;Time, Clocks and the Ordering of Events in a Distributed System\u0026rdquo; 这篇论文，我们不能用时间来判断提案的先后顺序。\nProposal Number # 一种简单的方式就是每个请求一个唯一的编号，例如：\u0026lt;seq_id, server_id\u0026gt;，为了排序 seq_id 是自增的；同时为了避免崩溃重启，必须能在本地持久化存储。\nPaxos # 现在我们终于可以开始描述 Paxos 算法了。\n如上所述，Paxos 是一个两阶段算法。我们把第一个阶段叫做准备（Prepare）阶段，第二个阶段叫做接受（Accept）阶段。分别对应两轮 RPC。\n第一轮 Prepare RPCs： # 请求（也叫 Prepare 阶段）： # Proposer 选择一个提案编号 n，向超过半数的 Acceptor 广播 Prepare(n) 请求。\n注：这里讲义的算法流程图是向所有的 Acceptor 发起 Prepare() 请求，鄙人认为应该改为向多数派 Acceptor 发起。参考论文原文： Phase 1. (a) A proposer selects a proposal number n and sends a prepare request with number n to a majority of acceptors.\n这里 Prepare（n）不包含提案的值。\n伪代码：\nsend PREPARE(++n) 响应（也叫 PROMISE 阶段）： # Acceptor 接收到 Prepare（n) 请求，此时有两种情况：\n如果 n 大于之前接受到的所有 Prepare 请求的编号，则返回 Promise() 响应，并承诺将不会接收编号小于 n 的提案。如果有提案被 Chosen 的话，Promise() 响应还应包含前一次提案编号和对应的值。 否则（即 n 小于等于 Acceptor 之前收到的最大编号）忽略，但常常会回复一个拒绝响应。 所以，Acceptor 需要持久化存储 max_n、accepted_N 和 accepted_VALUE。\n伪代码：\nif (n \u0026gt; max_n) max_n = n // save highest n we\u0026#39;ve seen so far if (proposal_accepted == true) // was a proposal already accepted? respond: PROMISE(n, accepted_N, accepted_VALUE) else respond: PROMISE(n) else do not respond (or respond with a \u0026#34;fail\u0026#34; message) 第二轮 Accept RPCs： # 请求（也叫 PROPOSE 阶段）： # 当 Proposer 收到超过半数 Acceptor 的 Promise() 响应后，Proposer 向多数派的 Acceptor 发起 Accept(n, value) 请求并带上提案编号和值。（注：这里讲义的算法流程图是向所有的 Acceptor 发起 Accept() 请求，鄙人认为应该改为向多数派 Acceptor 发起。）\n注意：Proposer 不一定是将 Accept() 请求发给有应答的多数派 Acceptors，可以再选另一个多数派 Acceptors 广播 Accept() 请求。\n**关于值 value 的选择：**如果前面的 Promise 响应有返回 accepted_VALUE，那就使用这个值作为 value。如果没有返回 accepted_VALUE，那可以自由决定提案值 value。\n伪代码：\ndid I receive PROMISE responses from a majority of acceptors? if yes do any responses contain accepted values (from other proposals)? if yes val = accepted_VALUE // value from PROMISE message with the highest accepted ID if no val = VALUE // we can use our proposed value send Accept(ID, val) to at least a majority of acceptors 响应（也叫 ACCEPT 阶段）： # Acceptor 收到 Accept() 请求，在这期间如果 Acceptor 没有对比 n 更大的编号另行 Promise，则接受该提案。\n伪代码：\nif (n \u0026gt;= max_n) // is the n the largest I have seen so far? proposal_accepted = true // note that we accepted a proposal accepted_N = n // save the accepted proposal number accepted_VALUE = VALUE // save the accepted proposal data respond: Accepted(N, VALUE) to the proposer and all learners else do not respond (or respond with a \u0026#34;fail\u0026#34; message) Notes # 值得注意的是，只有 Proposer 知道某个提案的值是否被 Chosen，如果其它节点想知道某个值是否被 Chosen，那就必须用该值发起一次提案，执行一次 Paxos 算法。\n一些例子 # 情况 1：提案已 Chosen # S1 收到客户端提案请求 X，于是 S1 向 S1-S3 发起 Prepare(3.1) 请求，PROMISE() 响应返回没有提案被 Chosen 由于 S1-S3 没有任何提案被 Chosen，S1 继续向 S1-S3 发送 Accept(3.1, X) 请求，提案被成功 Chosen 在提案被 Chosen 后，S5 收到客户端提案值为 Y 的请求，向 S3-S5 发送 Prepare(4.5) 请求，由于编号 4 \u0026gt; 3 会收到提案值为 X 已经被 Chosen 的 PROMISE() 响应 于是 S5 将提案值 Y 替换成 X，向 S1-S3 发送 Accept(4.5, X) 请求，提案再次被 Chosen 情况 2：提案未 Chosen，Proposer 可见 # 情况 2 和情况 1 类似，在 S3 Chosen 了提案后，S5 收到来自 S3 的 PROMISE() 响应包含了已经 Chosen 的提案值 X，所以同样会将提案值替换成 X，最终所有 Acceptor 对 X 达成共识。\n注意上面的伪代码：do any responses contain accepted values，也就是说只要有一个 Acceptor 在 Promise() 响应中返回了提案值，就要用它来替换提案值。\n情况 3：提案未提交，Proposer 不可见 # 情况 3 中，提案只被 S1 Chosen，S3 还未 Chosen 该提案，S3-S5 的 Promise() 响应中没有任何提案信息，所以 S5 自行决定提案值为 Y，发送 Accept(4.5, Y) 请求。\n由于此时 S3 承诺的提案编号 n 变为了 4 且 4 大于 3，所以 S3 不再接受 S1 后续的 Accept(3.1, X) 请求。提案值 X 被阻止，而提案值 Y 最终被 Chosen。\n活锁 # 如图：当 Proposer 在第一轮 Prepare 发出请求，还没来得及后续的第二轮 Accept 请求，紧接着第二个 Proposer 在第一阶段也发出编号更大的请求。如果这样无穷无尽，Acceptor 始终停留在决定顺序号的过程上，那大家谁也成功不了。\n解决活锁最简单的方式就是引入随机超时，这样可以让某个 Proposer 先进行提案，减少一直互相抢占的可能。\n结语 # Paxos 只从一个或多个值中选择一个值，如果需要重复运行 Paxos 来创建复制状态机，我们称之为 multi-Paxos，但如果每个命令都通过一个Basic Paxos算法实例来达到一致，会产生大量开销。对于 multi-Paxos 可以做一些优化，我们在下篇文章中讨论 Paxos 的变种。\n","date":"2020-09-29","permalink":"/posts/202009-basic-paxos/","section":"Posts","summary":"Google Chubby 的作者 Mike Burrows 说过：There is only one consensus protocol, and that\u0026rsquo;s Paxos. 引言 # 上文我们","title":"理解 Paxos（含伪代码）"},{"content":"","date":"2020-09-13","permalink":"/tags/git/","section":"Tags","summary":"","title":"git"},{"content":" 最近在给 kubernetes 提交代码，k8s 社区要求非常严格，既要分支保持与主干的代码同步，还要一次只能有一条 commit。过程中我错误地使用了一把 git merge 和 git rebase，特此总结一下。\n区别 # 同样更新分支，git merge 和 rebase 有什么区别？让我们从这个例子来看：\n* 33facc8 (master) Commit 3 | | * 3b36f32 (second_branch) Detached commit | | |/ * 29af11f Commit 2 | * 1439f8e Commit 1 我们在 Commit 2 创建分支 second_branch 写代码，并提交了一个 commit: 3b36f32，在这之后，主干有人也提交了代码 Commit 3。\n问题来了：如何把 Commit 3 拉到我们的分支继续开发？（你的领导或同事肯定经常让你这样干！）\n这时候用 git merge master 或 git rebase master 都能更新 second_branch，也许有时候还要处理下冲突。但他们的结果却不相同，如下图：\ngit merge master git rebase master 合并 master 的记录到分支，合并之后的所有 commit 会按提交时间从新到旧排列。 当前分支的 HEAD 会移动到 master 的结尾，但会变成一个新的 commit。 用 git log --graph 查看的话，会有一条丑陋的边！ git log --graph 是一条漂亮的直线 保持了所有 commit 的连贯性 commit 历史被修改了，3b36f32 被修改成了 a018520 什么时候用 rebase，什么时候用 merge？ # 用 merge 来把分支合并到主干。（废话！） 如果你的分支要跟别人共享，则不建议用 rebase，因为 rebase 会创建不一致的提交历史。 如果只有你个人开发推荐使用 rebase。 如果你想保留完整的提交历史，推荐使用 merge，merge 保留历史 而 rebase 重写历史。 rebase 还可以压缩、简化历史，通过 git rebase -i 可以在分支合并到主干前，整理自己分支的提交历史，把很多细碎的 commit 整理成一条详细的 commit。 rebase 一次只处理一个冲突，merge 则一次处理全部冲突。处理冲突 rebase 更方便，但如果有很多冲突的话，撤销一个 rebase 会比 merge 更复杂，merge 只需要撤销一次。 ","date":"2020-09-13","permalink":"/posts/202009-git-merge-vs-rebase/","section":"Posts","summary":"最近在给 kubernetes 提交代码，k8s 社区要求非常严格，既要分支保持与主","title":"同样更新分支，git merge 和 rebase 有什么区别？"},{"content":"混乱的“一致性”问题 # Consensus != Consistency\n受翻译影响，网上很多讨论 paxos 或 raft 的博客使用“分布式一致性协议”或者“分布式一致性算法”这样的字眼，虽然在汉语中“达成共识”和“达成一致”是一个意思，但是必须要说明在这里讨论的是 consensus 问题，使用“共识”来表达更清晰一些。而 CAP 定理中的 C 和数据库 ACID 的 C 才是真正的“一致性”—— consistency 问题，尽管这两个 C 讨论的也不是同一个问题，但在这里不展开。\n为了规范和清晰表达，在讨论 consensus 问题的时候统一使用“共识”一词。\n注：在早些的文献中，共识（consensus）也叫做协商（agreement）。\n共识问题 # 那么共识问题到底是什么呢？举个生活中的例子，小明和小王出去聚会，小明问：“小王，我们喝点什么吧？” 小王：“喝咖啡怎么样？” 小明：“好啊，那就来杯咖啡。”\n在上面的场景中，小王提议喝一杯咖啡，小明表示同意，两人就“喝杯咖啡”这个问题达成共识，并根据这个结果采取行动。这就是生活中的共识。\n在分布式系统中，共识就是系统中的多个节点对某个值达成一致。共识问题可以用数学语言来描述：一个分布式系统包含 n 个进程 {0, 1, 2,\u0026hellip;, n-1}，每个进程都有一个初值，进程之间互相通信，设计一种算法使得尽管出现故障，进程们仍协商出某个不可撤销的最终决定值，且每次执行都满足以下三个性质：\n终止性（Termination）：所有正确的进程最终都会认同某一个值。 协定性（Agreement）：所有正确的进程认同的值都是同一个值。 完整性（Integrity），也称作有效性（Validity）：如果正确的进程都提议同一个值，那么所有处于认同状态的正确进程都选择该值。 完整性可以有一些变化，例如，一种较弱的完整性是认定值等于某些正确经常提议的值，而不必是所有进程提议的值。完整性也隐含了，最终被认同的值必定是某个节点提出过的。\n为什么要达成共识？ # 我们首先介绍分布式系统达成共识的动机。\n在前文中，我们已经了解到分布式系统的几个主要难题：\n网络问题 时钟问题 节点故障问题 第一篇提到共识问题的文献1 来自于 lamport 的 \u0026ldquo;Time, Clocks and the Ordering of Events in a Distributed System2\u0026quot;，尽管它并没有明确的提出共识(consensus)或者协商(agreement)的概念。论文阐述了在分布式系统中，你无法判断事件 A 是否发生在事件 B 之前，除非 A 和 B 存在某种依赖关系。由此还引出了分布式状态机的概念。\n在分布式系统中，共识就常常应用在这种多副本状态机（Replicated state machines），状态机在每台节点上都存有副本，这些状态机都有相同的初始状态，每次状态转变、下个状态是什么都由相关进程共同决定，每一台节点的日志的值和顺序都相同。每个状态机在“哪个状态是下一个需要处理的状态”这个问题上达成共识，这就是一个共识问题。\n最终，这些节点看起来就像一个单独的、高可靠的状态机。Raft 的论文3提到，使用状态机我们就能克服上述三个问题：\n满足在所有非拜占庭条件下确保安全（不会返回错误结果），包括网络延迟、分区、丢包、重复和重排序。 不依赖于时序。 高可用。只要集群中的大部分节点正常运行，并能够互相通信且可以同客户端通信，这个集群就完全可用。因此，拥有5个节点的集群可以容忍其中的2个节点失败。假使通过停掉某些节点使其失败，稍后它们会从持久化存储的状态进行恢复，并重新加入到集群中。 不仅如此，达成共识还可以解决分布式系统中的以下经典问题：\n互斥（Mutual exclusion）：哪个进程进入临界区访问资源？分布式锁？ 选主（Leader election）：在单主复制的数据库，需要所有节点就哪个节点是领导者达成共识。如果一些由于网络故障而无法与其他节点通信，可能会产生两个领导者，它们都会接受写入，数据就可能会产生分歧，从而导致数据不一致或丢失。 原子提交（Atomic commit）：跨多节点或跨多分区事务的数据库中，一个事务可能在某些节点上失败，但在其他节点上成功。如果我们想要维护这种事务的原子性，必须让所有节点对事务的结果达成共识：要么全部提交，要么全部中止/回滚。 总而言之，在共识的帮助下，分布式系统就可以像单一节点一样工作——所以共识问题是分布式系统最基本的问题。\n系统模型 # 在考虑如何达成共识之前，需要考虑分布式系统中有哪些可供选择的计算模型。主要有以下几个方面：\n网络模型：\n同步（Synchronous）：响应时间是在一个固定且已知的有限范围内。 异步（Asynchronous）：响应时间是无限的。 故障类型：\nFail-stop failures：节点突然宕机并停止响应其它节点。 Byzantine failures：源自“拜占庭将军问题” ，是指节点响应的数据会产生无法预料的结果，可能会互相矛盾或完全没有意义，这个节点甚至是在“说谎”，例如一个被黑客入侵的节点。 消息模型：\n口头消息(oral messages)：消息被转述的时候是可能被篡改的。 签名消息(signed messages)：消息被发出来之后是无法伪造的，只要被篡改就会被发现。 作为最常见的，我们将分别讨论在同步系统和异步系统中的共识。在同步通信系统中达成共识是可行的(下文将会谈论这点)，但是，在实际的分布式系统中同步通信是不切实际的，我们不知道消息是故障了还是延迟了。异步与同步相比是一种更通用的情况。一个适用于异步系统的算法，也能被用于同步系统，但是反过来并不成立。\n让我们先从异步的情况开始。\n异步系统中的共识 # FLP 不可能（FLP Impossibility） # 早在 1985 年，Fischer、Lynch 和 Paterson （FLP）在 \u0026ldquo;Impossibility of Distributed Consensus with One Faulty Process\u0026rdquo;4 证明了：在一个异步系统中，即使只有一个进程出现了故障，也没有算法能保证达成共识。\n简单来说，因为在一个异步系统中，进程可以随时发出响应，所以没有办法分辨一个进程是速度很慢还是已经崩溃，这不满足终止性（Termination）。详细的证明已经超出本文范围，不在细述5。\n此时，人们意识到一个分布式共识算法需要具有的两个属性：安全性(safety)和活性(liveness)。安全性意味着所有正确的进程都认同同一个值，活性意味着分布式系统最终会认同某一个值。每个共识算法要么牺牲掉一个属性，要么放宽对网络异步的假设。\n虽然 FLP 不可能定理听着让人望而生畏，但也给后来的人们提供了研究的思路——不再尝试寻找异步通信系统中共识问题完全正确的解法。FLP 不可能是指无法确保达成共识，并不是说如果有一个进程出错，就永远无法达成共识。这种不可能的结果来自于算法流程中最坏的结果：\n一个完全异步的系统 发生了故障 最后，不可能有一个确定的共识算法。 针对这些最坏的情况，可以找到一些方法，尽可能去绕过 FLP 不可能，能满足大部分情况下都能达成共识。《分布式系统：概念与设计》提到一般有三种办法：\n故障屏蔽（Fault masking） 使用故障检测器（Failure detectors） 使用随机性算法（Non-Determinism） 1、故障屏蔽（Fault masking） # 既然异步系统中无法证明能够达成共识，我们可以将异步系统转换为同步系统，故障屏蔽就是第一种方法。故障屏蔽假设故障的进程最终会恢复，并找到一种重新加入分布式系统的方式。如果没有收到来自某个进程的消息，就一直等待直到收到预期的消息。\n例如，两阶段提交事务使用持久存储，能够从崩溃中恢复。如果一个进程崩溃，它会被重启（自动重启或由管理员重启）。进程在程序的关键点的持久存储中保留了足够多的信息，以便在崩溃和重启时能够利用这些数据继续工作。换句话说故障程序也能够像正确的进程一样工作，只是它有时候需要很长时间来执行一个恢复处理。\n故障屏蔽被应用在各种系统设计中。\n2、使用故障检测器（Failure detectors） # 将异步系统转换为同步系统的第二个办法就是引入故障检测器，进程可以认为在超过一定时间没有响应的进程已经故障。一种很常见的故障检测器的实现：超时（timeout）。\n但是，这种办法要求故障检测器是精确的。如果故障器不精确的话，系统可能放弃一个正常的进程；如果超时时间设定得很长，进程就需要等待（并且不能执行任何工作）较长的时间才能得出出错的结论。这个方法甚至有可能导致网络分区。\n解决办法是使用“不完美”的故障检测器。Chanadra 和 Toueg 在 \u0026ldquo;The weakest failure detector for solving consensus6\u0026rdquo; 中分析了一个故障检测器必须拥有的两个属性：\n完全性（Completeness）：每一个故障的进程都会被每一个正确的进程怀疑。 精确性（Accuracy）：正确的进程没有被怀疑。 同时，他们还证明了，即使是使用不可靠的故障检测器，只要通信可靠，崩溃的进程不超过 N/2，那么共识问题是可以解决的。我们不需要实现 Strong Completeness 和 Strong Accuracy，只需要一个最终弱故障检测器（eventually weakly failure detector），该检测器具有如下性质：\n最终弱完全性（eventually weakly complete）：每一个错误进程最终常常被一些正确进程怀疑； 最终弱精确性（eventually weakly accurate）：经过某个时刻后，至少一个正确的进程从来没有被其它正确进程怀疑。 该论文还证明了，在异步系统中，我们不能只依靠消息来实现一个最终弱故障检测器。但是，实际的故障检测器能够根据观察到的响应时间调节它的超时值。如果一个进程或者一个到检测器的连接很慢，那么超时值就会增加，那么错误地怀疑一个进程的情况将变得很少。从实用目的来看，这样的弱故障检测器与理想的最终弱故障检测器十分接近。\n3、使用随机性算法(Non-Determinism) # 这种解决不可能性的技术是引入一个随机算法，随机算法的输出不仅取决于外部的输入，还取决于执行过程中的随机概率。因此，给定两个完全相同的输入，该算法可能会输出两个不同的值。随机性算法使得“敌人”不能有效地阻碍达成共识。\n和传统选出领导、节点再协作的模式不同，像区块链这类共识是基于哪个节点最快计算出难题来达成的。区块链中每一个新区块都由本轮最快计算出数学难题的节点添加，整个分布式网络持续不断地建设这条有时间戳的区块链，而承载了最多计算量的区块链正是达成了共识的主链（即累积计算难度最大）。\n比特币使用了 PoW（Proof of Work）来维持共识，一些其它加密货币（如 DASH、NEO）使用 PoS（Proof of Stake），还有一些（如 Ripple）使用分布式账本（ledger）。\n但是，这些随机性算法都无法严格满足安全性(safety)。攻击者可以囤积巨量算力，从而控制或影响网络的大量正常节点，例如控制 50% 以上网络算力即可以对 PoW 发起女巫攻击（Sybil Attack）。只不过前提是攻击者需要付出一大笔资金来囤积算力，实际中这种风险性很低，如果有这么强的算力还不如直接挖矿赚取收益。\n同步系统中的共识 # 上述的方法 1 和 2，都想办法让系统比较“同步”。我们熟知的 Paxos 在异步系统中，由于活锁的存在，并没有完全解决共识问题（liveness不满足）。但 Paxos 被广泛应用在各种分布式系统中，就是因为在达成共识之前，系统并没有那么“异步”，还是有极大概率达成共识的。\nDolev 和 Strong 在 \u0026ldquo;Authenticated Algorithms for Byzantine Agreement7\u0026rdquo; 证明了：同步系统中，如果 N 个进程中最多有 f 个会出现崩溃故障，那么经过 f + 1 轮消息传递后即可达成共识。\nFischer 和 Lynch 的 \u0026ldquo;A lower bound for the time to assure interactive consistency8\u0026rdquo; 证明了，该结论同样适用于拜占庭故障。\n基于此，大多数实际应用都依赖于同步系统或部分同步系统的假设。\n同步系统中的拜占庭将军问题 # Leslie Lamport、Robert Shostak 和 Marshall Pease 在 \u0026ldquo;拜占庭将军问题（The Byzantine General’s Problem)9\u0026rdquo; 论文中讨论了 3 个进程互相发送未签名（口头的）的消息，并证明了只要有一个进程出现故障，就无法满足拜占庭将军的条件。但如果使用签名的消息，那么 3 个将军中有一个出现故障，也能实现拜占庭共识。\nPease 将这种情况推广到了 N 个进程，也就是在一个有 f 个拜占庭故障节点的系统中，必须总共至少有 3f + 1 个节点才能够达成共识。即 N \u0026gt;= 3f + 1。\n虽然同步系统下拜占庭将军问题的确存在解，但是代价很高，需要 O(N^f+1 ) 的信息交换量，只有在那些安全威胁很严重的地方使用（例如：航天工业）。\nPBFT 算法 # PBFT(Practical Byzantine Fault Tolerance) 10 算法顾名思义是一种实用的拜占庭容错算法，由 Miguel Castro 和 Barbara Liskov 发表于 1999 年。\n算法的主要细节不再展开。PBFT 也是通过使用同步假设保证活性来绕过 FLP 不可能。PBFT 算法容错数量同样也是 N \u0026gt;= 3f + 1，但只需要 O(n^2 ) 信息交换量，即每台计算机都需要与网络中其他所有计算机通讯。\n虽然 PBFT 已经有了一定的改进，但在大量参与者的场景还是不够实用，不过在拜占庭容错上已经作出很重要的突破，一些重要的思想也被后面的共识算法所借鉴。\n结语 # 本文参考了很多资料文献，对“共识问题”的研究历史做一些基础概述，希望能对你带来一点帮助。\n本文提到的论文，很多直接谈论结果，忽略了其中的数学证明，一是本文只是提纲挈领的讨论共识问题，建立一个知识框架，后续方便往里面填充内容；二是考虑到大部分读者对数学证明过程并不敢兴趣，也不想本文变成一本书那么长。本文也遗漏许多重要算法，后续如有必要会继续补充。\n限于本人能力，恳请读者们对本文存在的错误和不足之处，欢迎留言或私信告诉我。\n下篇我们将会讨论 Paxos 算法。\nReference # Mark Mc Keown: \u0026ldquo;A brief history of Consensus, 2PC and Transaction\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLeslie Lamport: \u0026ldquo;Time, Clocks and the Ordering of Events in a Distributed System\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDiego Ongaro and John Ousterhout: \u0026ldquo;In Search of an Understandable Consensus Algorithm\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFischer、Lynch and Paterson; \u0026ldquo;Impossibility of Distributed Consensus with One Faulty Process\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brief Tour of FLP Impossibility\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nChanadra and Toueg: \u0026ldquo;The weakest failure detector for solving consensus\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAuthenticated Algorithms for Byzantine Agreement\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA lower bound for the time to assure interactive consistency\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLeslie Lamport, Robert Shostak, and Marshall Pease: \u0026ldquo;The Byzantine General’s Problem\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPractical Byzantine Fault Tolerance\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2020-09-06","permalink":"/posts/202009-consensus/","section":"Posts","summary":"混乱的“一致性”问题 # Consensus != Consistency 受翻译影响，网上很多讨论 paxos 或 raft 的","title":"分布式系统的核心：共识问题"},{"content":"Google “三驾马车”论文发布以后，“大数据”和“分布式”就成了各种科技新闻媒体的“宠儿”，虽然如今头版已被“人工智能”、“机器学习”占据，围绕着解决大规模分布式应用技术挑战的话题还是能引起广泛的关注。\n回想当初还在大学时候，我喜欢分布式而不是机器学习，主要是因为相比于机器学习大块大块的公式，分布式系统几乎不需要太多数学知识，它是一门理论模型与工程技法并重的学科。作为一个分布式系列的开篇，本文没有高深的论文和复杂的数学，只是分享我对分布式系统的一些认识。\n费林分类法（Flynn\u0026rsquo;s Taxonomy） # 费林根据资讯流（information stream）可分成指令（Instruction）和资料（Data）两种，把计算模型分为4类：\n单指令流单数据流计算机（SISD）：传统的单核 CPU，没有并行计算。 单指令流多数据流计算机（SIMD）：一个常见的例子就是 GPU 架构。Intel 也有支持 SIMD 的 CPU 架构。 多指令流单数据流计算机（MISD）：用多个指令操作单个数据流，几乎没有意义。 多指令流多数据流计算机（MIMD）：多个独立的 CPU 可以协作解决完全不同的子问题甚至是单一的大问题。涵盖了并行和分布式系统。 MIMD 的程序可以是同步的，也可以是异步的。能力强大的 MIMD 是目前主流的架构类型，超级计算机、并行计算机集群、分布式系统、多处理器计算机和多核计算机都属于这种类型。\nMIMD 可以继续分类：\n按内存：具有共享存储器的通常称为多处理器（multiprocessor）；而不具有共享存储器的则称为多计算机(multicomputer) 多处理器系统（multi-processor） 多计算机系统（multi-computer） 连接方式： 总线式（Bus-based） 交换式（Switched） 关联程度： 紧耦合式：消息延迟短、带宽高、系统可用性高，多用于并行系统 松耦合式：消息延迟长、带宽更低、部分组件可能发生故障而不影响其他组件，多用于分布式系统 可以发现，我们要讨论的分布式系统，就是 MIMD 类型架构中 Bus-based multicomputers 和 Switched multicomputers\n什么是分布式系统？ # 提到分布式系统我们常常会想到很多大型的系统，比如说搜索引擎，你知道它背后肯定不止一台服务器进行处理，也知道微博、微信和淘宝背后是多个大型数据中心。这种观点当然是正确的。但是分布式系统不一定是大规模的，一个家用的 NAS 服务器也是分布式系统，甚至和你电脑连接的蓝牙键盘也是分布式系统。\n在《分布式系统：概念与设计》一书中，对分布式系统做了如下定义：\n分布式系统是其组件分布在连网的计算机上，组件之间通过传递消息进行通信和协调的系统。\n通常有以下特点：\n不共享内存（只能通过网络通信） 不共享时钟 不共享操作系统 为什么需要分布式系统？ # 有时候，并不是仅仅因为觉得便宜而将多台计算机连接在一起。建立分布式系统有真正的好处：\n社交：让地理位置分散的用户可以一起工作一起玩，这样的例子很多：分布式文档系统、多人游戏、视频会议和社交网络等； 高可用：一小部分机器宕机了，整个系统仍然可以正常工作； 扩展性：当业务扩张、用户数变多或历史数据变得越来越大时，可以直接往现有分布式系统中添加机器； 远程服务：我们的电脑和手机无法放下日益增长的数据，像 Dropbox、iCloud 等软件为我们提供远程服务； IoT：智能家居、自动售卖机、智能收费站和未来更多的的 IoT 设备都会涉及到分布式系统的应用。 性价比：价格当然也是其中的原因。 分布式系统的挑战 # 分布式系统虽好，但系统的复杂性同时会引入很多棘手的问题，下面重点关注三个问题：\n网络延迟问题 # 分布式系统中的多个节点以网络进行通信，但是网络并不保证什么时候到达以及是否一定到达。很多事情可能会出错：\n请求可能已经丢失（可能有人拔掉了网线）。 请求可能正在排队，稍后将交付（也许网络或收件人超载）。 远程节点可能已经失效（可能是崩溃或关机）。 远程节点可能暂时停止了响应（可能会遇到长时间的垃圾回收暂停；参阅“暂停进程”），但稍后会再次响应。 远程节点可能已经处理了请求，但是网络上的响应已经丢失（可能是网络交换机配置错误）。 远程节点可能已经处理了请求，但是响应已经被延迟，并且稍后将被传递（可能是网络或者你自己的机器过载）。 时钟问题 # ​消息通过网络从一台机器传送到另一台机器需要时间，但由于网络中的可变延迟，我们不知道到底花了多少时间。这个事实有时很难确定在涉及多台机器时发生事情的顺序。 网络上的每台机器都有自己的时钟，可能比其他机器稍快或更慢。 部分失效 # 单机系统上的程序要么工作，要么出错。\n在分布式系统中，系统的某些部分可能会以某种不可预知的方式宕机。这被称为部分失效（partial failure）。\n难点在于部分失效是不确定的：如果你试图做任何涉及多个节点和网络的事情，它有时可能会工作，有时会出现不可预知的失败。正如我们将要看到的，你甚至不知道是否成功了，因为消息通过网络传播的时间也是不确定的！\n这种不确定性和部分失效的可能性，使得分布式系统难以琢磨和调试。\n结语 # 分布式系统主要研究三大方向：\n分布式存储系统 分布式计算系统 分布式调度系统 这些方向都有一些特定的算法，但是，分布式共识问题试图探讨分布式系统中最基本的问题——如何让分布式系统中的节点达成共识？到底什么是共识？下一篇，我们将深入讨论这个问题。\n","date":"2020-08-18","permalink":"/posts/202008-intro-distributed-system/","section":"Posts","summary":"Google “三驾马车”论文发布以后，“大数据”和“分布式”就成了各种","title":"认识分布式系统"}]